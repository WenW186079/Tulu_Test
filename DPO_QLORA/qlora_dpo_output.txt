Number of GPUs: 4
Using config file: configs/train_configs/dpo/mini.yaml
CUDA_VISIBLE_DEVICES: 0,1,2,3
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
[2025-01-09 19:56:19,551] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
W0109 19:56:25.116000 129104769488000 torch/distributed/run.py:779] 
W0109 19:56:25.116000 129104769488000 torch/distributed/run.py:779] *****************************************
W0109 19:56:25.116000 129104769488000 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0109 19:56:25.116000 129104769488000 torch/distributed/run.py:779] *****************************************
[2025-01-09 19:56:40,993] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-01-09 19:56:42,028] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-01-09 19:56:42,052] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-01-09 19:56:42,072] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-01-09 19:57:02,957] [INFO] [comm.py:652:init_distributed] cdb=None
[2025-01-09 19:57:02,958] [INFO] [comm.py:652:init_distributed] cdb=None
[2025-01-09 19:57:02,958] [INFO] [comm.py:652:init_distributed] cdb=None
[2025-01-09 19:57:02,958] [INFO] [comm.py:652:init_distributed] cdb=None
[2025-01-09 19:57:02,959] [INFO] [comm.py:683:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
/workspace/tulu_env/lib/python3.10/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(use_seedable_sampler=True)
  warnings.warn(
INFO:__main__:Distributed environment: DEEPSPEED  Backend: nccl
Num processes: 4
Process index: 0
Local process index: 0
Device: cuda:0

Mixed precision type: no
ds_config: {'bf16': {'enabled': False}, 'zero_optimization': {'stage': 2, 'reduce_scatter': True, 'allgather_partitions': True, 'allgather_bucket_size': 500000000.0, 'reduce_bucket_size': 500000000.0, 'overlap_comm': True, 'contiguous_gradients': True}, 'gradient_accumulation_steps': 'auto', 'gradient_clipping': 'auto', 'steps_per_print': inf, 'train_batch_size': 'auto', 'train_micro_batch_size_per_gpu': 'auto', 'wall_clock_breakdown': False, 'save_16bit_model': True, 'fp16': {'enabled': False}}

/workspace/tulu_env/lib/python3.10/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(use_seedable_sampler=True)
  warnings.warn(
INFO:__main__:Distributed environment: DEEPSPEED  Backend: nccl
Num processes: 4
Process index: 2
Local process index: 2
Device: cuda:2

Mixed precision type: no
ds_config: {'bf16': {'enabled': False}, 'zero_optimization': {'stage': 2, 'reduce_scatter': True, 'allgather_partitions': True, 'allgather_bucket_size': 500000000.0, 'reduce_bucket_size': 500000000.0, 'overlap_comm': True, 'contiguous_gradients': True}, 'gradient_accumulation_steps': 'auto', 'gradient_clipping': 'auto', 'steps_per_print': inf, 'train_batch_size': 'auto', 'train_micro_batch_size_per_gpu': 'auto', 'wall_clock_breakdown': False, 'save_16bit_model': True, 'fp16': {'enabled': False}}

/workspace/tulu_env/lib/python3.10/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(use_seedable_sampler=True)
  warnings.warn(
INFO:__main__:Distributed environment: DEEPSPEED  Backend: nccl
Num processes: 4
Process index: 1
Local process index: 1
Device: cuda:1

Mixed precision type: no
ds_config: {'bf16': {'enabled': False}, 'zero_optimization': {'stage': 2, 'reduce_scatter': True, 'allgather_partitions': True, 'allgather_bucket_size': 500000000.0, 'reduce_bucket_size': 500000000.0, 'overlap_comm': True, 'contiguous_gradients': True}, 'gradient_accumulation_steps': 'auto', 'gradient_clipping': 'auto', 'steps_per_print': inf, 'train_batch_size': 'auto', 'train_micro_batch_size_per_gpu': 'auto', 'wall_clock_breakdown': False, 'save_16bit_model': True, 'fp16': {'enabled': False}}

/workspace/tulu_env/lib/python3.10/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(use_seedable_sampler=True)
  warnings.warn(
INFO:__main__:Distributed environment: DEEPSPEED  Backend: nccl
Num processes: 4
Process index: 3
Local process index: 3
Device: cuda:3

Mixed precision type: no
ds_config: {'bf16': {'enabled': False}, 'zero_optimization': {'stage': 2, 'reduce_scatter': True, 'allgather_partitions': True, 'allgather_bucket_size': 500000000.0, 'reduce_bucket_size': 500000000.0, 'overlap_comm': True, 'contiguous_gradients': True}, 'gradient_accumulation_steps': 'auto', 'gradient_clipping': 'auto', 'steps_per_print': inf, 'train_batch_size': 'auto', 'train_micro_batch_size_per_gpu': 'auto', 'wall_clock_breakdown': False, 'save_16bit_model': True, 'fp16': {'enabled': False}}

loading configuration file config.json from cache at /workspace/hub/models--WenWW--sparse_Llama_8B_2of4_SFT_qlora_test/snapshots/5190ea998c5a259a5e63b8f43a8a70d83522c7d4/config.json
Model config LlamaConfig {
  "_name_or_path": "WenWW/sparse_Llama_8B_2of4_SFT_qlora_test",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "quantization_config": {
    "_load_in_4bit": true,
    "_load_in_8bit": false,
    "bnb_4bit_compute_dtype": "bfloat16",
    "bnb_4bit_quant_storage": "uint8",
    "bnb_4bit_quant_type": "nf4",
    "bnb_4bit_use_double_quant": true,
    "llm_int8_enable_fp32_cpu_offload": false,
    "llm_int8_has_fp16_weight": false,
    "llm_int8_skip_modules": null,
    "llm_int8_threshold": 6.0,
    "load_in_4bit": true,
    "load_in_8bit": false,
    "quant_method": "bitsandbytes"
  },
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.43.4",
  "use_cache": true,
  "vocab_size": 128264
}

loading file tokenizer.json from cache at /workspace/hub/models--neuralmagic--Sparse-Llama-3.1-8B-2of4/snapshots/c28c229cb0b8b8dab0e3419f489046fddd1eb8d2/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /workspace/hub/models--neuralmagic--Sparse-Llama-3.1-8B-2of4/snapshots/c28c229cb0b8b8dab0e3419f489046fddd1eb8d2/special_tokens_map.json
loading file tokenizer_config.json from cache at /workspace/hub/models--neuralmagic--Sparse-Llama-3.1-8B-2of4/snapshots/c28c229cb0b8b8dab0e3419f489046fddd1eb8d2/tokenizer_config.json
==============qlora!==========
==============qlora!==========
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
==============qlora!==========
==============qlora!==========
loading configuration file config.json from cache at /workspace/hub/models--neuralmagic--Sparse-Llama-3.1-8B-2of4/snapshots/c28c229cb0b8b8dab0e3419f489046fddd1eb8d2/config.json
Model config LlamaConfig {
  "_name_or_path": "neuralmagic/Sparse-Llama-3.1-8B-2of4",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.43.4",
  "use_cache": true,
  "vocab_size": 128256
}

loading weights file model.safetensors from cache at /workspace/hub/models--neuralmagic--Sparse-Llama-3.1-8B-2of4/snapshots/c28c229cb0b8b8dab0e3419f489046fddd1eb8d2/model.safetensors.index.json
Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.
The model was loaded with use_flash_attention_2=True, which is deprecated and may be removed in a future release. Please use `attn_implementation="flash_attention_2"` instead.
Generate config GenerationConfig {
  "bos_token_id": 128000,
  "eos_token_id": 128001
}

Loading checkpoint shards:   0%|                                                     | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                     | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                     | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|                                                     | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|███████████▎                                 | 1/4 [00:49<02:27, 49.07s/it]Loading checkpoint shards:  25%|███████████▎                                 | 1/4 [00:49<02:27, 49.07s/it]Loading checkpoint shards:  25%|███████████▎                                 | 1/4 [00:49<02:27, 49.09s/it]Loading checkpoint shards:  25%|███████████▎                                 | 1/4 [00:49<02:27, 49.17s/it]Loading checkpoint shards:  50%|██████████████████████▌                      | 2/4 [01:34<01:33, 46.88s/it]Loading checkpoint shards:  50%|██████████████████████▌                      | 2/4 [01:34<01:33, 46.89s/it]Loading checkpoint shards:  50%|██████████████████████▌                      | 2/4 [01:34<01:33, 46.89s/it]Loading checkpoint shards:  50%|██████████████████████▌                      | 2/4 [01:34<01:33, 46.88s/it]Loading checkpoint shards:  75%|█████████████████████████████████▊           | 3/4 [02:13<00:43, 43.45s/it]Loading checkpoint shards:  75%|█████████████████████████████████▊           | 3/4 [02:13<00:43, 43.44s/it]Loading checkpoint shards:  75%|█████████████████████████████████▊           | 3/4 [02:13<00:43, 43.47s/it]Loading checkpoint shards:  75%|█████████████████████████████████▊           | 3/4 [02:13<00:43, 43.47s/it]Loading checkpoint shards: 100%|█████████████████████████████████████████████| 4/4 [02:24<00:00, 30.60s/it]Loading checkpoint shards: 100%|█████████████████████████████████████████████| 4/4 [02:24<00:00, 36.18s/it]
Loading checkpoint shards: 100%|█████████████████████████████████████████████| 4/4 [02:24<00:00, 30.61s/it]Loading checkpoint shards: 100%|█████████████████████████████████████████████| 4/4 [02:24<00:00, 36.18s/it]
All model checkpoint weights were used when initializing LlamaForCausalLM.

All the weights of LlamaForCausalLM were initialized from the model checkpoint at neuralmagic/Sparse-Llama-3.1-8B-2of4.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
Loading checkpoint shards: 100%|█████████████████████████████████████████████| 4/4 [02:24<00:00, 30.63s/it]Loading checkpoint shards: 100%|█████████████████████████████████████████████| 4/4 [02:24<00:00, 36.19s/it]
Loading checkpoint shards: 100%|█████████████████████████████████████████████| 4/4 [02:24<00:00, 30.62s/it]Loading checkpoint shards: 100%|█████████████████████████████████████████████| 4/4 [02:24<00:00, 36.19s/it]
loading configuration file generation_config.json from cache at /workspace/hub/models--neuralmagic--Sparse-Llama-3.1-8B-2of4/snapshots/c28c229cb0b8b8dab0e3419f489046fddd1eb8d2/generation_config.json
Generate config GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "temperature": 0.6,
  "top_p": 0.9
}

=========base_model========
INFO:__main__:Base model loaded with quantization.
=========base_model========
LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(128256, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaFlashAttention2(
          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)
          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)
          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)
          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)
          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)
)
LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(128256, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaFlashAttention2(
          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)
          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)
          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)
          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)
          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)
)
=========base_model========
LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(128256, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaFlashAttention2(
          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)
          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)
          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)
          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)
          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)
)
=========base_model========
LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(128256, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaFlashAttention2(
          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)
          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)
          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)
          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)
          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)
)
/workspace/tulu_env/lib/python3.10/site-packages/peft/utils/save_and_load.py:473: UserWarning: Some weights of PeftModelForCausalLM were not initialized from the model checkpoint and are being ignored because you passed `ignore_mismatched_sizes=True`: - base_model.model.lm_head.weight: found shape torch.Size([128264, 4096]) in the checkpoint and torch.Size([128256, 4096]) in the model instantiated
- base_model.model.model.embed_tokens.weight: found shape torch.Size([128264, 4096]) in the checkpoint and torch.Size([128256, 4096]) in the model instantiated.
  warnings.warn(msg)
INFO:__main__:LoRA weights loaded successfully.
=========qloar  model is... ========
PeftModelForCausalLM(
  (base_model): LoraModel(
    (model): LlamaForCausalLM(
      (model): LlamaModel(
        (embed_tokens): Embedding(128256, 4096)
        (layers): ModuleList(
          (0-31): 32 x LlamaDecoderLayer(
            (self_attn): LlamaFlashAttention2(
              (q_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=64, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=64, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (k_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=64, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=64, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (v_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=64, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=64, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (o_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=64, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=64, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (rotary_emb): LlamaRotaryEmbedding()
            )
            (mlp): LlamaMLP(
              (gate_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=64, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=64, out_features=14336, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (up_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=64, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=64, out_features=14336, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (down_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=14336, out_features=64, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=64, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (act_fn): SiLU()
            )
            (input_layernorm): LlamaRMSNorm()
            (post_attention_layernorm): LlamaRMSNorm()
          )
        )
        (norm): LlamaRMSNorm()
        (rotary_emb): LlamaRotaryEmbedding()
      )
      (lm_head): Linear(in_features=4096, out_features=128256, bias=False)
    )
  )
)
trainable params: 167,772,160 || all params: 8,198,033,408 || trainable%: 2.0465
Base model params: 4708372480
LoRA params: 167772160
Total model params: 4876144640
base_model.model.model.embed_tokens.weight: False
base_model.model.model.layers.0.self_attn.q_proj.base_layer.weight: False
base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight: True
base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight: True
base_model.model.model.layers.0.self_attn.k_proj.base_layer.weight: False
base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight: True
base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight: True
base_model.model.model.layers.0.self_attn.v_proj.base_layer.weight: False
base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight: True
base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight: True
base_model.model.model.layers.0.self_attn.o_proj.base_layer.weight: False
base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight: True
base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight: True
base_model.model.model.layers.0.mlp.gate_proj.base_layer.weight: False
base_model.model.model.layers.0.mlp.gate_proj.lora_A.default.weight: True
base_model.model.model.layers.0.mlp.gate_proj.lora_B.default.weight: True
base_model.model.model.layers.0.mlp.up_proj.base_layer.weight: False
base_model.model.model.layers.0.mlp.up_proj.lora_A.default.weight: True
base_model.model.model.layers.0.mlp.up_proj.lora_B.default.weight: True
base_model.model.model.layers.0.mlp.down_proj.base_layer.weight: False
base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight: True
base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight: True
base_model.model.model.layers.0.input_layernorm.weight: False
base_model.model.model.layers.0.post_attention_layernorm.weight: False
base_model.model.model.layers.1.self_attn.q_proj.base_layer.weight: False
base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight: True
base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight: True
base_model.model.model.layers.1.self_attn.k_proj.base_layer.weight: False
base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight: True
base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight: True
base_model.model.model.layers.1.self_attn.v_proj.base_layer.weight: False
base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight: True
base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight: True
base_model.model.model.layers.1.self_attn.o_proj.base_layer.weight: False
base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight: True
base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight: True
base_model.model.model.layers.1.mlp.gate_proj.base_layer.weight: False
base_model.model.model.layers.1.mlp.gate_proj.lora_A.default.weight: True
base_model.model.model.layers.1.mlp.gate_proj.lora_B.default.weight: True
base_model.model.model.layers.1.mlp.up_proj.base_layer.weight: False
base_model.model.model.layers.1.mlp.up_proj.lora_A.default.weight: True
base_model.model.model.layers.1.mlp.up_proj.lora_B.default.weight: True
base_model.model.model.layers.1.mlp.down_proj.base_layer.weight: False
base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight: True
base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight: True
base_model.model.model.layers.1.input_layernorm.weight: False
base_model.model.model.layers.1.post_attention_layernorm.weight: False
base_model.model.model.layers.2.self_attn.q_proj.base_layer.weight: False
base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight: True
base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight: True
base_model.model.model.layers.2.self_attn.k_proj.base_layer.weight: False
base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight: True
base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight: True
base_model.model.model.layers.2.self_attn.v_proj.base_layer.weight: False
base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight: True
base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight: True
base_model.model.model.layers.2.self_attn.o_proj.base_layer.weight: False
base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight: True
base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight: True
base_model.model.model.layers.2.mlp.gate_proj.base_layer.weight: False
base_model.model.model.layers.2.mlp.gate_proj.lora_A.default.weight: True
base_model.model.model.layers.2.mlp.gate_proj.lora_B.default.weight: True
base_model.model.model.layers.2.mlp.up_proj.base_layer.weight: False
base_model.model.model.layers.2.mlp.up_proj.lora_A.default.weight: True
base_model.model.model.layers.2.mlp.up_proj.lora_B.default.weight: True
base_model.model.model.layers.2.mlp.down_proj.base_layer.weight: False
base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight: True
base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight: True
base_model.model.model.layers.2.input_layernorm.weight: False
base_model.model.model.layers.2.post_attention_layernorm.weight: False
base_model.model.model.layers.3.self_attn.q_proj.base_layer.weight: False
base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight: True
base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight: True
base_model.model.model.layers.3.self_attn.k_proj.base_layer.weight: False
base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight: True
base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight: True
base_model.model.model.layers.3.self_attn.v_proj.base_layer.weight: False
base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight: True
base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight: True
base_model.model.model.layers.3.self_attn.o_proj.base_layer.weight: False
base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight: True
base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight: True
base_model.model.model.layers.3.mlp.gate_proj.base_layer.weight: False
base_model.model.model.layers.3.mlp.gate_proj.lora_A.default.weight: True
base_model.model.model.layers.3.mlp.gate_proj.lora_B.default.weight: True
base_model.model.model.layers.3.mlp.up_proj.base_layer.weight: False
base_model.model.model.layers.3.mlp.up_proj.lora_A.default.weight: True
base_model.model.model.layers.3.mlp.up_proj.lora_B.default.weight: True
base_model.model.model.layers.3.mlp.down_proj.base_layer.weight: False
base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight: True
base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight: True
base_model.model.model.layers.3.input_layernorm.weight: False
base_model.model.model.layers.3.post_attention_layernorm.weight: False
base_model.model.model.layers.4.self_attn.q_proj.base_layer.weight: False
base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight: True
base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight: True
base_model.model.model.layers.4.self_attn.k_proj.base_layer.weight: False
base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight: True
base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight: True
base_model.model.model.layers.4.self_attn.v_proj.base_layer.weight: False
base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight: True
base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight: True
base_model.model.model.layers.4.self_attn.o_proj.base_layer.weight: False
base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight: True
base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight: True
base_model.model.model.layers.4.mlp.gate_proj.base_layer.weight: False
base_model.model.model.layers.4.mlp.gate_proj.lora_A.default.weight: True
base_model.model.model.layers.4.mlp.gate_proj.lora_B.default.weight: True
base_model.model.model.layers.4.mlp.up_proj.base_layer.weight: False
base_model.model.model.layers.4.mlp.up_proj.lora_A.default.weight: True
base_model.model.model.layers.4.mlp.up_proj.lora_B.default.weight: True
base_model.model.model.layers.4.mlp.down_proj.base_layer.weight: False
base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight: True
base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight: True
base_model.model.model.layers.4.input_layernorm.weight: False
base_model.model.model.layers.4.post_attention_layernorm.weight: False
base_model.model.model.layers.5.self_attn.q_proj.base_layer.weight: False
base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight: True
base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight: True
base_model.model.model.layers.5.self_attn.k_proj.base_layer.weight: False
base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight: True
base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight: True
base_model.model.model.layers.5.self_attn.v_proj.base_layer.weight: False
base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight: True
base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight: True
base_model.model.model.layers.5.self_attn.o_proj.base_layer.weight: False
base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight: True
base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight: True
base_model.model.model.layers.5.mlp.gate_proj.base_layer.weight: False
base_model.model.model.layers.5.mlp.gate_proj.lora_A.default.weight: True
base_model.model.model.layers.5.mlp.gate_proj.lora_B.default.weight: True
base_model.model.model.layers.5.mlp.up_proj.base_layer.weight: False
base_model.model.model.layers.5.mlp.up_proj.lora_A.default.weight: True
base_model.model.model.layers.5.mlp.up_proj.lora_B.default.weight: True
base_model.model.model.layers.5.mlp.down_proj.base_layer.weight: False
base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight: True
base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight: True
base_model.model.model.layers.5.input_layernorm.weight: False
base_model.model.model.layers.5.post_attention_layernorm.weight: False
base_model.model.model.layers.6.self_attn.q_proj.base_layer.weight: False
base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight: True
base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight: True
base_model.model.model.layers.6.self_attn.k_proj.base_layer.weight: False
base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight: True
base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight: True
base_model.model.model.layers.6.self_attn.v_proj.base_layer.weight: False
base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight: True
base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight: True
base_model.model.model.layers.6.self_attn.o_proj.base_layer.weight: False
base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight: True
base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight: True
base_model.model.model.layers.6.mlp.gate_proj.base_layer.weight: False
base_model.model.model.layers.6.mlp.gate_proj.lora_A.default.weight: True
base_model.model.model.layers.6.mlp.gate_proj.lora_B.default.weight: True
base_model.model.model.layers.6.mlp.up_proj.base_layer.weight: False
base_model.model.model.layers.6.mlp.up_proj.lora_A.default.weight: True
base_model.model.model.layers.6.mlp.up_proj.lora_B.default.weight: True
base_model.model.model.layers.6.mlp.down_proj.base_layer.weight: False
base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight: True
base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight: True
base_model.model.model.layers.6.input_layernorm.weight: False
base_model.model.model.layers.6.post_attention_layernorm.weight: False
base_model.model.model.layers.7.self_attn.q_proj.base_layer.weight: False
base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight: True
base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight: True
base_model.model.model.layers.7.self_attn.k_proj.base_layer.weight: False
base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight: True
base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight: True
base_model.model.model.layers.7.self_attn.v_proj.base_layer.weight: False
base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight: True
base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight: True
base_model.model.model.layers.7.self_attn.o_proj.base_layer.weight: False
base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight: True
base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight: True
base_model.model.model.layers.7.mlp.gate_proj.base_layer.weight: False
base_model.model.model.layers.7.mlp.gate_proj.lora_A.default.weight: True
base_model.model.model.layers.7.mlp.gate_proj.lora_B.default.weight: True
base_model.model.model.layers.7.mlp.up_proj.base_layer.weight: False
base_model.model.model.layers.7.mlp.up_proj.lora_A.default.weight: True
base_model.model.model.layers.7.mlp.up_proj.lora_B.default.weight: True
base_model.model.model.layers.7.mlp.down_proj.base_layer.weight: False
base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight: True
base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight: True
base_model.model.model.layers.7.input_layernorm.weight: False
base_model.model.model.layers.7.post_attention_layernorm.weight: False
base_model.model.model.layers.8.self_attn.q_proj.base_layer.weight: False
base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight: True
base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight: True
base_model.model.model.layers.8.self_attn.k_proj.base_layer.weight: False
base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight: True
base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight: True
base_model.model.model.layers.8.self_attn.v_proj.base_layer.weight: False
base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight: True
base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight: True
base_model.model.model.layers.8.self_attn.o_proj.base_layer.weight: False
base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight: True
base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight: True
base_model.model.model.layers.8.mlp.gate_proj.base_layer.weight: False
base_model.model.model.layers.8.mlp.gate_proj.lora_A.default.weight: True
base_model.model.model.layers.8.mlp.gate_proj.lora_B.default.weight: True
base_model.model.model.layers.8.mlp.up_proj.base_layer.weight: False
base_model.model.model.layers.8.mlp.up_proj.lora_A.default.weight: True
base_model.model.model.layers.8.mlp.up_proj.lora_B.default.weight: True
base_model.model.model.layers.8.mlp.down_proj.base_layer.weight: False
base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight: True
base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight: True
base_model.model.model.layers.8.input_layernorm.weight: False
base_model.model.model.layers.8.post_attention_layernorm.weight: False
base_model.model.model.layers.9.self_attn.q_proj.base_layer.weight: False
base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight: True
base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight: True
base_model.model.model.layers.9.self_attn.k_proj.base_layer.weight: False
base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight: True
base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight: True
base_model.model.model.layers.9.self_attn.v_proj.base_layer.weight: False
base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight: True
base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight: True
base_model.model.model.layers.9.self_attn.o_proj.base_layer.weight: False
base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight: True
base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight: True
base_model.model.model.layers.9.mlp.gate_proj.base_layer.weight: False
base_model.model.model.layers.9.mlp.gate_proj.lora_A.default.weight: True
base_model.model.model.layers.9.mlp.gate_proj.lora_B.default.weight: True
base_model.model.model.layers.9.mlp.up_proj.base_layer.weight: False
base_model.model.model.layers.9.mlp.up_proj.lora_A.default.weight: True
base_model.model.model.layers.9.mlp.up_proj.lora_B.default.weight: True
base_model.model.model.layers.9.mlp.down_proj.base_layer.weight: False
base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight: True
base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight: True
base_model.model.model.layers.9.input_layernorm.weight: False
base_model.model.model.layers.9.post_attention_layernorm.weight: False
base_model.model.model.layers.10.self_attn.q_proj.base_layer.weight: False
base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight: True
base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight: True
base_model.model.model.layers.10.self_attn.k_proj.base_layer.weight: False
base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight: True
base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight: True
base_model.model.model.layers.10.self_attn.v_proj.base_layer.weight: False
base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight: True
base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight: True
base_model.model.model.layers.10.self_attn.o_proj.base_layer.weight: False
base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight: True
base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight: True
base_model.model.model.layers.10.mlp.gate_proj.base_layer.weight: False
base_model.model.model.layers.10.mlp.gate_proj.lora_A.default.weight: True
base_model.model.model.layers.10.mlp.gate_proj.lora_B.default.weight: True
base_model.model.model.layers.10.mlp.up_proj.base_layer.weight: False
base_model.model.model.layers.10.mlp.up_proj.lora_A.default.weight: True
base_model.model.model.layers.10.mlp.up_proj.lora_B.default.weight: True
base_model.model.model.layers.10.mlp.down_proj.base_layer.weight: False
base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight: True
base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight: True
base_model.model.model.layers.10.input_layernorm.weight: False
base_model.model.model.layers.10.post_attention_layernorm.weight: False
base_model.model.model.layers.11.self_attn.q_proj.base_layer.weight: False
base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight: True
base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight: True
base_model.model.model.layers.11.self_attn.k_proj.base_layer.weight: False
base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight: True
base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight: True
base_model.model.model.layers.11.self_attn.v_proj.base_layer.weight: False
base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight: True
base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight: True
base_model.model.model.layers.11.self_attn.o_proj.base_layer.weight: False
base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight: True
base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight: True
base_model.model.model.layers.11.mlp.gate_proj.base_layer.weight: False
base_model.model.model.layers.11.mlp.gate_proj.lora_A.default.weight: True
base_model.model.model.layers.11.mlp.gate_proj.lora_B.default.weight: True
base_model.model.model.layers.11.mlp.up_proj.base_layer.weight: False
base_model.model.model.layers.11.mlp.up_proj.lora_A.default.weight: True
base_model.model.model.layers.11.mlp.up_proj.lora_B.default.weight: True
base_model.model.model.layers.11.mlp.down_proj.base_layer.weight: False
base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight: True
base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight: True
base_model.model.model.layers.11.input_layernorm.weight: False
base_model.model.model.layers.11.post_attention_layernorm.weight: False
base_model.model.model.layers.12.self_attn.q_proj.base_layer.weight: False
base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight: True
base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight: True
base_model.model.model.layers.12.self_attn.k_proj.base_layer.weight: False
base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight: True
base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight: True
base_model.model.model.layers.12.self_attn.v_proj.base_layer.weight: False
base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight: True
base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight: True
base_model.model.model.layers.12.self_attn.o_proj.base_layer.weight: False
base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight: True
base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight: True
base_model.model.model.layers.12.mlp.gate_proj.base_layer.weight: False
base_model.model.model.layers.12.mlp.gate_proj.lora_A.default.weight: True
base_model.model.model.layers.12.mlp.gate_proj.lora_B.default.weight: True
base_model.model.model.layers.12.mlp.up_proj.base_layer.weight: False
base_model.model.model.layers.12.mlp.up_proj.lora_A.default.weight: True
base_model.model.model.layers.12.mlp.up_proj.lora_B.default.weight: True
base_model.model.model.layers.12.mlp.down_proj.base_layer.weight: False
base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight: True
base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight: True
base_model.model.model.layers.12.input_layernorm.weight: False
base_model.model.model.layers.12.post_attention_layernorm.weight: False
base_model.model.model.layers.13.self_attn.q_proj.base_layer.weight: False
base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight: True
base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight: True
base_model.model.model.layers.13.self_attn.k_proj.base_layer.weight: False
base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight: True
base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight: True
base_model.model.model.layers.13.self_attn.v_proj.base_layer.weight: False
base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight: True
base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight: True
base_model.model.model.layers.13.self_attn.o_proj.base_layer.weight: False
base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight: True
base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight: True
base_model.model.model.layers.13.mlp.gate_proj.base_layer.weight: False
base_model.model.model.layers.13.mlp.gate_proj.lora_A.default.weight: True
base_model.model.model.layers.13.mlp.gate_proj.lora_B.default.weight: True
base_model.model.model.layers.13.mlp.up_proj.base_layer.weight: False
base_model.model.model.layers.13.mlp.up_proj.lora_A.default.weight: True
base_model.model.model.layers.13.mlp.up_proj.lora_B.default.weight: True
base_model.model.model.layers.13.mlp.down_proj.base_layer.weight: False
base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight: True
base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight: True
base_model.model.model.layers.13.input_layernorm.weight: False
base_model.model.model.layers.13.post_attention_layernorm.weight: False
base_model.model.model.layers.14.self_attn.q_proj.base_layer.weight: False
base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight: True
base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight: True
base_model.model.model.layers.14.self_attn.k_proj.base_layer.weight: False
base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight: True
base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight: True
base_model.model.model.layers.14.self_attn.v_proj.base_layer.weight: False
base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight: True
base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight: True
base_model.model.model.layers.14.self_attn.o_proj.base_layer.weight: False
base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight: True
base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight: True
base_model.model.model.layers.14.mlp.gate_proj.base_layer.weight: False
base_model.model.model.layers.14.mlp.gate_proj.lora_A.default.weight: True
base_model.model.model.layers.14.mlp.gate_proj.lora_B.default.weight: True
base_model.model.model.layers.14.mlp.up_proj.base_layer.weight: False
base_model.model.model.layers.14.mlp.up_proj.lora_A.default.weight: True
base_model.model.model.layers.14.mlp.up_proj.lora_B.default.weight: True
base_model.model.model.layers.14.mlp.down_proj.base_layer.weight: False
base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight: True
base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight: True
base_model.model.model.layers.14.input_layernorm.weight: False
base_model.model.model.layers.14.post_attention_layernorm.weight: False
base_model.model.model.layers.15.self_attn.q_proj.base_layer.weight: False
base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight: True
base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight: True
base_model.model.model.layers.15.self_attn.k_proj.base_layer.weight: False
base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight: True
base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight: True
base_model.model.model.layers.15.self_attn.v_proj.base_layer.weight: False
base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight: True
base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight: True
base_model.model.model.layers.15.self_attn.o_proj.base_layer.weight: False
base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight: True
base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight: True
base_model.model.model.layers.15.mlp.gate_proj.base_layer.weight: False
base_model.model.model.layers.15.mlp.gate_proj.lora_A.default.weight: True
base_model.model.model.layers.15.mlp.gate_proj.lora_B.default.weight: True
base_model.model.model.layers.15.mlp.up_proj.base_layer.weight: False
base_model.model.model.layers.15.mlp.up_proj.lora_A.default.weight: True
base_model.model.model.layers.15.mlp.up_proj.lora_B.default.weight: True
base_model.model.model.layers.15.mlp.down_proj.base_layer.weight: False
base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight: True
base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight: True
base_model.model.model.layers.15.input_layernorm.weight: False
base_model.model.model.layers.15.post_attention_layernorm.weight: False
base_model.model.model.layers.16.self_attn.q_proj.base_layer.weight: False
base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight: True
base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight: True
base_model.model.model.layers.16.self_attn.k_proj.base_layer.weight: False
base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight: True
base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight: True
base_model.model.model.layers.16.self_attn.v_proj.base_layer.weight: False
base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight: True
base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight: True
base_model.model.model.layers.16.self_attn.o_proj.base_layer.weight: False
base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight: True
base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight: True
base_model.model.model.layers.16.mlp.gate_proj.base_layer.weight: False
base_model.model.model.layers.16.mlp.gate_proj.lora_A.default.weight: True
base_model.model.model.layers.16.mlp.gate_proj.lora_B.default.weight: True
base_model.model.model.layers.16.mlp.up_proj.base_layer.weight: False
base_model.model.model.layers.16.mlp.up_proj.lora_A.default.weight: True
base_model.model.model.layers.16.mlp.up_proj.lora_B.default.weight: True
base_model.model.model.layers.16.mlp.down_proj.base_layer.weight: False
base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight: True
base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight: True
base_model.model.model.layers.16.input_layernorm.weight: False
base_model.model.model.layers.16.post_attention_layernorm.weight: False
base_model.model.model.layers.17.self_attn.q_proj.base_layer.weight: False
base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight: True
base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight: True
base_model.model.model.layers.17.self_attn.k_proj.base_layer.weight: False
base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight: True
base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight: True
base_model.model.model.layers.17.self_attn.v_proj.base_layer.weight: False
base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight: True
base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight: True
base_model.model.model.layers.17.self_attn.o_proj.base_layer.weight: False
base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight: True
base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight: True
base_model.model.model.layers.17.mlp.gate_proj.base_layer.weight: False
base_model.model.model.layers.17.mlp.gate_proj.lora_A.default.weight: True
base_model.model.model.layers.17.mlp.gate_proj.lora_B.default.weight: True
base_model.model.model.layers.17.mlp.up_proj.base_layer.weight: False
base_model.model.model.layers.17.mlp.up_proj.lora_A.default.weight: True
base_model.model.model.layers.17.mlp.up_proj.lora_B.default.weight: True
base_model.model.model.layers.17.mlp.down_proj.base_layer.weight: False
base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight: True
base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight: True
base_model.model.model.layers.17.input_layernorm.weight: False
base_model.model.model.layers.17.post_attention_layernorm.weight: False
base_model.model.model.layers.18.self_attn.q_proj.base_layer.weight: False
base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight: True
base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight: True
base_model.model.model.layers.18.self_attn.k_proj.base_layer.weight: False
base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight: True
base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight: True
base_model.model.model.layers.18.self_attn.v_proj.base_layer.weight: False
base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight: True
base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight: True
base_model.model.model.layers.18.self_attn.o_proj.base_layer.weight: False
base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight: True
base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight: True
base_model.model.model.layers.18.mlp.gate_proj.base_layer.weight: False
base_model.model.model.layers.18.mlp.gate_proj.lora_A.default.weight: True
base_model.model.model.layers.18.mlp.gate_proj.lora_B.default.weight: True
base_model.model.model.layers.18.mlp.up_proj.base_layer.weight: False
base_model.model.model.layers.18.mlp.up_proj.lora_A.default.weight: True
base_model.model.model.layers.18.mlp.up_proj.lora_B.default.weight: True
base_model.model.model.layers.18.mlp.down_proj.base_layer.weight: False
base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight: True
base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight: True
base_model.model.model.layers.18.input_layernorm.weight: False
base_model.model.model.layers.18.post_attention_layernorm.weight: False
base_model.model.model.layers.19.self_attn.q_proj.base_layer.weight: False
base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight: True
base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight: True
base_model.model.model.layers.19.self_attn.k_proj.base_layer.weight: False
base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight: True
base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight: True
base_model.model.model.layers.19.self_attn.v_proj.base_layer.weight: False
base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight: True
base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight: True
base_model.model.model.layers.19.self_attn.o_proj.base_layer.weight: False
base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight: True
base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight: True
base_model.model.model.layers.19.mlp.gate_proj.base_layer.weight: False
base_model.model.model.layers.19.mlp.gate_proj.lora_A.default.weight: True
base_model.model.model.layers.19.mlp.gate_proj.lora_B.default.weight: True
base_model.model.model.layers.19.mlp.up_proj.base_layer.weight: False
base_model.model.model.layers.19.mlp.up_proj.lora_A.default.weight: True
base_model.model.model.layers.19.mlp.up_proj.lora_B.default.weight: True
base_model.model.model.layers.19.mlp.down_proj.base_layer.weight: False
base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight: True
base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight: True
base_model.model.model.layers.19.input_layernorm.weight: False
base_model.model.model.layers.19.post_attention_layernorm.weight: False
base_model.model.model.layers.20.self_attn.q_proj.base_layer.weight: False
base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight: True
base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight: True
base_model.model.model.layers.20.self_attn.k_proj.base_layer.weight: False
base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight: True
base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight: True
base_model.model.model.layers.20.self_attn.v_proj.base_layer.weight: False
base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight: True
base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight: True
base_model.model.model.layers.20.self_attn.o_proj.base_layer.weight: False
base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight: True
base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight: True
base_model.model.model.layers.20.mlp.gate_proj.base_layer.weight: False
base_model.model.model.layers.20.mlp.gate_proj.lora_A.default.weight: True
base_model.model.model.layers.20.mlp.gate_proj.lora_B.default.weight: True
base_model.model.model.layers.20.mlp.up_proj.base_layer.weight: False
base_model.model.model.layers.20.mlp.up_proj.lora_A.default.weight: True
base_model.model.model.layers.20.mlp.up_proj.lora_B.default.weight: True
base_model.model.model.layers.20.mlp.down_proj.base_layer.weight: False
base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight: True
base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight: True
base_model.model.model.layers.20.input_layernorm.weight: False
base_model.model.model.layers.20.post_attention_layernorm.weight: False
base_model.model.model.layers.21.self_attn.q_proj.base_layer.weight: False
base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight: True
base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight: True
base_model.model.model.layers.21.self_attn.k_proj.base_layer.weight: False
base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight: True
base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight: True
base_model.model.model.layers.21.self_attn.v_proj.base_layer.weight: False
base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight: True
base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight: True
base_model.model.model.layers.21.self_attn.o_proj.base_layer.weight: False
base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight: True
base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight: True
base_model.model.model.layers.21.mlp.gate_proj.base_layer.weight: False
base_model.model.model.layers.21.mlp.gate_proj.lora_A.default.weight: True
base_model.model.model.layers.21.mlp.gate_proj.lora_B.default.weight: True
base_model.model.model.layers.21.mlp.up_proj.base_layer.weight: False
base_model.model.model.layers.21.mlp.up_proj.lora_A.default.weight: True
base_model.model.model.layers.21.mlp.up_proj.lora_B.default.weight: True
base_model.model.model.layers.21.mlp.down_proj.base_layer.weight: False
base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight: True
base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight: True
base_model.model.model.layers.21.input_layernorm.weight: False
base_model.model.model.layers.21.post_attention_layernorm.weight: False
base_model.model.model.layers.22.self_attn.q_proj.base_layer.weight: False
base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight: True
base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight: True
base_model.model.model.layers.22.self_attn.k_proj.base_layer.weight: False
base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight: True
base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight: True
base_model.model.model.layers.22.self_attn.v_proj.base_layer.weight: False
base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight: True
base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight: True
base_model.model.model.layers.22.self_attn.o_proj.base_layer.weight: False
base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight: True
base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight: True
base_model.model.model.layers.22.mlp.gate_proj.base_layer.weight: False
base_model.model.model.layers.22.mlp.gate_proj.lora_A.default.weight: True
base_model.model.model.layers.22.mlp.gate_proj.lora_B.default.weight: True
base_model.model.model.layers.22.mlp.up_proj.base_layer.weight: False
base_model.model.model.layers.22.mlp.up_proj.lora_A.default.weight: True
base_model.model.model.layers.22.mlp.up_proj.lora_B.default.weight: True
base_model.model.model.layers.22.mlp.down_proj.base_layer.weight: False
base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight: True
base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight: True
base_model.model.model.layers.22.input_layernorm.weight: False
base_model.model.model.layers.22.post_attention_layernorm.weight: False
base_model.model.model.layers.23.self_attn.q_proj.base_layer.weight: False
base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight: True
base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight: True
base_model.model.model.layers.23.self_attn.k_proj.base_layer.weight: False
base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight: True
base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight: True
base_model.model.model.layers.23.self_attn.v_proj.base_layer.weight: False
base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight: True
base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight: True
base_model.model.model.layers.23.self_attn.o_proj.base_layer.weight: False
base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight: True
base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight: True
base_model.model.model.layers.23.mlp.gate_proj.base_layer.weight: False
base_model.model.model.layers.23.mlp.gate_proj.lora_A.default.weight: True
base_model.model.model.layers.23.mlp.gate_proj.lora_B.default.weight: True
base_model.model.model.layers.23.mlp.up_proj.base_layer.weight: False
base_model.model.model.layers.23.mlp.up_proj.lora_A.default.weight: True
base_model.model.model.layers.23.mlp.up_proj.lora_B.default.weight: True
base_model.model.model.layers.23.mlp.down_proj.base_layer.weight: False
base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight: True
base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight: True
base_model.model.model.layers.23.input_layernorm.weight: False
base_model.model.model.layers.23.post_attention_layernorm.weight: False
base_model.model.model.layers.24.self_attn.q_proj.base_layer.weight: False
base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight: True
base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight: True
base_model.model.model.layers.24.self_attn.k_proj.base_layer.weight: False
base_model.model.model.layers.24.self_attn.k_proj.lora_A.default.weight: True
base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight: True
base_model.model.model.layers.24.self_attn.v_proj.base_layer.weight: False
base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight: True
base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight: True
base_model.model.model.layers.24.self_attn.o_proj.base_layer.weight: False
base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight: True
base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight: True
base_model.model.model.layers.24.mlp.gate_proj.base_layer.weight: False
base_model.model.model.layers.24.mlp.gate_proj.lora_A.default.weight: True
base_model.model.model.layers.24.mlp.gate_proj.lora_B.default.weight: True
base_model.model.model.layers.24.mlp.up_proj.base_layer.weight: False
base_model.model.model.layers.24.mlp.up_proj.lora_A.default.weight: True
base_model.model.model.layers.24.mlp.up_proj.lora_B.default.weight: True
base_model.model.model.layers.24.mlp.down_proj.base_layer.weight: False
base_model.model.model.layers.24.mlp.down_proj.lora_A.default.weight: True
base_model.model.model.layers.24.mlp.down_proj.lora_B.default.weight: True
base_model.model.model.layers.24.input_layernorm.weight: False
base_model.model.model.layers.24.post_attention_layernorm.weight: False
base_model.model.model.layers.25.self_attn.q_proj.base_layer.weight: False
base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight: True
base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight: True
base_model.model.model.layers.25.self_attn.k_proj.base_layer.weight: False
base_model.model.model.layers.25.self_attn.k_proj.lora_A.default.weight: True
base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight: True
base_model.model.model.layers.25.self_attn.v_proj.base_layer.weight: False
base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight: True
base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight: True
base_model.model.model.layers.25.self_attn.o_proj.base_layer.weight: False
base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight: True
base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight: True
base_model.model.model.layers.25.mlp.gate_proj.base_layer.weight: False
base_model.model.model.layers.25.mlp.gate_proj.lora_A.default.weight: True
base_model.model.model.layers.25.mlp.gate_proj.lora_B.default.weight: True
base_model.model.model.layers.25.mlp.up_proj.base_layer.weight: False
base_model.model.model.layers.25.mlp.up_proj.lora_A.default.weight: True
base_model.model.model.layers.25.mlp.up_proj.lora_B.default.weight: True
base_model.model.model.layers.25.mlp.down_proj.base_layer.weight: False
base_model.model.model.layers.25.mlp.down_proj.lora_A.default.weight: True
base_model.model.model.layers.25.mlp.down_proj.lora_B.default.weight: True
base_model.model.model.layers.25.input_layernorm.weight: False
base_model.model.model.layers.25.post_attention_layernorm.weight: False
base_model.model.model.layers.26.self_attn.q_proj.base_layer.weight: False
base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight: True
base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight: True
base_model.model.model.layers.26.self_attn.k_proj.base_layer.weight: False
base_model.model.model.layers.26.self_attn.k_proj.lora_A.default.weight: True
base_model.model.model.layers.26.self_attn.k_proj.lora_B.default.weight: True
base_model.model.model.layers.26.self_attn.v_proj.base_layer.weight: False
base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight: True
base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight: True
base_model.model.model.layers.26.self_attn.o_proj.base_layer.weight: False
base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight: True
base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight: True
base_model.model.model.layers.26.mlp.gate_proj.base_layer.weight: False
base_model.model.model.layers.26.mlp.gate_proj.lora_A.default.weight: True
base_model.model.model.layers.26.mlp.gate_proj.lora_B.default.weight: True
base_model.model.model.layers.26.mlp.up_proj.base_layer.weight: False
base_model.model.model.layers.26.mlp.up_proj.lora_A.default.weight: True
base_model.model.model.layers.26.mlp.up_proj.lora_B.default.weight: True
base_model.model.model.layers.26.mlp.down_proj.base_layer.weight: False
base_model.model.model.layers.26.mlp.down_proj.lora_A.default.weight: True
base_model.model.model.layers.26.mlp.down_proj.lora_B.default.weight: True
base_model.model.model.layers.26.input_layernorm.weight: False
base_model.model.model.layers.26.post_attention_layernorm.weight: False
base_model.model.model.layers.27.self_attn.q_proj.base_layer.weight: False
base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight: True
base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight: True
base_model.model.model.layers.27.self_attn.k_proj.base_layer.weight: False
base_model.model.model.layers.27.self_attn.k_proj.lora_A.default.weight: True
base_model.model.model.layers.27.self_attn.k_proj.lora_B.default.weight: True
base_model.model.model.layers.27.self_attn.v_proj.base_layer.weight: False
base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight: True
base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight: True
base_model.model.model.layers.27.self_attn.o_proj.base_layer.weight: False
base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight: True
base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight: True
base_model.model.model.layers.27.mlp.gate_proj.base_layer.weight: False
base_model.model.model.layers.27.mlp.gate_proj.lora_A.default.weight: True
base_model.model.model.layers.27.mlp.gate_proj.lora_B.default.weight: True
base_model.model.model.layers.27.mlp.up_proj.base_layer.weight: False
base_model.model.model.layers.27.mlp.up_proj.lora_A.default.weight: True
base_model.model.model.layers.27.mlp.up_proj.lora_B.default.weight: True
base_model.model.model.layers.27.mlp.down_proj.base_layer.weight: False
base_model.model.model.layers.27.mlp.down_proj.lora_A.default.weight: True
base_model.model.model.layers.27.mlp.down_proj.lora_B.default.weight: True
base_model.model.model.layers.27.input_layernorm.weight: False
base_model.model.model.layers.27.post_attention_layernorm.weight: False
base_model.model.model.layers.28.self_attn.q_proj.base_layer.weight: False
base_model.model.model.layers.28.self_attn.q_proj.lora_A.default.weight: True
base_model.model.model.layers.28.self_attn.q_proj.lora_B.default.weight: True
base_model.model.model.layers.28.self_attn.k_proj.base_layer.weight: False
base_model.model.model.layers.28.self_attn.k_proj.lora_A.default.weight: True
base_model.model.model.layers.28.self_attn.k_proj.lora_B.default.weight: True
base_model.model.model.layers.28.self_attn.v_proj.base_layer.weight: False
base_model.model.model.layers.28.self_attn.v_proj.lora_A.default.weight: True
base_model.model.model.layers.28.self_attn.v_proj.lora_B.default.weight: True
base_model.model.model.layers.28.self_attn.o_proj.base_layer.weight: False
base_model.model.model.layers.28.self_attn.o_proj.lora_A.default.weight: True
base_model.model.model.layers.28.self_attn.o_proj.lora_B.default.weight: True
base_model.model.model.layers.28.mlp.gate_proj.base_layer.weight: False
base_model.model.model.layers.28.mlp.gate_proj.lora_A.default.weight: True
base_model.model.model.layers.28.mlp.gate_proj.lora_B.default.weight: True
base_model.model.model.layers.28.mlp.up_proj.base_layer.weight: False
base_model.model.model.layers.28.mlp.up_proj.lora_A.default.weight: True
base_model.model.model.layers.28.mlp.up_proj.lora_B.default.weight: True
base_model.model.model.layers.28.mlp.down_proj.base_layer.weight: False
base_model.model.model.layers.28.mlp.down_proj.lora_A.default.weight: True
base_model.model.model.layers.28.mlp.down_proj.lora_B.default.weight: True
base_model.model.model.layers.28.input_layernorm.weight: False
base_model.model.model.layers.28.post_attention_layernorm.weight: False
base_model.model.model.layers.29.self_attn.q_proj.base_layer.weight: False
base_model.model.model.layers.29.self_attn.q_proj.lora_A.default.weight: True
base_model.model.model.layers.29.self_attn.q_proj.lora_B.default.weight: True
base_model.model.model.layers.29.self_attn.k_proj.base_layer.weight: False
base_model.model.model.layers.29.self_attn.k_proj.lora_A.default.weight: True
base_model.model.model.layers.29.self_attn.k_proj.lora_B.default.weight: True
base_model.model.model.layers.29.self_attn.v_proj.base_layer.weight: False
base_model.model.model.layers.29.self_attn.v_proj.lora_A.default.weight: True
base_model.model.model.layers.29.self_attn.v_proj.lora_B.default.weight: True
base_model.model.model.layers.29.self_attn.o_proj.base_layer.weight: False
base_model.model.model.layers.29.self_attn.o_proj.lora_A.default.weight: True
base_model.model.model.layers.29.self_attn.o_proj.lora_B.default.weight: True
base_model.model.model.layers.29.mlp.gate_proj.base_layer.weight: False
base_model.model.model.layers.29.mlp.gate_proj.lora_A.default.weight: True
base_model.model.model.layers.29.mlp.gate_proj.lora_B.default.weight: True
base_model.model.model.layers.29.mlp.up_proj.base_layer.weight: False
base_model.model.model.layers.29.mlp.up_proj.lora_A.default.weight: True
base_model.model.model.layers.29.mlp.up_proj.lora_B.default.weight: True
base_model.model.model.layers.29.mlp.down_proj.base_layer.weight: False
base_model.model.model.layers.29.mlp.down_proj.lora_A.default.weight: True
base_model.model.model.layers.29.mlp.down_proj.lora_B.default.weight: True
base_model.model.model.layers.29.input_layernorm.weight: False
base_model.model.model.layers.29.post_attention_layernorm.weight: False
base_model.model.model.layers.30.self_attn.q_proj.base_layer.weight: False
base_model.model.model.layers.30.self_attn.q_proj.lora_A.default.weight: True
base_model.model.model.layers.30.self_attn.q_proj.lora_B.default.weight: True
base_model.model.model.layers.30.self_attn.k_proj.base_layer.weight: False
base_model.model.model.layers.30.self_attn.k_proj.lora_A.default.weight: True
base_model.model.model.layers.30.self_attn.k_proj.lora_B.default.weight: True
base_model.model.model.layers.30.self_attn.v_proj.base_layer.weight: False
base_model.model.model.layers.30.self_attn.v_proj.lora_A.default.weight: True
base_model.model.model.layers.30.self_attn.v_proj.lora_B.default.weight: True
base_model.model.model.layers.30.self_attn.o_proj.base_layer.weight: False
base_model.model.model.layers.30.self_attn.o_proj.lora_A.default.weight: True
base_model.model.model.layers.30.self_attn.o_proj.lora_B.default.weight: True
base_model.model.model.layers.30.mlp.gate_proj.base_layer.weight: False
base_model.model.model.layers.30.mlp.gate_proj.lora_A.default.weight: True
base_model.model.model.layers.30.mlp.gate_proj.lora_B.default.weight: True
base_model.model.model.layers.30.mlp.up_proj.base_layer.weight: False
base_model.model.model.layers.30.mlp.up_proj.lora_A.default.weight: True
base_model.model.model.layers.30.mlp.up_proj.lora_B.default.weight: True
base_model.model.model.layers.30.mlp.down_proj.base_layer.weight: False
base_model.model.model.layers.30.mlp.down_proj.lora_A.default.weight: True
base_model.model.model.layers.30.mlp.down_proj.lora_B.default.weight: True
base_model.model.model.layers.30.input_layernorm.weight: False
base_model.model.model.layers.30.post_attention_layernorm.weight: False
base_model.model.model.layers.31.self_attn.q_proj.base_layer.weight: False
base_model.model.model.layers.31.self_attn.q_proj.lora_A.default.weight: True
base_model.model.model.layers.31.self_attn.q_proj.lora_B.default.weight: True
base_model.model.model.layers.31.self_attn.k_proj.base_layer.weight: False
base_model.model.model.layers.31.self_attn.k_proj.lora_A.default.weight: True
base_model.model.model.layers.31.self_attn.k_proj.lora_B.default.weight: True
base_model.model.model.layers.31.self_attn.v_proj.base_layer.weight: False
base_model.model.model.layers.31.self_attn.v_proj.lora_A.default.weight: True
base_model.model.model.layers.31.self_attn.v_proj.lora_B.default.weight: True
base_model.model.model.layers.31.self_attn.o_proj.base_layer.weight: False
base_model.model.model.layers.31.self_attn.o_proj.lora_A.default.weight: True
base_model.model.model.layers.31.self_attn.o_proj.lora_B.default.weight: True
base_model.model.model.layers.31.mlp.gate_proj.base_layer.weight: False
base_model.model.model.layers.31.mlp.gate_proj.lora_A.default.weight: True
base_model.model.model.layers.31.mlp.gate_proj.lora_B.default.weight: True
base_model.model.model.layers.31.mlp.up_proj.base_layer.weight: False
base_model.model.model.layers.31.mlp.up_proj.lora_A.default.weight: True
base_model.model.model.layers.31.mlp.up_proj.lora_B.default.weight: True
base_model.model.model.layers.31.mlp.down_proj.base_layer.weight: False
base_model.model.model.layers.31.mlp.down_proj.lora_A.default.weight: True
base_model.model.model.layers.31.mlp.down_proj.lora_B.default.weight: True
base_model.model.model.layers.31.input_layernorm.weight: False
base_model.model.model.layers.31.post_attention_layernorm.weight: False
base_model.model.model.norm.weight: False
base_model.model.lm_head.weight: False
INFO:__main__:Model embedding size after loading: 128256
=======Model loaded and ready for training========
You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding dimension will be 128257. This might induce some performance reduction as *Tensor Cores* will not be available. For more details about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc
/workspace/tulu_env/lib/python3.10/site-packages/peft/utils/save_and_load.py:473: UserWarning: Some weights of PeftModelForCausalLM were not initialized from the model checkpoint and are being ignored because you passed `ignore_mismatched_sizes=True`: - base_model.model.lm_head.weight: found shape torch.Size([128264, 4096]) in the checkpoint and torch.Size([128256, 4096]) in the model instantiated
- base_model.model.model.embed_tokens.weight: found shape torch.Size([128264, 4096]) in the checkpoint and torch.Size([128256, 4096]) in the model instantiated.
  warnings.warn(msg)
=========qloar  model is... ========
PeftModelForCausalLM(
  (base_model): LoraModel(
    (model): LlamaForCausalLM(
      (model): LlamaModel(
        (embed_tokens): Embedding(128256, 4096)
        (layers): ModuleList(
          (0-31): 32 x LlamaDecoderLayer(
            (self_attn): LlamaFlashAttention2(
              (q_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=64, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=64, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (k_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=64, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=64, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (v_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=64, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=64, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (o_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=64, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=64, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (rotary_emb): LlamaRotaryEmbedding()
            )
            (mlp): LlamaMLP(
              (gate_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=64, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=64, out_features=14336, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (up_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=64, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=64, out_features=14336, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (down_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=14336, out_features=64, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=64, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (act_fn): SiLU()
            )
            (input_layernorm): LlamaRMSNorm()
            (post_attention_layernorm): LlamaRMSNorm()
          )
        )
        (norm): LlamaRMSNorm()
        (rotary_emb): LlamaRotaryEmbedding()
      )
      (lm_head): Linear(in_features=4096, out_features=128256, bias=False)
    )
  )
)
trainable params: 167,772,160 || all params: 8,198,033,408 || trainable%: 2.0465
Base model params: 4708372480
LoRA params: 167772160
Total model params: 4876144640
base_model.model.model.embed_tokens.weight: False
base_model.model.model.layers.0.self_attn.q_proj.base_layer.weight: False
base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight: True
base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight: True
base_model.model.model.layers.0.self_attn.k_proj.base_layer.weight: False
base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight: True
base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight: True
base_model.model.model.layers.0.self_attn.v_proj.base_layer.weight: False
base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight: True
base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight: True
base_model.model.model.layers.0.self_attn.o_proj.base_layer.weight: False
base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight: True
base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight: True
base_model.model.model.layers.0.mlp.gate_proj.base_layer.weight: False
base_model.model.model.layers.0.mlp.gate_proj.lora_A.default.weight: True
base_model.model.model.layers.0.mlp.gate_proj.lora_B.default.weight: True
base_model.model.model.layers.0.mlp.up_proj.base_layer.weight: False
base_model.model.model.layers.0.mlp.up_proj.lora_A.default.weight: True
base_model.model.model.layers.0.mlp.up_proj.lora_B.default.weight: True
base_model.model.model.layers.0.mlp.down_proj.base_layer.weight: False
base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight: True
base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight: True
base_model.model.model.layers.0.input_layernorm.weight: False
base_model.model.model.layers.0.post_attention_layernorm.weight: False
base_model.model.model.layers.1.self_attn.q_proj.base_layer.weight: False
base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight: True
base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight: True
base_model.model.model.layers.1.self_attn.k_proj.base_layer.weight: False
base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight: True
base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight: True
base_model.model.model.layers.1.self_attn.v_proj.base_layer.weight: False
base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight: True
base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight: True
base_model.model.model.layers.1.self_attn.o_proj.base_layer.weight: False
base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight: True
base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight: True
base_model.model.model.layers.1.mlp.gate_proj.base_layer.weight: False
base_model.model.model.layers.1.mlp.gate_proj.lora_A.default.weight: True
base_model.model.model.layers.1.mlp.gate_proj.lora_B.default.weight: True
base_model.model.model.layers.1.mlp.up_proj.base_layer.weight: False
base_model.model.model.layers.1.mlp.up_proj.lora_A.default.weight: True
base_model.model.model.layers.1.mlp.up_proj.lora_B.default.weight: True
base_model.model.model.layers.1.mlp.down_proj.base_layer.weight: False
base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight: True
base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight: True
base_model.model.model.layers.1.input_layernorm.weight: False
base_model.model.model.layers.1.post_attention_layernorm.weight: False
base_model.model.model.layers.2.self_attn.q_proj.base_layer.weight: False
base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight: True
base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight: True
base_model.model.model.layers.2.self_attn.k_proj.base_layer.weight: False
base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight: True
base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight: True
base_model.model.model.layers.2.self_attn.v_proj.base_layer.weight: False
base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight: True
base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight: True
base_model.model.model.layers.2.self_attn.o_proj.base_layer.weight: False
base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight: True
base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight: True
base_model.model.model.layers.2.mlp.gate_proj.base_layer.weight: False
base_model.model.model.layers.2.mlp.gate_proj.lora_A.default.weight: True
base_model.model.model.layers.2.mlp.gate_proj.lora_B.default.weight: True
base_model.model.model.layers.2.mlp.up_proj.base_layer.weight: False
base_model.model.model.layers.2.mlp.up_proj.lora_A.default.weight: True
base_model.model.model.layers.2.mlp.up_proj.lora_B.default.weight: True
base_model.model.model.layers.2.mlp.down_proj.base_layer.weight: False
base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight: True
base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight: True
base_model.model.model.layers.2.input_layernorm.weight: False
base_model.model.model.layers.2.post_attention_layernorm.weight: False
base_model.model.model.layers.3.self_attn.q_proj.base_layer.weight: False
base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight: True
base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight: True
base_model.model.model.layers.3.self_attn.k_proj.base_layer.weight: False
base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight: True
base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight: True
base_model.model.model.layers.3.self_attn.v_proj.base_layer.weight: False
base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight: True
base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight: True
base_model.model.model.layers.3.self_attn.o_proj.base_layer.weight: False
base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight: True
base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight: True
base_model.model.model.layers.3.mlp.gate_proj.base_layer.weight: False
base_model.model.model.layers.3.mlp.gate_proj.lora_A.default.weight: True
base_model.model.model.layers.3.mlp.gate_proj.lora_B.default.weight: True
base_model.model.model.layers.3.mlp.up_proj.base_layer.weight: False
base_model.model.model.layers.3.mlp.up_proj.lora_A.default.weight: True
base_model.model.model.layers.3.mlp.up_proj.lora_B.default.weight: True
base_model.model.model.layers.3.mlp.down_proj.base_layer.weight: False
base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight: True
base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight: True
base_model.model.model.layers.3.input_layernorm.weight: False
base_model.model.model.layers.3.post_attention_layernorm.weight: False
base_model.model.model.layers.4.self_attn.q_proj.base_layer.weight: False
base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight: True
base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight: True
base_model.model.model.layers.4.self_attn.k_proj.base_layer.weight: False
base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight: True
base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight: True
base_model.model.model.layers.4.self_attn.v_proj.base_layer.weight: False
base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight: True
base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight: True
base_model.model.model.layers.4.self_attn.o_proj.base_layer.weight: False
base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight: True
base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight: True
base_model.model.model.layers.4.mlp.gate_proj.base_layer.weight: False
base_model.model.model.layers.4.mlp.gate_proj.lora_A.default.weight: True
base_model.model.model.layers.4.mlp.gate_proj.lora_B.default.weight: True
base_model.model.model.layers.4.mlp.up_proj.base_layer.weight: False
base_model.model.model.layers.4.mlp.up_proj.lora_A.default.weight: True
base_model.model.model.layers.4.mlp.up_proj.lora_B.default.weight: True
base_model.model.model.layers.4.mlp.down_proj.base_layer.weight: False
base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight: True
base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight: True
base_model.model.model.layers.4.input_layernorm.weight: False
base_model.model.model.layers.4.post_attention_layernorm.weight: False
base_model.model.model.layers.5.self_attn.q_proj.base_layer.weight: False
base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight: True
base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight: True
base_model.model.model.layers.5.self_attn.k_proj.base_layer.weight: False
base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight: True
base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight: True
base_model.model.model.layers.5.self_attn.v_proj.base_layer.weight: False
base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight: True
base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight: True
base_model.model.model.layers.5.self_attn.o_proj.base_layer.weight: False
base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight: True
base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight: True
base_model.model.model.layers.5.mlp.gate_proj.base_layer.weight: False
base_model.model.model.layers.5.mlp.gate_proj.lora_A.default.weight: True
base_model.model.model.layers.5.mlp.gate_proj.lora_B.default.weight: True
base_model.model.model.layers.5.mlp.up_proj.base_layer.weight: False
base_model.model.model.layers.5.mlp.up_proj.lora_A.default.weight: True
base_model.model.model.layers.5.mlp.up_proj.lora_B.default.weight: True
base_model.model.model.layers.5.mlp.down_proj.base_layer.weight: False
base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight: True
base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight: True
base_model.model.model.layers.5.input_layernorm.weight: False
base_model.model.model.layers.5.post_attention_layernorm.weight: False
base_model.model.model.layers.6.self_attn.q_proj.base_layer.weight: False
base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight: True
base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight: True
base_model.model.model.layers.6.self_attn.k_proj.base_layer.weight: False
base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight: True
base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight: True
base_model.model.model.layers.6.self_attn.v_proj.base_layer.weight: False
base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight: True
base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight: True
base_model.model.model.layers.6.self_attn.o_proj.base_layer.weight: False
base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight: True
base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight: True
base_model.model.model.layers.6.mlp.gate_proj.base_layer.weight: False
base_model.model.model.layers.6.mlp.gate_proj.lora_A.default.weight: True
base_model.model.model.layers.6.mlp.gate_proj.lora_B.default.weight: True
base_model.model.model.layers.6.mlp.up_proj.base_layer.weight: False
base_model.model.model.layers.6.mlp.up_proj.lora_A.default.weight: True
base_model.model.model.layers.6.mlp.up_proj.lora_B.default.weight: True
base_model.model.model.layers.6.mlp.down_proj.base_layer.weight: False
base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight: True
base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight: True
base_model.model.model.layers.6.input_layernorm.weight: False
base_model.model.model.layers.6.post_attention_layernorm.weight: False
base_model.model.model.layers.7.self_attn.q_proj.base_layer.weight: False
base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight: True
base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight: True
base_model.model.model.layers.7.self_attn.k_proj.base_layer.weight: False
base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight: True
base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight: True
base_model.model.model.layers.7.self_attn.v_proj.base_layer.weight: False
base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight: True
base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight: True
base_model.model.model.layers.7.self_attn.o_proj.base_layer.weight: False
base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight: True
base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight: True
base_model.model.model.layers.7.mlp.gate_proj.base_layer.weight: False
base_model.model.model.layers.7.mlp.gate_proj.lora_A.default.weight: True
base_model.model.model.layers.7.mlp.gate_proj.lora_B.default.weight: True
base_model.model.model.layers.7.mlp.up_proj.base_layer.weight: False
base_model.model.model.layers.7.mlp.up_proj.lora_A.default.weight: True
base_model.model.model.layers.7.mlp.up_proj.lora_B.default.weight: True
base_model.model.model.layers.7.mlp.down_proj.base_layer.weight: False
base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight: True
base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight: True
base_model.model.model.layers.7.input_layernorm.weight: False
base_model.model.model.layers.7.post_attention_layernorm.weight: False
base_model.model.model.layers.8.self_attn.q_proj.base_layer.weight: False
base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight: True
base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight: True
base_model.model.model.layers.8.self_attn.k_proj.base_layer.weight: False
base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight: True
base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight: True
base_model.model.model.layers.8.self_attn.v_proj.base_layer.weight: False
base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight: True
base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight: True
base_model.model.model.layers.8.self_attn.o_proj.base_layer.weight: False
base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight: True
base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight: True
base_model.model.model.layers.8.mlp.gate_proj.base_layer.weight: False
base_model.model.model.layers.8.mlp.gate_proj.lora_A.default.weight: True
base_model.model.model.layers.8.mlp.gate_proj.lora_B.default.weight: True
base_model.model.model.layers.8.mlp.up_proj.base_layer.weight: False
base_model.model.model.layers.8.mlp.up_proj.lora_A.default.weight: True
base_model.model.model.layers.8.mlp.up_proj.lora_B.default.weight: True
base_model.model.model.layers.8.mlp.down_proj.base_layer.weight: False
base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight: True
base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight: True
base_model.model.model.layers.8.input_layernorm.weight: False
base_model.model.model.layers.8.post_attention_layernorm.weight: False
base_model.model.model.layers.9.self_attn.q_proj.base_layer.weight: False
base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight: True
base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight: True
base_model.model.model.layers.9.self_attn.k_proj.base_layer.weight: False
base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight: True
base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight: True
base_model.model.model.layers.9.self_attn.v_proj.base_layer.weight: False
base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight: True
base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight: True
base_model.model.model.layers.9.self_attn.o_proj.base_layer.weight: False
base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight: True
base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight: True
base_model.model.model.layers.9.mlp.gate_proj.base_layer.weight: False
base_model.model.model.layers.9.mlp.gate_proj.lora_A.default.weight: True
base_model.model.model.layers.9.mlp.gate_proj.lora_B.default.weight: True
base_model.model.model.layers.9.mlp.up_proj.base_layer.weight: False
base_model.model.model.layers.9.mlp.up_proj.lora_A.default.weight: True
base_model.model.model.layers.9.mlp.up_proj.lora_B.default.weight: True
base_model.model.model.layers.9.mlp.down_proj.base_layer.weight: False
base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight: True
base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight: True
base_model.model.model.layers.9.input_layernorm.weight: False
base_model.model.model.layers.9.post_attention_layernorm.weight: False
base_model.model.model.layers.10.self_attn.q_proj.base_layer.weight: False
base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight: True
base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight: True
base_model.model.model.layers.10.self_attn.k_proj.base_layer.weight: False
base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight: True
base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight: True
base_model.model.model.layers.10.self_attn.v_proj.base_layer.weight: False
base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight: True
base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight: True
base_model.model.model.layers.10.self_attn.o_proj.base_layer.weight: False
base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight: True
base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight: True
base_model.model.model.layers.10.mlp.gate_proj.base_layer.weight: False
base_model.model.model.layers.10.mlp.gate_proj.lora_A.default.weight: True
base_model.model.model.layers.10.mlp.gate_proj.lora_B.default.weight: True
base_model.model.model.layers.10.mlp.up_proj.base_layer.weight: False
base_model.model.model.layers.10.mlp.up_proj.lora_A.default.weight: True
base_model.model.model.layers.10.mlp.up_proj.lora_B.default.weight: True
base_model.model.model.layers.10.mlp.down_proj.base_layer.weight: False
base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight: True
base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight: True
base_model.model.model.layers.10.input_layernorm.weight: False
base_model.model.model.layers.10.post_attention_layernorm.weight: False
base_model.model.model.layers.11.self_attn.q_proj.base_layer.weight: False
base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight: True
base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight: True
base_model.model.model.layers.11.self_attn.k_proj.base_layer.weight: False
base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight: True
base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight: True
base_model.model.model.layers.11.self_attn.v_proj.base_layer.weight: False
base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight: True
base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight: True
base_model.model.model.layers.11.self_attn.o_proj.base_layer.weight: False
base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight: True
base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight: True
base_model.model.model.layers.11.mlp.gate_proj.base_layer.weight: False
base_model.model.model.layers.11.mlp.gate_proj.lora_A.default.weight: True
base_model.model.model.layers.11.mlp.gate_proj.lora_B.default.weight: True
base_model.model.model.layers.11.mlp.up_proj.base_layer.weight: False
base_model.model.model.layers.11.mlp.up_proj.lora_A.default.weight: True
base_model.model.model.layers.11.mlp.up_proj.lora_B.default.weight: True
base_model.model.model.layers.11.mlp.down_proj.base_layer.weight: False
base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight: True
base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight: True
base_model.model.model.layers.11.input_layernorm.weight: False
base_model.model.model.layers.11.post_attention_layernorm.weight: False
base_model.model.model.layers.12.self_attn.q_proj.base_layer.weight: False
base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight: True
base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight: True
base_model.model.model.layers.12.self_attn.k_proj.base_layer.weight: False
base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight: True
base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight: True
base_model.model.model.layers.12.self_attn.v_proj.base_layer.weight: False
base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight: True
base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight: True
base_model.model.model.layers.12.self_attn.o_proj.base_layer.weight: False
base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight: True
base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight: True
base_model.model.model.layers.12.mlp.gate_proj.base_layer.weight: False
base_model.model.model.layers.12.mlp.gate_proj.lora_A.default.weight: True
base_model.model.model.layers.12.mlp.gate_proj.lora_B.default.weight: True
base_model.model.model.layers.12.mlp.up_proj.base_layer.weight: False
base_model.model.model.layers.12.mlp.up_proj.lora_A.default.weight: True
base_model.model.model.layers.12.mlp.up_proj.lora_B.default.weight: True
base_model.model.model.layers.12.mlp.down_proj.base_layer.weight: False
base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight: True
base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight: True
base_model.model.model.layers.12.input_layernorm.weight: False
base_model.model.model.layers.12.post_attention_layernorm.weight: False
base_model.model.model.layers.13.self_attn.q_proj.base_layer.weight: False
base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight: True
base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight: True
base_model.model.model.layers.13.self_attn.k_proj.base_layer.weight: False
base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight: True
base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight: True
base_model.model.model.layers.13.self_attn.v_proj.base_layer.weight: False
base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight: True
base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight: True
base_model.model.model.layers.13.self_attn.o_proj.base_layer.weight: False
base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight: True
base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight: True
base_model.model.model.layers.13.mlp.gate_proj.base_layer.weight: False
base_model.model.model.layers.13.mlp.gate_proj.lora_A.default.weight: True
base_model.model.model.layers.13.mlp.gate_proj.lora_B.default.weight: True
base_model.model.model.layers.13.mlp.up_proj.base_layer.weight: False
base_model.model.model.layers.13.mlp.up_proj.lora_A.default.weight: True
base_model.model.model.layers.13.mlp.up_proj.lora_B.default.weight: True
base_model.model.model.layers.13.mlp.down_proj.base_layer.weight: False
base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight: True
base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight: True
base_model.model.model.layers.13.input_layernorm.weight: False
base_model.model.model.layers.13.post_attention_layernorm.weight: False
base_model.model.model.layers.14.self_attn.q_proj.base_layer.weight: False
base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight: True
base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight: True
base_model.model.model.layers.14.self_attn.k_proj.base_layer.weight: False
base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight: True
base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight: True
base_model.model.model.layers.14.self_attn.v_proj.base_layer.weight: False
base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight: True
base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight: True
base_model.model.model.layers.14.self_attn.o_proj.base_layer.weight: False
base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight: True
base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight: True
base_model.model.model.layers.14.mlp.gate_proj.base_layer.weight: False
base_model.model.model.layers.14.mlp.gate_proj.lora_A.default.weight: True
base_model.model.model.layers.14.mlp.gate_proj.lora_B.default.weight: True
base_model.model.model.layers.14.mlp.up_proj.base_layer.weight: False
base_model.model.model.layers.14.mlp.up_proj.lora_A.default.weight: True
base_model.model.model.layers.14.mlp.up_proj.lora_B.default.weight: True
base_model.model.model.layers.14.mlp.down_proj.base_layer.weight: False
base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight: True
base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight: True
base_model.model.model.layers.14.input_layernorm.weight: False
base_model.model.model.layers.14.post_attention_layernorm.weight: False
base_model.model.model.layers.15.self_attn.q_proj.base_layer.weight: False
base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight: True
base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight: True
base_model.model.model.layers.15.self_attn.k_proj.base_layer.weight: False
base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight: True
base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight: True
base_model.model.model.layers.15.self_attn.v_proj.base_layer.weight: False
base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight: True
base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight: True
base_model.model.model.layers.15.self_attn.o_proj.base_layer.weight: False
base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight: True
base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight: True
base_model.model.model.layers.15.mlp.gate_proj.base_layer.weight: False
base_model.model.model.layers.15.mlp.gate_proj.lora_A.default.weight: True
base_model.model.model.layers.15.mlp.gate_proj.lora_B.default.weight: True
base_model.model.model.layers.15.mlp.up_proj.base_layer.weight: False
base_model.model.model.layers.15.mlp.up_proj.lora_A.default.weight: True
base_model.model.model.layers.15.mlp.up_proj.lora_B.default.weight: True
base_model.model.model.layers.15.mlp.down_proj.base_layer.weight: False
base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight: True
base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight: True
base_model.model.model.layers.15.input_layernorm.weight: False
base_model.model.model.layers.15.post_attention_layernorm.weight: False
base_model.model.model.layers.16.self_attn.q_proj.base_layer.weight: False
base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight: True
base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight: True
base_model.model.model.layers.16.self_attn.k_proj.base_layer.weight: False
base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight: True
base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight: True
base_model.model.model.layers.16.self_attn.v_proj.base_layer.weight: False
base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight: True
base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight: True
base_model.model.model.layers.16.self_attn.o_proj.base_layer.weight: False
base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight: True
base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight: True
base_model.model.model.layers.16.mlp.gate_proj.base_layer.weight: False
base_model.model.model.layers.16.mlp.gate_proj.lora_A.default.weight: True
base_model.model.model.layers.16.mlp.gate_proj.lora_B.default.weight: True
base_model.model.model.layers.16.mlp.up_proj.base_layer.weight: False
base_model.model.model.layers.16.mlp.up_proj.lora_A.default.weight: True
base_model.model.model.layers.16.mlp.up_proj.lora_B.default.weight: True
base_model.model.model.layers.16.mlp.down_proj.base_layer.weight: False
base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight: True
base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight: True
base_model.model.model.layers.16.input_layernorm.weight: False
base_model.model.model.layers.16.post_attention_layernorm.weight: False
base_model.model.model.layers.17.self_attn.q_proj.base_layer.weight: False
base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight: True
base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight: True
base_model.model.model.layers.17.self_attn.k_proj.base_layer.weight: False
base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight: True
base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight: True
base_model.model.model.layers.17.self_attn.v_proj.base_layer.weight: False
base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight: True
base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight: True
base_model.model.model.layers.17.self_attn.o_proj.base_layer.weight: False
base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight: True
base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight: True
base_model.model.model.layers.17.mlp.gate_proj.base_layer.weight: False
base_model.model.model.layers.17.mlp.gate_proj.lora_A.default.weight: True
base_model.model.model.layers.17.mlp.gate_proj.lora_B.default.weight: True
base_model.model.model.layers.17.mlp.up_proj.base_layer.weight: False
base_model.model.model.layers.17.mlp.up_proj.lora_A.default.weight: True
base_model.model.model.layers.17.mlp.up_proj.lora_B.default.weight: True
base_model.model.model.layers.17.mlp.down_proj.base_layer.weight: False
base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight: True
base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight: True
base_model.model.model.layers.17.input_layernorm.weight: False
base_model.model.model.layers.17.post_attention_layernorm.weight: False
base_model.model.model.layers.18.self_attn.q_proj.base_layer.weight: False
base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight: True
base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight: True
base_model.model.model.layers.18.self_attn.k_proj.base_layer.weight: False
base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight: True
base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight: True
base_model.model.model.layers.18.self_attn.v_proj.base_layer.weight: False
base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight: True
base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight: True
base_model.model.model.layers.18.self_attn.o_proj.base_layer.weight: False
base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight: True
base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight: True
base_model.model.model.layers.18.mlp.gate_proj.base_layer.weight: False
base_model.model.model.layers.18.mlp.gate_proj.lora_A.default.weight: True
base_model.model.model.layers.18.mlp.gate_proj.lora_B.default.weight: True
base_model.model.model.layers.18.mlp.up_proj.base_layer.weight: False
base_model.model.model.layers.18.mlp.up_proj.lora_A.default.weight: True
base_model.model.model.layers.18.mlp.up_proj.lora_B.default.weight: True
base_model.model.model.layers.18.mlp.down_proj.base_layer.weight: False
base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight: True
base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight: True
base_model.model.model.layers.18.input_layernorm.weight: False
base_model.model.model.layers.18.post_attention_layernorm.weight: False
base_model.model.model.layers.19.self_attn.q_proj.base_layer.weight: False
base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight: True
base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight: True
base_model.model.model.layers.19.self_attn.k_proj.base_layer.weight: False
base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight: True
base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight: True
base_model.model.model.layers.19.self_attn.v_proj.base_layer.weight: False
base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight: True
base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight: True
base_model.model.model.layers.19.self_attn.o_proj.base_layer.weight: False
base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight: True
base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight: True
base_model.model.model.layers.19.mlp.gate_proj.base_layer.weight: False
base_model.model.model.layers.19.mlp.gate_proj.lora_A.default.weight: True
base_model.model.model.layers.19.mlp.gate_proj.lora_B.default.weight: True
base_model.model.model.layers.19.mlp.up_proj.base_layer.weight: False
base_model.model.model.layers.19.mlp.up_proj.lora_A.default.weight: True
base_model.model.model.layers.19.mlp.up_proj.lora_B.default.weight: True
base_model.model.model.layers.19.mlp.down_proj.base_layer.weight: False
base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight: True
base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight: True
base_model.model.model.layers.19.input_layernorm.weight: False
base_model.model.model.layers.19.post_attention_layernorm.weight: False
base_model.model.model.layers.20.self_attn.q_proj.base_layer.weight: False
base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight: True
base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight: True
base_model.model.model.layers.20.self_attn.k_proj.base_layer.weight: False
base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight: True
base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight: True
base_model.model.model.layers.20.self_attn.v_proj.base_layer.weight: False
base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight: True
base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight: True
base_model.model.model.layers.20.self_attn.o_proj.base_layer.weight: False
base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight: True
base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight: True
base_model.model.model.layers.20.mlp.gate_proj.base_layer.weight: False
base_model.model.model.layers.20.mlp.gate_proj.lora_A.default.weight: True
base_model.model.model.layers.20.mlp.gate_proj.lora_B.default.weight: True
base_model.model.model.layers.20.mlp.up_proj.base_layer.weight: False
base_model.model.model.layers.20.mlp.up_proj.lora_A.default.weight: True
base_model.model.model.layers.20.mlp.up_proj.lora_B.default.weight: True
base_model.model.model.layers.20.mlp.down_proj.base_layer.weight: False
base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight: True
base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight: True
base_model.model.model.layers.20.input_layernorm.weight: False
base_model.model.model.layers.20.post_attention_layernorm.weight: False
base_model.model.model.layers.21.self_attn.q_proj.base_layer.weight: False
base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight: True
base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight: True
base_model.model.model.layers.21.self_attn.k_proj.base_layer.weight: False
base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight: True
base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight: True
base_model.model.model.layers.21.self_attn.v_proj.base_layer.weight: False
base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight: True
base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight: True
base_model.model.model.layers.21.self_attn.o_proj.base_layer.weight: False
base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight: True
base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight: True
base_model.model.model.layers.21.mlp.gate_proj.base_layer.weight: False
base_model.model.model.layers.21.mlp.gate_proj.lora_A.default.weight: True
base_model.model.model.layers.21.mlp.gate_proj.lora_B.default.weight: True
base_model.model.model.layers.21.mlp.up_proj.base_layer.weight: False
base_model.model.model.layers.21.mlp.up_proj.lora_A.default.weight: True
base_model.model.model.layers.21.mlp.up_proj.lora_B.default.weight: True
base_model.model.model.layers.21.mlp.down_proj.base_layer.weight: False
base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight: True
base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight: True
base_model.model.model.layers.21.input_layernorm.weight: False
base_model.model.model.layers.21.post_attention_layernorm.weight: False
base_model.model.model.layers.22.self_attn.q_proj.base_layer.weight: False
base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight: True
base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight: True
base_model.model.model.layers.22.self_attn.k_proj.base_layer.weight: False
base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight: True
base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight: True
base_model.model.model.layers.22.self_attn.v_proj.base_layer.weight: False
base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight: True
base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight: True
base_model.model.model.layers.22.self_attn.o_proj.base_layer.weight: False
base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight: True
base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight: True
base_model.model.model.layers.22.mlp.gate_proj.base_layer.weight: False
base_model.model.model.layers.22.mlp.gate_proj.lora_A.default.weight: True
base_model.model.model.layers.22.mlp.gate_proj.lora_B.default.weight: True
base_model.model.model.layers.22.mlp.up_proj.base_layer.weight: False
base_model.model.model.layers.22.mlp.up_proj.lora_A.default.weight: True
base_model.model.model.layers.22.mlp.up_proj.lora_B.default.weight: True
base_model.model.model.layers.22.mlp.down_proj.base_layer.weight: False
base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight: True
base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight: True
base_model.model.model.layers.22.input_layernorm.weight: False
base_model.model.model.layers.22.post_attention_layernorm.weight: False
base_model.model.model.layers.23.self_attn.q_proj.base_layer.weight: False
base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight: True
base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight: True
base_model.model.model.layers.23.self_attn.k_proj.base_layer.weight: False
base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight: True
base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight: True
base_model.model.model.layers.23.self_attn.v_proj.base_layer.weight: False
base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight: True
base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight: True
base_model.model.model.layers.23.self_attn.o_proj.base_layer.weight: False
base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight: True
base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight: True
base_model.model.model.layers.23.mlp.gate_proj.base_layer.weight: False
base_model.model.model.layers.23.mlp.gate_proj.lora_A.default.weight: True
base_model.model.model.layers.23.mlp.gate_proj.lora_B.default.weight: True
base_model.model.model.layers.23.mlp.up_proj.base_layer.weight: False
base_model.model.model.layers.23.mlp.up_proj.lora_A.default.weight: True
base_model.model.model.layers.23.mlp.up_proj.lora_B.default.weight: True
base_model.model.model.layers.23.mlp.down_proj.base_layer.weight: False
base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight: True
base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight: True
base_model.model.model.layers.23.input_layernorm.weight: False
base_model.model.model.layers.23.post_attention_layernorm.weight: False
base_model.model.model.layers.24.self_attn.q_proj.base_layer.weight: False
base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight: True
base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight: True
base_model.model.model.layers.24.self_attn.k_proj.base_layer.weight: False
base_model.model.model.layers.24.self_attn.k_proj.lora_A.default.weight: True
base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight: True
base_model.model.model.layers.24.self_attn.v_proj.base_layer.weight: False
base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight: True
base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight: True
base_model.model.model.layers.24.self_attn.o_proj.base_layer.weight: False
base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight: True
base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight: True
base_model.model.model.layers.24.mlp.gate_proj.base_layer.weight: False
base_model.model.model.layers.24.mlp.gate_proj.lora_A.default.weight: True
base_model.model.model.layers.24.mlp.gate_proj.lora_B.default.weight: True
base_model.model.model.layers.24.mlp.up_proj.base_layer.weight: False
base_model.model.model.layers.24.mlp.up_proj.lora_A.default.weight: True
base_model.model.model.layers.24.mlp.up_proj.lora_B.default.weight: True
base_model.model.model.layers.24.mlp.down_proj.base_layer.weight: False
base_model.model.model.layers.24.mlp.down_proj.lora_A.default.weight: True
base_model.model.model.layers.24.mlp.down_proj.lora_B.default.weight: True
base_model.model.model.layers.24.input_layernorm.weight: False
base_model.model.model.layers.24.post_attention_layernorm.weight: False
base_model.model.model.layers.25.self_attn.q_proj.base_layer.weight: False
base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight: True
base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight: True
base_model.model.model.layers.25.self_attn.k_proj.base_layer.weight: False
base_model.model.model.layers.25.self_attn.k_proj.lora_A.default.weight: True
base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight: True
base_model.model.model.layers.25.self_attn.v_proj.base_layer.weight: False
base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight: True
base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight: True
base_model.model.model.layers.25.self_attn.o_proj.base_layer.weight: False
base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight: True
base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight: True
base_model.model.model.layers.25.mlp.gate_proj.base_layer.weight: False
base_model.model.model.layers.25.mlp.gate_proj.lora_A.default.weight: True
base_model.model.model.layers.25.mlp.gate_proj.lora_B.default.weight: True
base_model.model.model.layers.25.mlp.up_proj.base_layer.weight: False
base_model.model.model.layers.25.mlp.up_proj.lora_A.default.weight: True
base_model.model.model.layers.25.mlp.up_proj.lora_B.default.weight: True
base_model.model.model.layers.25.mlp.down_proj.base_layer.weight: False
base_model.model.model.layers.25.mlp.down_proj.lora_A.default.weight: True
base_model.model.model.layers.25.mlp.down_proj.lora_B.default.weight: True
base_model.model.model.layers.25.input_layernorm.weight: False
base_model.model.model.layers.25.post_attention_layernorm.weight: False
base_model.model.model.layers.26.self_attn.q_proj.base_layer.weight: False
base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight: True
base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight: True
base_model.model.model.layers.26.self_attn.k_proj.base_layer.weight: False
base_model.model.model.layers.26.self_attn.k_proj.lora_A.default.weight: True
base_model.model.model.layers.26.self_attn.k_proj.lora_B.default.weight: True
base_model.model.model.layers.26.self_attn.v_proj.base_layer.weight: False
base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight: True
base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight: True
base_model.model.model.layers.26.self_attn.o_proj.base_layer.weight: False
base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight: True
base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight: True
base_model.model.model.layers.26.mlp.gate_proj.base_layer.weight: False
base_model.model.model.layers.26.mlp.gate_proj.lora_A.default.weight: True
base_model.model.model.layers.26.mlp.gate_proj.lora_B.default.weight: True
base_model.model.model.layers.26.mlp.up_proj.base_layer.weight: False
base_model.model.model.layers.26.mlp.up_proj.lora_A.default.weight: True
base_model.model.model.layers.26.mlp.up_proj.lora_B.default.weight: True
base_model.model.model.layers.26.mlp.down_proj.base_layer.weight: False
base_model.model.model.layers.26.mlp.down_proj.lora_A.default.weight: True
base_model.model.model.layers.26.mlp.down_proj.lora_B.default.weight: True
base_model.model.model.layers.26.input_layernorm.weight: False
base_model.model.model.layers.26.post_attention_layernorm.weight: False
base_model.model.model.layers.27.self_attn.q_proj.base_layer.weight: False
base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight: True
base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight: True
base_model.model.model.layers.27.self_attn.k_proj.base_layer.weight: False
base_model.model.model.layers.27.self_attn.k_proj.lora_A.default.weight: True
base_model.model.model.layers.27.self_attn.k_proj.lora_B.default.weight: True
base_model.model.model.layers.27.self_attn.v_proj.base_layer.weight: False
base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight: True
base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight: True
base_model.model.model.layers.27.self_attn.o_proj.base_layer.weight: False
base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight: True
base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight: True
base_model.model.model.layers.27.mlp.gate_proj.base_layer.weight: False
base_model.model.model.layers.27.mlp.gate_proj.lora_A.default.weight: True
base_model.model.model.layers.27.mlp.gate_proj.lora_B.default.weight: True
base_model.model.model.layers.27.mlp.up_proj.base_layer.weight: False
base_model.model.model.layers.27.mlp.up_proj.lora_A.default.weight: True
base_model.model.model.layers.27.mlp.up_proj.lora_B.default.weight: True
base_model.model.model.layers.27.mlp.down_proj.base_layer.weight: False
base_model.model.model.layers.27.mlp.down_proj.lora_A.default.weight: True
base_model.model.model.layers.27.mlp.down_proj.lora_B.default.weight: True
base_model.model.model.layers.27.input_layernorm.weight: False
base_model.model.model.layers.27.post_attention_layernorm.weight: False
base_model.model.model.layers.28.self_attn.q_proj.base_layer.weight: False
base_model.model.model.layers.28.self_attn.q_proj.lora_A.default.weight: True
base_model.model.model.layers.28.self_attn.q_proj.lora_B.default.weight: True
base_model.model.model.layers.28.self_attn.k_proj.base_layer.weight: False
base_model.model.model.layers.28.self_attn.k_proj.lora_A.default.weight: True
base_model.model.model.layers.28.self_attn.k_proj.lora_B.default.weight: True
base_model.model.model.layers.28.self_attn.v_proj.base_layer.weight: False
base_model.model.model.layers.28.self_attn.v_proj.lora_A.default.weight: True
base_model.model.model.layers.28.self_attn.v_proj.lora_B.default.weight: True
base_model.model.model.layers.28.self_attn.o_proj.base_layer.weight: False
base_model.model.model.layers.28.self_attn.o_proj.lora_A.default.weight: True
base_model.model.model.layers.28.self_attn.o_proj.lora_B.default.weight: True
base_model.model.model.layers.28.mlp.gate_proj.base_layer.weight: False
base_model.model.model.layers.28.mlp.gate_proj.lora_A.default.weight: True
base_model.model.model.layers.28.mlp.gate_proj.lora_B.default.weight: True
base_model.model.model.layers.28.mlp.up_proj.base_layer.weight: False
base_model.model.model.layers.28.mlp.up_proj.lora_A.default.weight: True
base_model.model.model.layers.28.mlp.up_proj.lora_B.default.weight: True
base_model.model.model.layers.28.mlp.down_proj.base_layer.weight: False
base_model.model.model.layers.28.mlp.down_proj.lora_A.default.weight: True
base_model.model.model.layers.28.mlp.down_proj.lora_B.default.weight: True
base_model.model.model.layers.28.input_layernorm.weight: False
base_model.model.model.layers.28.post_attention_layernorm.weight: False
base_model.model.model.layers.29.self_attn.q_proj.base_layer.weight: False
base_model.model.model.layers.29.self_attn.q_proj.lora_A.default.weight: True
base_model.model.model.layers.29.self_attn.q_proj.lora_B.default.weight: True
base_model.model.model.layers.29.self_attn.k_proj.base_layer.weight: False
base_model.model.model.layers.29.self_attn.k_proj.lora_A.default.weight: True
base_model.model.model.layers.29.self_attn.k_proj.lora_B.default.weight: True
base_model.model.model.layers.29.self_attn.v_proj.base_layer.weight: False
base_model.model.model.layers.29.self_attn.v_proj.lora_A.default.weight: True
base_model.model.model.layers.29.self_attn.v_proj.lora_B.default.weight: True
base_model.model.model.layers.29.self_attn.o_proj.base_layer.weight: False
base_model.model.model.layers.29.self_attn.o_proj.lora_A.default.weight: True
base_model.model.model.layers.29.self_attn.o_proj.lora_B.default.weight: True
base_model.model.model.layers.29.mlp.gate_proj.base_layer.weight: False
base_model.model.model.layers.29.mlp.gate_proj.lora_A.default.weight: True
base_model.model.model.layers.29.mlp.gate_proj.lora_B.default.weight: True
base_model.model.model.layers.29.mlp.up_proj.base_layer.weight: False
base_model.model.model.layers.29.mlp.up_proj.lora_A.default.weight: True
base_model.model.model.layers.29.mlp.up_proj.lora_B.default.weight: True
base_model.model.model.layers.29.mlp.down_proj.base_layer.weight: False
base_model.model.model.layers.29.mlp.down_proj.lora_A.default.weight: True
base_model.model.model.layers.29.mlp.down_proj.lora_B.default.weight: True
base_model.model.model.layers.29.input_layernorm.weight: False
base_model.model.model.layers.29.post_attention_layernorm.weight: False
base_model.model.model.layers.30.self_attn.q_proj.base_layer.weight: False
base_model.model.model.layers.30.self_attn.q_proj.lora_A.default.weight: True
base_model.model.model.layers.30.self_attn.q_proj.lora_B.default.weight: True
base_model.model.model.layers.30.self_attn.k_proj.base_layer.weight: False
base_model.model.model.layers.30.self_attn.k_proj.lora_A.default.weight: True
base_model.model.model.layers.30.self_attn.k_proj.lora_B.default.weight: True
base_model.model.model.layers.30.self_attn.v_proj.base_layer.weight: False
base_model.model.model.layers.30.self_attn.v_proj.lora_A.default.weight: True
base_model.model.model.layers.30.self_attn.v_proj.lora_B.default.weight: True
base_model.model.model.layers.30.self_attn.o_proj.base_layer.weight: False
base_model.model.model.layers.30.self_attn.o_proj.lora_A.default.weight: True
base_model.model.model.layers.30.self_attn.o_proj.lora_B.default.weight: True
base_model.model.model.layers.30.mlp.gate_proj.base_layer.weight: False
base_model.model.model.layers.30.mlp.gate_proj.lora_A.default.weight: True
base_model.model.model.layers.30.mlp.gate_proj.lora_B.default.weight: True
base_model.model.model.layers.30.mlp.up_proj.base_layer.weight: False
base_model.model.model.layers.30.mlp.up_proj.lora_A.default.weight: True
base_model.model.model.layers.30.mlp.up_proj.lora_B.default.weight: True
base_model.model.model.layers.30.mlp.down_proj.base_layer.weight: False
base_model.model.model.layers.30.mlp.down_proj.lora_A.default.weight: True
base_model.model.model.layers.30.mlp.down_proj.lora_B.default.weight: True
base_model.model.model.layers.30.input_layernorm.weight: False
base_model.model.model.layers.30.post_attention_layernorm.weight: False
base_model.model.model.layers.31.self_attn.q_proj.base_layer.weight: False
base_model.model.model.layers.31.self_attn.q_proj.lora_A.default.weight: True
base_model.model.model.layers.31.self_attn.q_proj.lora_B.default.weight: True
base_model.model.model.layers.31.self_attn.k_proj.base_layer.weight: False
base_model.model.model.layers.31.self_attn.k_proj.lora_A.default.weight: True
base_model.model.model.layers.31.self_attn.k_proj.lora_B.default.weight: True
base_model.model.model.layers.31.self_attn.v_proj.base_layer.weight: False
base_model.model.model.layers.31.self_attn.v_proj.lora_A.default.weight: True
base_model.model.model.layers.31.self_attn.v_proj.lora_B.default.weight: True
base_model.model.model.layers.31.self_attn.o_proj.base_layer.weight: False
base_model.model.model.layers.31.self_attn.o_proj.lora_A.default.weight: True
base_model.model.model.layers.31.self_attn.o_proj.lora_B.default.weight: True
base_model.model.model.layers.31.mlp.gate_proj.base_layer.weight: False
base_model.model.model.layers.31.mlp.gate_proj.lora_A.default.weight: True
base_model.model.model.layers.31.mlp.gate_proj.lora_B.default.weight: True
base_model.model.model.layers.31.mlp.up_proj.base_layer.weight: False
base_model.model.model.layers.31.mlp.up_proj.lora_A.default.weight: True
base_model.model.model.layers.31.mlp.up_proj.lora_B.default.weight: True
base_model.model.model.layers.31.mlp.down_proj.base_layer.weight: False
base_model.model.model.layers.31.mlp.down_proj.lora_A.default.weight: True
base_model.model.model.layers.31.mlp.down_proj.lora_B.default.weight: True
base_model.model.model.layers.31.input_layernorm.weight: False
base_model.model.model.layers.31.post_attention_layernorm.weight: False
base_model.model.model.norm.weight: False
base_model.model.lm_head.weight: False
=======Model loaded and ready for training========
/workspace/tulu_env/lib/python3.10/site-packages/peft/utils/save_and_load.py:473: UserWarning: Some weights of PeftModelForCausalLM were not initialized from the model checkpoint and are being ignored because you passed `ignore_mismatched_sizes=True`: - base_model.model.lm_head.weight: found shape torch.Size([128264, 4096]) in the checkpoint and torch.Size([128256, 4096]) in the model instantiated
- base_model.model.model.embed_tokens.weight: found shape torch.Size([128264, 4096]) in the checkpoint and torch.Size([128256, 4096]) in the model instantiated.
  warnings.warn(msg)
=========qloar  model is... ========
PeftModelForCausalLM(
  (base_model): LoraModel(
    (model): LlamaForCausalLM(
      (model): LlamaModel(
        (embed_tokens): Embedding(128256, 4096)
        (layers): ModuleList(
          (0-31): 32 x LlamaDecoderLayer(
            (self_attn): LlamaFlashAttention2(
              (q_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=64, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=64, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (k_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=64, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=64, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (v_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=64, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=64, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (o_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=64, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=64, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (rotary_emb): LlamaRotaryEmbedding()
            )
            (mlp): LlamaMLP(
              (gate_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=64, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=64, out_features=14336, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (up_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=64, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=64, out_features=14336, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (down_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=14336, out_features=64, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=64, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (act_fn): SiLU()
            )
            (input_layernorm): LlamaRMSNorm()
            (post_attention_layernorm): LlamaRMSNorm()
          )
        )
        (norm): LlamaRMSNorm()
        (rotary_emb): LlamaRotaryEmbedding()
      )
      (lm_head): Linear(in_features=4096, out_features=128256, bias=False)
    )
  )
)
trainable params: 167,772,160 || all params: 8,198,033,408 || trainable%: 2.0465
Base model params: 4708372480
LoRA params: 167772160
Total model params: 4876144640
base_model.model.model.embed_tokens.weight: False
base_model.model.model.layers.0.self_attn.q_proj.base_layer.weight: False
base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight: True
base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight: True
base_model.model.model.layers.0.self_attn.k_proj.base_layer.weight: False
base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight: True
base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight: True
base_model.model.model.layers.0.self_attn.v_proj.base_layer.weight: False
base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight: True
base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight: True
base_model.model.model.layers.0.self_attn.o_proj.base_layer.weight: False
base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight: True
base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight: True
base_model.model.model.layers.0.mlp.gate_proj.base_layer.weight: False
base_model.model.model.layers.0.mlp.gate_proj.lora_A.default.weight: True
base_model.model.model.layers.0.mlp.gate_proj.lora_B.default.weight: True
base_model.model.model.layers.0.mlp.up_proj.base_layer.weight: False
base_model.model.model.layers.0.mlp.up_proj.lora_A.default.weight: True
base_model.model.model.layers.0.mlp.up_proj.lora_B.default.weight: True
base_model.model.model.layers.0.mlp.down_proj.base_layer.weight: False
base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight: True
base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight: True
base_model.model.model.layers.0.input_layernorm.weight: False
base_model.model.model.layers.0.post_attention_layernorm.weight: False
base_model.model.model.layers.1.self_attn.q_proj.base_layer.weight: False
base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight: True
base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight: True
base_model.model.model.layers.1.self_attn.k_proj.base_layer.weight: False
base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight: True
base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight: True
base_model.model.model.layers.1.self_attn.v_proj.base_layer.weight: False
base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight: True
base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight: True
base_model.model.model.layers.1.self_attn.o_proj.base_layer.weight: False
base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight: True
base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight: True
base_model.model.model.layers.1.mlp.gate_proj.base_layer.weight: False
base_model.model.model.layers.1.mlp.gate_proj.lora_A.default.weight: True
base_model.model.model.layers.1.mlp.gate_proj.lora_B.default.weight: True
base_model.model.model.layers.1.mlp.up_proj.base_layer.weight: False
base_model.model.model.layers.1.mlp.up_proj.lora_A.default.weight: True
base_model.model.model.layers.1.mlp.up_proj.lora_B.default.weight: True
base_model.model.model.layers.1.mlp.down_proj.base_layer.weight: False
base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight: True
base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight: True
base_model.model.model.layers.1.input_layernorm.weight: False
base_model.model.model.layers.1.post_attention_layernorm.weight: False
base_model.model.model.layers.2.self_attn.q_proj.base_layer.weight: False
base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight: True
base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight: True
base_model.model.model.layers.2.self_attn.k_proj.base_layer.weight: False
base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight: True
base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight: True
base_model.model.model.layers.2.self_attn.v_proj.base_layer.weight: False
base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight: True
base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight: True
base_model.model.model.layers.2.self_attn.o_proj.base_layer.weight: False
base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight: True
base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight: True
base_model.model.model.layers.2.mlp.gate_proj.base_layer.weight: False
base_model.model.model.layers.2.mlp.gate_proj.lora_A.default.weight: True
base_model.model.model.layers.2.mlp.gate_proj.lora_B.default.weight: True
base_model.model.model.layers.2.mlp.up_proj.base_layer.weight: False
base_model.model.model.layers.2.mlp.up_proj.lora_A.default.weight: True
base_model.model.model.layers.2.mlp.up_proj.lora_B.default.weight: True
base_model.model.model.layers.2.mlp.down_proj.base_layer.weight: False
base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight: True
base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight: True
base_model.model.model.layers.2.input_layernorm.weight: False
base_model.model.model.layers.2.post_attention_layernorm.weight: False
base_model.model.model.layers.3.self_attn.q_proj.base_layer.weight: False
base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight: True
base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight: True
base_model.model.model.layers.3.self_attn.k_proj.base_layer.weight: False
base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight: True
base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight: True
base_model.model.model.layers.3.self_attn.v_proj.base_layer.weight: False
base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight: True
base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight: True
base_model.model.model.layers.3.self_attn.o_proj.base_layer.weight: False
base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight: True
base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight: True
base_model.model.model.layers.3.mlp.gate_proj.base_layer.weight: False
base_model.model.model.layers.3.mlp.gate_proj.lora_A.default.weight: True
base_model.model.model.layers.3.mlp.gate_proj.lora_B.default.weight: True
base_model.model.model.layers.3.mlp.up_proj.base_layer.weight: False
base_model.model.model.layers.3.mlp.up_proj.lora_A.default.weight: True
base_model.model.model.layers.3.mlp.up_proj.lora_B.default.weight: True
base_model.model.model.layers.3.mlp.down_proj.base_layer.weight: False
base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight: True
base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight: True
base_model.model.model.layers.3.input_layernorm.weight: False
base_model.model.model.layers.3.post_attention_layernorm.weight: False
base_model.model.model.layers.4.self_attn.q_proj.base_layer.weight: False
base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight: True
base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight: True
base_model.model.model.layers.4.self_attn.k_proj.base_layer.weight: False
base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight: True
base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight: True
base_model.model.model.layers.4.self_attn.v_proj.base_layer.weight: False
base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight: True
base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight: True
base_model.model.model.layers.4.self_attn.o_proj.base_layer.weight: False
base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight: True
base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight: True
base_model.model.model.layers.4.mlp.gate_proj.base_layer.weight: False
base_model.model.model.layers.4.mlp.gate_proj.lora_A.default.weight: True
base_model.model.model.layers.4.mlp.gate_proj.lora_B.default.weight: True
base_model.model.model.layers.4.mlp.up_proj.base_layer.weight: False
base_model.model.model.layers.4.mlp.up_proj.lora_A.default.weight: True
base_model.model.model.layers.4.mlp.up_proj.lora_B.default.weight: True
base_model.model.model.layers.4.mlp.down_proj.base_layer.weight: False
base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight: True
base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight: True
base_model.model.model.layers.4.input_layernorm.weight: False
base_model.model.model.layers.4.post_attention_layernorm.weight: False
base_model.model.model.layers.5.self_attn.q_proj.base_layer.weight: False
base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight: True
base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight: True
base_model.model.model.layers.5.self_attn.k_proj.base_layer.weight: False
base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight: True
base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight: True
base_model.model.model.layers.5.self_attn.v_proj.base_layer.weight: False
base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight: True
base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight: True
base_model.model.model.layers.5.self_attn.o_proj.base_layer.weight: False
base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight: True
base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight: True
base_model.model.model.layers.5.mlp.gate_proj.base_layer.weight: False
base_model.model.model.layers.5.mlp.gate_proj.lora_A.default.weight: True
base_model.model.model.layers.5.mlp.gate_proj.lora_B.default.weight: True
base_model.model.model.layers.5.mlp.up_proj.base_layer.weight: False
base_model.model.model.layers.5.mlp.up_proj.lora_A.default.weight: True
base_model.model.model.layers.5.mlp.up_proj.lora_B.default.weight: True
base_model.model.model.layers.5.mlp.down_proj.base_layer.weight: False
base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight: True
base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight: True
base_model.model.model.layers.5.input_layernorm.weight: False
base_model.model.model.layers.5.post_attention_layernorm.weight: False
base_model.model.model.layers.6.self_attn.q_proj.base_layer.weight: False
base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight: True
base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight: True
base_model.model.model.layers.6.self_attn.k_proj.base_layer.weight: False
base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight: True
base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight: True
base_model.model.model.layers.6.self_attn.v_proj.base_layer.weight: False
base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight: True
base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight: True
base_model.model.model.layers.6.self_attn.o_proj.base_layer.weight: False
base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight: True
base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight: True
base_model.model.model.layers.6.mlp.gate_proj.base_layer.weight: False
base_model.model.model.layers.6.mlp.gate_proj.lora_A.default.weight: True
base_model.model.model.layers.6.mlp.gate_proj.lora_B.default.weight: True
base_model.model.model.layers.6.mlp.up_proj.base_layer.weight: False
base_model.model.model.layers.6.mlp.up_proj.lora_A.default.weight: True
base_model.model.model.layers.6.mlp.up_proj.lora_B.default.weight: True
base_model.model.model.layers.6.mlp.down_proj.base_layer.weight: False
base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight: True
base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight: True
base_model.model.model.layers.6.input_layernorm.weight: False
base_model.model.model.layers.6.post_attention_layernorm.weight: False
base_model.model.model.layers.7.self_attn.q_proj.base_layer.weight: False
base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight: True
base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight: True
base_model.model.model.layers.7.self_attn.k_proj.base_layer.weight: False
base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight: True
base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight: True
base_model.model.model.layers.7.self_attn.v_proj.base_layer.weight: False
base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight: True
base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight: True
base_model.model.model.layers.7.self_attn.o_proj.base_layer.weight: False
base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight: True
base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight: True
base_model.model.model.layers.7.mlp.gate_proj.base_layer.weight: False
base_model.model.model.layers.7.mlp.gate_proj.lora_A.default.weight: True
base_model.model.model.layers.7.mlp.gate_proj.lora_B.default.weight: True
base_model.model.model.layers.7.mlp.up_proj.base_layer.weight: False
base_model.model.model.layers.7.mlp.up_proj.lora_A.default.weight: True
base_model.model.model.layers.7.mlp.up_proj.lora_B.default.weight: True
base_model.model.model.layers.7.mlp.down_proj.base_layer.weight: False
base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight: True
base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight: True
base_model.model.model.layers.7.input_layernorm.weight: False
base_model.model.model.layers.7.post_attention_layernorm.weight: False
base_model.model.model.layers.8.self_attn.q_proj.base_layer.weight: False
base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight: True
base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight: True
base_model.model.model.layers.8.self_attn.k_proj.base_layer.weight: False
base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight: True
base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight: True
base_model.model.model.layers.8.self_attn.v_proj.base_layer.weight: False
base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight: True
base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight: True
base_model.model.model.layers.8.self_attn.o_proj.base_layer.weight: False
base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight: True
base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight: True
base_model.model.model.layers.8.mlp.gate_proj.base_layer.weight: False
base_model.model.model.layers.8.mlp.gate_proj.lora_A.default.weight: True
base_model.model.model.layers.8.mlp.gate_proj.lora_B.default.weight: True
base_model.model.model.layers.8.mlp.up_proj.base_layer.weight: False
base_model.model.model.layers.8.mlp.up_proj.lora_A.default.weight: True
base_model.model.model.layers.8.mlp.up_proj.lora_B.default.weight: True
base_model.model.model.layers.8.mlp.down_proj.base_layer.weight: False
base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight: True
base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight: True
base_model.model.model.layers.8.input_layernorm.weight: False
base_model.model.model.layers.8.post_attention_layernorm.weight: False
base_model.model.model.layers.9.self_attn.q_proj.base_layer.weight: False
base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight: True
base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight: True
base_model.model.model.layers.9.self_attn.k_proj.base_layer.weight: False
base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight: True
base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight: True
base_model.model.model.layers.9.self_attn.v_proj.base_layer.weight: False
base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight: True
base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight: True
base_model.model.model.layers.9.self_attn.o_proj.base_layer.weight: False
base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight: True
base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight: True
base_model.model.model.layers.9.mlp.gate_proj.base_layer.weight: False
base_model.model.model.layers.9.mlp.gate_proj.lora_A.default.weight: True
base_model.model.model.layers.9.mlp.gate_proj.lora_B.default.weight: True
base_model.model.model.layers.9.mlp.up_proj.base_layer.weight: False
base_model.model.model.layers.9.mlp.up_proj.lora_A.default.weight: True
base_model.model.model.layers.9.mlp.up_proj.lora_B.default.weight: True
base_model.model.model.layers.9.mlp.down_proj.base_layer.weight: False
base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight: True
base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight: True
base_model.model.model.layers.9.input_layernorm.weight: False
base_model.model.model.layers.9.post_attention_layernorm.weight: False
base_model.model.model.layers.10.self_attn.q_proj.base_layer.weight: False
base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight: True
base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight: True
base_model.model.model.layers.10.self_attn.k_proj.base_layer.weight: False
base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight: True
base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight: True
base_model.model.model.layers.10.self_attn.v_proj.base_layer.weight: False
base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight: True
base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight: True
base_model.model.model.layers.10.self_attn.o_proj.base_layer.weight: False
base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight: True
base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight: True
base_model.model.model.layers.10.mlp.gate_proj.base_layer.weight: False
base_model.model.model.layers.10.mlp.gate_proj.lora_A.default.weight: True
base_model.model.model.layers.10.mlp.gate_proj.lora_B.default.weight: True
base_model.model.model.layers.10.mlp.up_proj.base_layer.weight: False
base_model.model.model.layers.10.mlp.up_proj.lora_A.default.weight: True
base_model.model.model.layers.10.mlp.up_proj.lora_B.default.weight: True
base_model.model.model.layers.10.mlp.down_proj.base_layer.weight: False
base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight: True
base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight: True
base_model.model.model.layers.10.input_layernorm.weight: False
base_model.model.model.layers.10.post_attention_layernorm.weight: False
base_model.model.model.layers.11.self_attn.q_proj.base_layer.weight: False
base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight: True
base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight: True
base_model.model.model.layers.11.self_attn.k_proj.base_layer.weight: False
base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight: True
base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight: True
base_model.model.model.layers.11.self_attn.v_proj.base_layer.weight: False
base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight: True
base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight: True
base_model.model.model.layers.11.self_attn.o_proj.base_layer.weight: False
base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight: True
base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight: True
base_model.model.model.layers.11.mlp.gate_proj.base_layer.weight: False
base_model.model.model.layers.11.mlp.gate_proj.lora_A.default.weight: True
base_model.model.model.layers.11.mlp.gate_proj.lora_B.default.weight: True
base_model.model.model.layers.11.mlp.up_proj.base_layer.weight: False
base_model.model.model.layers.11.mlp.up_proj.lora_A.default.weight: True
base_model.model.model.layers.11.mlp.up_proj.lora_B.default.weight: True
base_model.model.model.layers.11.mlp.down_proj.base_layer.weight: False
base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight: True
base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight: True
base_model.model.model.layers.11.input_layernorm.weight: False
base_model.model.model.layers.11.post_attention_layernorm.weight: False
base_model.model.model.layers.12.self_attn.q_proj.base_layer.weight: False
base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight: True
base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight: True
base_model.model.model.layers.12.self_attn.k_proj.base_layer.weight: False
base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight: True
base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight: True
base_model.model.model.layers.12.self_attn.v_proj.base_layer.weight: False
base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight: True
base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight: True
base_model.model.model.layers.12.self_attn.o_proj.base_layer.weight: False
base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight: True
base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight: True
base_model.model.model.layers.12.mlp.gate_proj.base_layer.weight: False
base_model.model.model.layers.12.mlp.gate_proj.lora_A.default.weight: True
base_model.model.model.layers.12.mlp.gate_proj.lora_B.default.weight: True
base_model.model.model.layers.12.mlp.up_proj.base_layer.weight: False
base_model.model.model.layers.12.mlp.up_proj.lora_A.default.weight: True
base_model.model.model.layers.12.mlp.up_proj.lora_B.default.weight: True
base_model.model.model.layers.12.mlp.down_proj.base_layer.weight: False
base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight: True
base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight: True
base_model.model.model.layers.12.input_layernorm.weight: False
base_model.model.model.layers.12.post_attention_layernorm.weight: False
base_model.model.model.layers.13.self_attn.q_proj.base_layer.weight: False
base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight: True
base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight: True
base_model.model.model.layers.13.self_attn.k_proj.base_layer.weight: False
base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight: True
base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight: True
base_model.model.model.layers.13.self_attn.v_proj.base_layer.weight: False
base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight: True
base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight: True
base_model.model.model.layers.13.self_attn.o_proj.base_layer.weight: False
base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight: True
base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight: True
base_model.model.model.layers.13.mlp.gate_proj.base_layer.weight: False
base_model.model.model.layers.13.mlp.gate_proj.lora_A.default.weight: True
base_model.model.model.layers.13.mlp.gate_proj.lora_B.default.weight: True
base_model.model.model.layers.13.mlp.up_proj.base_layer.weight: False
base_model.model.model.layers.13.mlp.up_proj.lora_A.default.weight: True
base_model.model.model.layers.13.mlp.up_proj.lora_B.default.weight: True
base_model.model.model.layers.13.mlp.down_proj.base_layer.weight: False
base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight: True
base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight: True
base_model.model.model.layers.13.input_layernorm.weight: False
base_model.model.model.layers.13.post_attention_layernorm.weight: False
base_model.model.model.layers.14.self_attn.q_proj.base_layer.weight: False
base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight: True
base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight: True
base_model.model.model.layers.14.self_attn.k_proj.base_layer.weight: False
base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight: True
base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight: True
base_model.model.model.layers.14.self_attn.v_proj.base_layer.weight: False
base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight: True
base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight: True
base_model.model.model.layers.14.self_attn.o_proj.base_layer.weight: False
base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight: True
base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight: True
base_model.model.model.layers.14.mlp.gate_proj.base_layer.weight: False
base_model.model.model.layers.14.mlp.gate_proj.lora_A.default.weight: True
base_model.model.model.layers.14.mlp.gate_proj.lora_B.default.weight: True
base_model.model.model.layers.14.mlp.up_proj.base_layer.weight: False
base_model.model.model.layers.14.mlp.up_proj.lora_A.default.weight: True
base_model.model.model.layers.14.mlp.up_proj.lora_B.default.weight: True
base_model.model.model.layers.14.mlp.down_proj.base_layer.weight: False
base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight: True
base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight: True
base_model.model.model.layers.14.input_layernorm.weight: False
base_model.model.model.layers.14.post_attention_layernorm.weight: False
base_model.model.model.layers.15.self_attn.q_proj.base_layer.weight: False
base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight: True
base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight: True
base_model.model.model.layers.15.self_attn.k_proj.base_layer.weight: False
base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight: True
base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight: True
base_model.model.model.layers.15.self_attn.v_proj.base_layer.weight: False
base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight: True
base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight: True
base_model.model.model.layers.15.self_attn.o_proj.base_layer.weight: False
base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight: True
base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight: True
base_model.model.model.layers.15.mlp.gate_proj.base_layer.weight: False
base_model.model.model.layers.15.mlp.gate_proj.lora_A.default.weight: True
base_model.model.model.layers.15.mlp.gate_proj.lora_B.default.weight: True
base_model.model.model.layers.15.mlp.up_proj.base_layer.weight: False
base_model.model.model.layers.15.mlp.up_proj.lora_A.default.weight: True
base_model.model.model.layers.15.mlp.up_proj.lora_B.default.weight: True
base_model.model.model.layers.15.mlp.down_proj.base_layer.weight: False
base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight: True
base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight: True
base_model.model.model.layers.15.input_layernorm.weight: False
base_model.model.model.layers.15.post_attention_layernorm.weight: False
base_model.model.model.layers.16.self_attn.q_proj.base_layer.weight: False
base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight: True
base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight: True
base_model.model.model.layers.16.self_attn.k_proj.base_layer.weight: False
base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight: True
base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight: True
base_model.model.model.layers.16.self_attn.v_proj.base_layer.weight: False
base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight: True
base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight: True
base_model.model.model.layers.16.self_attn.o_proj.base_layer.weight: False
base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight: True
base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight: True
base_model.model.model.layers.16.mlp.gate_proj.base_layer.weight: False
base_model.model.model.layers.16.mlp.gate_proj.lora_A.default.weight: True
base_model.model.model.layers.16.mlp.gate_proj.lora_B.default.weight: True
base_model.model.model.layers.16.mlp.up_proj.base_layer.weight: False
base_model.model.model.layers.16.mlp.up_proj.lora_A.default.weight: True
base_model.model.model.layers.16.mlp.up_proj.lora_B.default.weight: True
base_model.model.model.layers.16.mlp.down_proj.base_layer.weight: False
base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight: True
base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight: True
base_model.model.model.layers.16.input_layernorm.weight: False
base_model.model.model.layers.16.post_attention_layernorm.weight: False
base_model.model.model.layers.17.self_attn.q_proj.base_layer.weight: False
base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight: True
base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight: True
base_model.model.model.layers.17.self_attn.k_proj.base_layer.weight: False
base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight: True
base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight: True
base_model.model.model.layers.17.self_attn.v_proj.base_layer.weight: False
base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight: True
base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight: True
base_model.model.model.layers.17.self_attn.o_proj.base_layer.weight: False
base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight: True
base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight: True
base_model.model.model.layers.17.mlp.gate_proj.base_layer.weight: False
base_model.model.model.layers.17.mlp.gate_proj.lora_A.default.weight: True
base_model.model.model.layers.17.mlp.gate_proj.lora_B.default.weight: True
base_model.model.model.layers.17.mlp.up_proj.base_layer.weight: False
base_model.model.model.layers.17.mlp.up_proj.lora_A.default.weight: True
base_model.model.model.layers.17.mlp.up_proj.lora_B.default.weight: True
base_model.model.model.layers.17.mlp.down_proj.base_layer.weight: False
base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight: True
base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight: True
base_model.model.model.layers.17.input_layernorm.weight: False
base_model.model.model.layers.17.post_attention_layernorm.weight: False
base_model.model.model.layers.18.self_attn.q_proj.base_layer.weight: False
base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight: True
base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight: True
base_model.model.model.layers.18.self_attn.k_proj.base_layer.weight: False
base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight: True
base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight: True
base_model.model.model.layers.18.self_attn.v_proj.base_layer.weight: False
base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight: True
base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight: True
base_model.model.model.layers.18.self_attn.o_proj.base_layer.weight: False
base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight: True
base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight: True
base_model.model.model.layers.18.mlp.gate_proj.base_layer.weight: False
base_model.model.model.layers.18.mlp.gate_proj.lora_A.default.weight: True
base_model.model.model.layers.18.mlp.gate_proj.lora_B.default.weight: True
base_model.model.model.layers.18.mlp.up_proj.base_layer.weight: False
base_model.model.model.layers.18.mlp.up_proj.lora_A.default.weight: True
base_model.model.model.layers.18.mlp.up_proj.lora_B.default.weight: True
base_model.model.model.layers.18.mlp.down_proj.base_layer.weight: False
base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight: True
base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight: True
base_model.model.model.layers.18.input_layernorm.weight: False
base_model.model.model.layers.18.post_attention_layernorm.weight: False
base_model.model.model.layers.19.self_attn.q_proj.base_layer.weight: False
base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight: True
base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight: True
base_model.model.model.layers.19.self_attn.k_proj.base_layer.weight: False
base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight: True
base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight: True
base_model.model.model.layers.19.self_attn.v_proj.base_layer.weight: False
base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight: True
base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight: True
base_model.model.model.layers.19.self_attn.o_proj.base_layer.weight: False
base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight: True
base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight: True
base_model.model.model.layers.19.mlp.gate_proj.base_layer.weight: False
base_model.model.model.layers.19.mlp.gate_proj.lora_A.default.weight: True
base_model.model.model.layers.19.mlp.gate_proj.lora_B.default.weight: True
base_model.model.model.layers.19.mlp.up_proj.base_layer.weight: False
base_model.model.model.layers.19.mlp.up_proj.lora_A.default.weight: True
base_model.model.model.layers.19.mlp.up_proj.lora_B.default.weight: True
base_model.model.model.layers.19.mlp.down_proj.base_layer.weight: False
base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight: True
base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight: True
base_model.model.model.layers.19.input_layernorm.weight: False
base_model.model.model.layers.19.post_attention_layernorm.weight: False
base_model.model.model.layers.20.self_attn.q_proj.base_layer.weight: False
base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight: True
base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight: True
base_model.model.model.layers.20.self_attn.k_proj.base_layer.weight: False
base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight: True
base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight: True
base_model.model.model.layers.20.self_attn.v_proj.base_layer.weight: False
base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight: True
base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight: True
base_model.model.model.layers.20.self_attn.o_proj.base_layer.weight: False
base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight: True
base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight: True
base_model.model.model.layers.20.mlp.gate_proj.base_layer.weight: False
base_model.model.model.layers.20.mlp.gate_proj.lora_A.default.weight: True
base_model.model.model.layers.20.mlp.gate_proj.lora_B.default.weight: True
base_model.model.model.layers.20.mlp.up_proj.base_layer.weight: False
base_model.model.model.layers.20.mlp.up_proj.lora_A.default.weight: True
base_model.model.model.layers.20.mlp.up_proj.lora_B.default.weight: True
base_model.model.model.layers.20.mlp.down_proj.base_layer.weight: False
base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight: True
base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight: True
base_model.model.model.layers.20.input_layernorm.weight: False
base_model.model.model.layers.20.post_attention_layernorm.weight: False
base_model.model.model.layers.21.self_attn.q_proj.base_layer.weight: False
base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight: True
base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight: True
base_model.model.model.layers.21.self_attn.k_proj.base_layer.weight: False
base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight: True
base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight: True
base_model.model.model.layers.21.self_attn.v_proj.base_layer.weight: False
base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight: True
base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight: True
base_model.model.model.layers.21.self_attn.o_proj.base_layer.weight: False
base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight: True
base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight: True
base_model.model.model.layers.21.mlp.gate_proj.base_layer.weight: False
base_model.model.model.layers.21.mlp.gate_proj.lora_A.default.weight: True
base_model.model.model.layers.21.mlp.gate_proj.lora_B.default.weight: True
base_model.model.model.layers.21.mlp.up_proj.base_layer.weight: False
base_model.model.model.layers.21.mlp.up_proj.lora_A.default.weight: True
base_model.model.model.layers.21.mlp.up_proj.lora_B.default.weight: True
base_model.model.model.layers.21.mlp.down_proj.base_layer.weight: False
base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight: True
base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight: True
base_model.model.model.layers.21.input_layernorm.weight: False
base_model.model.model.layers.21.post_attention_layernorm.weight: False
base_model.model.model.layers.22.self_attn.q_proj.base_layer.weight: False
base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight: True
base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight: True
base_model.model.model.layers.22.self_attn.k_proj.base_layer.weight: False
base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight: True
base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight: True
base_model.model.model.layers.22.self_attn.v_proj.base_layer.weight: False
base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight: True
base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight: True
base_model.model.model.layers.22.self_attn.o_proj.base_layer.weight: False
base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight: True
base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight: True
base_model.model.model.layers.22.mlp.gate_proj.base_layer.weight: False
base_model.model.model.layers.22.mlp.gate_proj.lora_A.default.weight: True
base_model.model.model.layers.22.mlp.gate_proj.lora_B.default.weight: True
base_model.model.model.layers.22.mlp.up_proj.base_layer.weight: False
base_model.model.model.layers.22.mlp.up_proj.lora_A.default.weight: True
base_model.model.model.layers.22.mlp.up_proj.lora_B.default.weight: True
base_model.model.model.layers.22.mlp.down_proj.base_layer.weight: False
base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight: True
base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight: True
base_model.model.model.layers.22.input_layernorm.weight: False
base_model.model.model.layers.22.post_attention_layernorm.weight: False
base_model.model.model.layers.23.self_attn.q_proj.base_layer.weight: False
base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight: True
base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight: True
base_model.model.model.layers.23.self_attn.k_proj.base_layer.weight: False
base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight: True
base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight: True
base_model.model.model.layers.23.self_attn.v_proj.base_layer.weight: False
base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight: True
base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight: True
base_model.model.model.layers.23.self_attn.o_proj.base_layer.weight: False
base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight: True
base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight: True
base_model.model.model.layers.23.mlp.gate_proj.base_layer.weight: False
base_model.model.model.layers.23.mlp.gate_proj.lora_A.default.weight: True
base_model.model.model.layers.23.mlp.gate_proj.lora_B.default.weight: True
base_model.model.model.layers.23.mlp.up_proj.base_layer.weight: False
base_model.model.model.layers.23.mlp.up_proj.lora_A.default.weight: True
base_model.model.model.layers.23.mlp.up_proj.lora_B.default.weight: True
base_model.model.model.layers.23.mlp.down_proj.base_layer.weight: False
base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight: True
base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight: True
base_model.model.model.layers.23.input_layernorm.weight: False
base_model.model.model.layers.23.post_attention_layernorm.weight: False
base_model.model.model.layers.24.self_attn.q_proj.base_layer.weight: False
base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight: True
base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight: True
base_model.model.model.layers.24.self_attn.k_proj.base_layer.weight: False
base_model.model.model.layers.24.self_attn.k_proj.lora_A.default.weight: True
base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight: True
base_model.model.model.layers.24.self_attn.v_proj.base_layer.weight: False
base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight: True
base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight: True
base_model.model.model.layers.24.self_attn.o_proj.base_layer.weight: False
base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight: True
base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight: True
base_model.model.model.layers.24.mlp.gate_proj.base_layer.weight: False
base_model.model.model.layers.24.mlp.gate_proj.lora_A.default.weight: True
base_model.model.model.layers.24.mlp.gate_proj.lora_B.default.weight: True
base_model.model.model.layers.24.mlp.up_proj.base_layer.weight: False
base_model.model.model.layers.24.mlp.up_proj.lora_A.default.weight: True
base_model.model.model.layers.24.mlp.up_proj.lora_B.default.weight: True
base_model.model.model.layers.24.mlp.down_proj.base_layer.weight: False
base_model.model.model.layers.24.mlp.down_proj.lora_A.default.weight: True
base_model.model.model.layers.24.mlp.down_proj.lora_B.default.weight: True
base_model.model.model.layers.24.input_layernorm.weight: False
base_model.model.model.layers.24.post_attention_layernorm.weight: False
base_model.model.model.layers.25.self_attn.q_proj.base_layer.weight: False
base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight: True
base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight: True
base_model.model.model.layers.25.self_attn.k_proj.base_layer.weight: False
base_model.model.model.layers.25.self_attn.k_proj.lora_A.default.weight: True
base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight: True
base_model.model.model.layers.25.self_attn.v_proj.base_layer.weight: False
base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight: True
base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight: True
base_model.model.model.layers.25.self_attn.o_proj.base_layer.weight: False
base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight: True
base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight: True
base_model.model.model.layers.25.mlp.gate_proj.base_layer.weight: False
base_model.model.model.layers.25.mlp.gate_proj.lora_A.default.weight: True
base_model.model.model.layers.25.mlp.gate_proj.lora_B.default.weight: True
base_model.model.model.layers.25.mlp.up_proj.base_layer.weight: False
base_model.model.model.layers.25.mlp.up_proj.lora_A.default.weight: True
base_model.model.model.layers.25.mlp.up_proj.lora_B.default.weight: True
base_model.model.model.layers.25.mlp.down_proj.base_layer.weight: False
base_model.model.model.layers.25.mlp.down_proj.lora_A.default.weight: True
base_model.model.model.layers.25.mlp.down_proj.lora_B.default.weight: True
base_model.model.model.layers.25.input_layernorm.weight: False
base_model.model.model.layers.25.post_attention_layernorm.weight: False
base_model.model.model.layers.26.self_attn.q_proj.base_layer.weight: False
base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight: True
base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight: True
base_model.model.model.layers.26.self_attn.k_proj.base_layer.weight: False
base_model.model.model.layers.26.self_attn.k_proj.lora_A.default.weight: True
base_model.model.model.layers.26.self_attn.k_proj.lora_B.default.weight: True
base_model.model.model.layers.26.self_attn.v_proj.base_layer.weight: False
base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight: True
base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight: True
base_model.model.model.layers.26.self_attn.o_proj.base_layer.weight: False
base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight: True
base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight: True
base_model.model.model.layers.26.mlp.gate_proj.base_layer.weight: False
base_model.model.model.layers.26.mlp.gate_proj.lora_A.default.weight: True
base_model.model.model.layers.26.mlp.gate_proj.lora_B.default.weight: True
base_model.model.model.layers.26.mlp.up_proj.base_layer.weight: False
base_model.model.model.layers.26.mlp.up_proj.lora_A.default.weight: True
base_model.model.model.layers.26.mlp.up_proj.lora_B.default.weight: True
base_model.model.model.layers.26.mlp.down_proj.base_layer.weight: False
base_model.model.model.layers.26.mlp.down_proj.lora_A.default.weight: True
base_model.model.model.layers.26.mlp.down_proj.lora_B.default.weight: True
base_model.model.model.layers.26.input_layernorm.weight: False
base_model.model.model.layers.26.post_attention_layernorm.weight: False
base_model.model.model.layers.27.self_attn.q_proj.base_layer.weight: False
base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight: True
base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight: True
base_model.model.model.layers.27.self_attn.k_proj.base_layer.weight: False
base_model.model.model.layers.27.self_attn.k_proj.lora_A.default.weight: True
base_model.model.model.layers.27.self_attn.k_proj.lora_B.default.weight: True
base_model.model.model.layers.27.self_attn.v_proj.base_layer.weight: False
base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight: True
base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight: True
base_model.model.model.layers.27.self_attn.o_proj.base_layer.weight: False
base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight: True
base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight: True
base_model.model.model.layers.27.mlp.gate_proj.base_layer.weight: False
base_model.model.model.layers.27.mlp.gate_proj.lora_A.default.weight: True
base_model.model.model.layers.27.mlp.gate_proj.lora_B.default.weight: True
base_model.model.model.layers.27.mlp.up_proj.base_layer.weight: False
base_model.model.model.layers.27.mlp.up_proj.lora_A.default.weight: True
base_model.model.model.layers.27.mlp.up_proj.lora_B.default.weight: True
base_model.model.model.layers.27.mlp.down_proj.base_layer.weight: False
base_model.model.model.layers.27.mlp.down_proj.lora_A.default.weight: True
base_model.model.model.layers.27.mlp.down_proj.lora_B.default.weight: True
base_model.model.model.layers.27.input_layernorm.weight: False
base_model.model.model.layers.27.post_attention_layernorm.weight: False
base_model.model.model.layers.28.self_attn.q_proj.base_layer.weight: False
base_model.model.model.layers.28.self_attn.q_proj.lora_A.default.weight: True
base_model.model.model.layers.28.self_attn.q_proj.lora_B.default.weight: True
base_model.model.model.layers.28.self_attn.k_proj.base_layer.weight: False
base_model.model.model.layers.28.self_attn.k_proj.lora_A.default.weight: True
base_model.model.model.layers.28.self_attn.k_proj.lora_B.default.weight: True
base_model.model.model.layers.28.self_attn.v_proj.base_layer.weight: False
base_model.model.model.layers.28.self_attn.v_proj.lora_A.default.weight: True
base_model.model.model.layers.28.self_attn.v_proj.lora_B.default.weight: True
base_model.model.model.layers.28.self_attn.o_proj.base_layer.weight: False
base_model.model.model.layers.28.self_attn.o_proj.lora_A.default.weight: True
base_model.model.model.layers.28.self_attn.o_proj.lora_B.default.weight: True
base_model.model.model.layers.28.mlp.gate_proj.base_layer.weight: False
base_model.model.model.layers.28.mlp.gate_proj.lora_A.default.weight: True
base_model.model.model.layers.28.mlp.gate_proj.lora_B.default.weight: True
base_model.model.model.layers.28.mlp.up_proj.base_layer.weight: False
base_model.model.model.layers.28.mlp.up_proj.lora_A.default.weight: True
base_model.model.model.layers.28.mlp.up_proj.lora_B.default.weight: True
base_model.model.model.layers.28.mlp.down_proj.base_layer.weight: False
base_model.model.model.layers.28.mlp.down_proj.lora_A.default.weight: True
base_model.model.model.layers.28.mlp.down_proj.lora_B.default.weight: True
base_model.model.model.layers.28.input_layernorm.weight: False
base_model.model.model.layers.28.post_attention_layernorm.weight: False
base_model.model.model.layers.29.self_attn.q_proj.base_layer.weight: False
base_model.model.model.layers.29.self_attn.q_proj.lora_A.default.weight: True
base_model.model.model.layers.29.self_attn.q_proj.lora_B.default.weight: True
base_model.model.model.layers.29.self_attn.k_proj.base_layer.weight: False
base_model.model.model.layers.29.self_attn.k_proj.lora_A.default.weight: True
base_model.model.model.layers.29.self_attn.k_proj.lora_B.default.weight: True
base_model.model.model.layers.29.self_attn.v_proj.base_layer.weight: False
base_model.model.model.layers.29.self_attn.v_proj.lora_A.default.weight: True
base_model.model.model.layers.29.self_attn.v_proj.lora_B.default.weight: True
base_model.model.model.layers.29.self_attn.o_proj.base_layer.weight: False
base_model.model.model.layers.29.self_attn.o_proj.lora_A.default.weight: True
base_model.model.model.layers.29.self_attn.o_proj.lora_B.default.weight: True
base_model.model.model.layers.29.mlp.gate_proj.base_layer.weight: False
base_model.model.model.layers.29.mlp.gate_proj.lora_A.default.weight: True
base_model.model.model.layers.29.mlp.gate_proj.lora_B.default.weight: True
base_model.model.model.layers.29.mlp.up_proj.base_layer.weight: False
base_model.model.model.layers.29.mlp.up_proj.lora_A.default.weight: True
base_model.model.model.layers.29.mlp.up_proj.lora_B.default.weight: True
base_model.model.model.layers.29.mlp.down_proj.base_layer.weight: False
base_model.model.model.layers.29.mlp.down_proj.lora_A.default.weight: True
base_model.model.model.layers.29.mlp.down_proj.lora_B.default.weight: True
base_model.model.model.layers.29.input_layernorm.weight: False
base_model.model.model.layers.29.post_attention_layernorm.weight: False
base_model.model.model.layers.30.self_attn.q_proj.base_layer.weight: False
base_model.model.model.layers.30.self_attn.q_proj.lora_A.default.weight: True
base_model.model.model.layers.30.self_attn.q_proj.lora_B.default.weight: True
base_model.model.model.layers.30.self_attn.k_proj.base_layer.weight: False
base_model.model.model.layers.30.self_attn.k_proj.lora_A.default.weight: True
base_model.model.model.layers.30.self_attn.k_proj.lora_B.default.weight: True
base_model.model.model.layers.30.self_attn.v_proj.base_layer.weight: False
base_model.model.model.layers.30.self_attn.v_proj.lora_A.default.weight: True
base_model.model.model.layers.30.self_attn.v_proj.lora_B.default.weight: True
base_model.model.model.layers.30.self_attn.o_proj.base_layer.weight: False
base_model.model.model.layers.30.self_attn.o_proj.lora_A.default.weight: True
base_model.model.model.layers.30.self_attn.o_proj.lora_B.default.weight: True
base_model.model.model.layers.30.mlp.gate_proj.base_layer.weight: False
base_model.model.model.layers.30.mlp.gate_proj.lora_A.default.weight: True
base_model.model.model.layers.30.mlp.gate_proj.lora_B.default.weight: True
base_model.model.model.layers.30.mlp.up_proj.base_layer.weight: False
base_model.model.model.layers.30.mlp.up_proj.lora_A.default.weight: True
base_model.model.model.layers.30.mlp.up_proj.lora_B.default.weight: True
base_model.model.model.layers.30.mlp.down_proj.base_layer.weight: False
base_model.model.model.layers.30.mlp.down_proj.lora_A.default.weight: True
base_model.model.model.layers.30.mlp.down_proj.lora_B.default.weight: True
base_model.model.model.layers.30.input_layernorm.weight: False
base_model.model.model.layers.30.post_attention_layernorm.weight: False
base_model.model.model.layers.31.self_attn.q_proj.base_layer.weight: False
base_model.model.model.layers.31.self_attn.q_proj.lora_A.default.weight: True
base_model.model.model.layers.31.self_attn.q_proj.lora_B.default.weight: True
base_model.model.model.layers.31.self_attn.k_proj.base_layer.weight: False
base_model.model.model.layers.31.self_attn.k_proj.lora_A.default.weight: True
base_model.model.model.layers.31.self_attn.k_proj.lora_B.default.weight: True
base_model.model.model.layers.31.self_attn.v_proj.base_layer.weight: False
base_model.model.model.layers.31.self_attn.v_proj.lora_A.default.weight: True
base_model.model.model.layers.31.self_attn.v_proj.lora_B.default.weight: True
base_model.model.model.layers.31.self_attn.o_proj.base_layer.weight: False
base_model.model.model.layers.31.self_attn.o_proj.lora_A.default.weight: True
base_model.model.model.layers.31.self_attn.o_proj.lora_B.default.weight: True
base_model.model.model.layers.31.mlp.gate_proj.base_layer.weight: False
base_model.model.model.layers.31.mlp.gate_proj.lora_A.default.weight: True
base_model.model.model.layers.31.mlp.gate_proj.lora_B.default.weight: True
base_model.model.model.layers.31.mlp.up_proj.base_layer.weight: False
base_model.model.model.layers.31.mlp.up_proj.lora_A.default.weight: True
base_model.model.model.layers.31.mlp.up_proj.lora_B.default.weight: True
base_model.model.model.layers.31.mlp.down_proj.base_layer.weight: False
base_model.model.model.layers.31.mlp.down_proj.lora_A.default.weight: True
base_model.model.model.layers.31.mlp.down_proj.lora_B.default.weight: True
base_model.model.model.layers.31.input_layernorm.weight: False
base_model.model.model.layers.31.post_attention_layernorm.weight: False
base_model.model.model.norm.weight: False
base_model.model.lm_head.weight: False
=======Model loaded and ready for training========
/workspace/tulu_env/lib/python3.10/site-packages/peft/utils/save_and_load.py:473: UserWarning: Some weights of PeftModelForCausalLM were not initialized from the model checkpoint and are being ignored because you passed `ignore_mismatched_sizes=True`: - base_model.model.lm_head.weight: found shape torch.Size([128264, 4096]) in the checkpoint and torch.Size([128256, 4096]) in the model instantiated
- base_model.model.model.embed_tokens.weight: found shape torch.Size([128264, 4096]) in the checkpoint and torch.Size([128256, 4096]) in the model instantiated.
  warnings.warn(msg)
=========qloar  model is... ========
PeftModelForCausalLM(
  (base_model): LoraModel(
    (model): LlamaForCausalLM(
      (model): LlamaModel(
        (embed_tokens): Embedding(128256, 4096)
        (layers): ModuleList(
          (0-31): 32 x LlamaDecoderLayer(
            (self_attn): LlamaFlashAttention2(
              (q_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=64, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=64, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (k_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=64, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=64, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (v_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=64, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=64, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (o_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=64, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=64, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (rotary_emb): LlamaRotaryEmbedding()
            )
            (mlp): LlamaMLP(
              (gate_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=64, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=64, out_features=14336, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (up_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=64, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=64, out_features=14336, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (down_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=14336, out_features=64, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=64, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (act_fn): SiLU()
            )
            (input_layernorm): LlamaRMSNorm()
            (post_attention_layernorm): LlamaRMSNorm()
          )
        )
        (norm): LlamaRMSNorm()
        (rotary_emb): LlamaRotaryEmbedding()
      )
      (lm_head): Linear(in_features=4096, out_features=128256, bias=False)
    )
  )
)
trainable params: 167,772,160 || all params: 8,198,033,408 || trainable%: 2.0465
Base model params: 4708372480
LoRA params: 167772160
Total model params: 4876144640
base_model.model.model.embed_tokens.weight: False
base_model.model.model.layers.0.self_attn.q_proj.base_layer.weight: False
base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight: True
base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight: True
base_model.model.model.layers.0.self_attn.k_proj.base_layer.weight: False
base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight: True
base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight: True
base_model.model.model.layers.0.self_attn.v_proj.base_layer.weight: False
base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight: True
base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight: True
base_model.model.model.layers.0.self_attn.o_proj.base_layer.weight: False
base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight: True
base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight: True
base_model.model.model.layers.0.mlp.gate_proj.base_layer.weight: False
base_model.model.model.layers.0.mlp.gate_proj.lora_A.default.weight: True
base_model.model.model.layers.0.mlp.gate_proj.lora_B.default.weight: True
base_model.model.model.layers.0.mlp.up_proj.base_layer.weight: False
base_model.model.model.layers.0.mlp.up_proj.lora_A.default.weight: True
base_model.model.model.layers.0.mlp.up_proj.lora_B.default.weight: True
base_model.model.model.layers.0.mlp.down_proj.base_layer.weight: False
base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight: True
base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight: True
base_model.model.model.layers.0.input_layernorm.weight: False
base_model.model.model.layers.0.post_attention_layernorm.weight: False
base_model.model.model.layers.1.self_attn.q_proj.base_layer.weight: False
base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight: True
base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight: True
base_model.model.model.layers.1.self_attn.k_proj.base_layer.weight: False
base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight: True
base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight: True
base_model.model.model.layers.1.self_attn.v_proj.base_layer.weight: False
base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight: True
base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight: True
base_model.model.model.layers.1.self_attn.o_proj.base_layer.weight: False
base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight: True
base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight: True
base_model.model.model.layers.1.mlp.gate_proj.base_layer.weight: False
base_model.model.model.layers.1.mlp.gate_proj.lora_A.default.weight: True
base_model.model.model.layers.1.mlp.gate_proj.lora_B.default.weight: True
base_model.model.model.layers.1.mlp.up_proj.base_layer.weight: False
base_model.model.model.layers.1.mlp.up_proj.lora_A.default.weight: True
base_model.model.model.layers.1.mlp.up_proj.lora_B.default.weight: True
base_model.model.model.layers.1.mlp.down_proj.base_layer.weight: False
base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight: True
base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight: True
base_model.model.model.layers.1.input_layernorm.weight: False
base_model.model.model.layers.1.post_attention_layernorm.weight: False
base_model.model.model.layers.2.self_attn.q_proj.base_layer.weight: False
base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight: True
base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight: True
base_model.model.model.layers.2.self_attn.k_proj.base_layer.weight: False
base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight: True
base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight: True
base_model.model.model.layers.2.self_attn.v_proj.base_layer.weight: False
base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight: True
base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight: True
base_model.model.model.layers.2.self_attn.o_proj.base_layer.weight: False
base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight: True
base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight: True
base_model.model.model.layers.2.mlp.gate_proj.base_layer.weight: False
base_model.model.model.layers.2.mlp.gate_proj.lora_A.default.weight: True
base_model.model.model.layers.2.mlp.gate_proj.lora_B.default.weight: True
base_model.model.model.layers.2.mlp.up_proj.base_layer.weight: False
base_model.model.model.layers.2.mlp.up_proj.lora_A.default.weight: True
base_model.model.model.layers.2.mlp.up_proj.lora_B.default.weight: True
base_model.model.model.layers.2.mlp.down_proj.base_layer.weight: False
base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight: True
base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight: True
base_model.model.model.layers.2.input_layernorm.weight: False
base_model.model.model.layers.2.post_attention_layernorm.weight: False
base_model.model.model.layers.3.self_attn.q_proj.base_layer.weight: False
base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight: True
base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight: True
base_model.model.model.layers.3.self_attn.k_proj.base_layer.weight: False
base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight: True
base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight: True
base_model.model.model.layers.3.self_attn.v_proj.base_layer.weight: False
base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight: True
base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight: True
base_model.model.model.layers.3.self_attn.o_proj.base_layer.weight: False
base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight: True
base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight: True
base_model.model.model.layers.3.mlp.gate_proj.base_layer.weight: False
base_model.model.model.layers.3.mlp.gate_proj.lora_A.default.weight: True
base_model.model.model.layers.3.mlp.gate_proj.lora_B.default.weight: True
base_model.model.model.layers.3.mlp.up_proj.base_layer.weight: False
base_model.model.model.layers.3.mlp.up_proj.lora_A.default.weight: True
base_model.model.model.layers.3.mlp.up_proj.lora_B.default.weight: True
base_model.model.model.layers.3.mlp.down_proj.base_layer.weight: False
base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight: True
base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight: True
base_model.model.model.layers.3.input_layernorm.weight: False
base_model.model.model.layers.3.post_attention_layernorm.weight: False
base_model.model.model.layers.4.self_attn.q_proj.base_layer.weight: False
base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight: True
base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight: True
base_model.model.model.layers.4.self_attn.k_proj.base_layer.weight: False
base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight: True
base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight: True
base_model.model.model.layers.4.self_attn.v_proj.base_layer.weight: False
base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight: True
base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight: True
base_model.model.model.layers.4.self_attn.o_proj.base_layer.weight: False
base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight: True
base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight: True
base_model.model.model.layers.4.mlp.gate_proj.base_layer.weight: False
base_model.model.model.layers.4.mlp.gate_proj.lora_A.default.weight: True
base_model.model.model.layers.4.mlp.gate_proj.lora_B.default.weight: True
base_model.model.model.layers.4.mlp.up_proj.base_layer.weight: False
base_model.model.model.layers.4.mlp.up_proj.lora_A.default.weight: True
base_model.model.model.layers.4.mlp.up_proj.lora_B.default.weight: True
base_model.model.model.layers.4.mlp.down_proj.base_layer.weight: False
base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight: True
base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight: True
base_model.model.model.layers.4.input_layernorm.weight: False
base_model.model.model.layers.4.post_attention_layernorm.weight: False
base_model.model.model.layers.5.self_attn.q_proj.base_layer.weight: False
base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight: True
base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight: True
base_model.model.model.layers.5.self_attn.k_proj.base_layer.weight: False
base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight: True
base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight: True
base_model.model.model.layers.5.self_attn.v_proj.base_layer.weight: False
base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight: True
base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight: True
base_model.model.model.layers.5.self_attn.o_proj.base_layer.weight: False
base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight: True
base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight: True
base_model.model.model.layers.5.mlp.gate_proj.base_layer.weight: False
base_model.model.model.layers.5.mlp.gate_proj.lora_A.default.weight: True
base_model.model.model.layers.5.mlp.gate_proj.lora_B.default.weight: True
base_model.model.model.layers.5.mlp.up_proj.base_layer.weight: False
base_model.model.model.layers.5.mlp.up_proj.lora_A.default.weight: True
base_model.model.model.layers.5.mlp.up_proj.lora_B.default.weight: True
base_model.model.model.layers.5.mlp.down_proj.base_layer.weight: False
base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight: True
base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight: True
base_model.model.model.layers.5.input_layernorm.weight: False
base_model.model.model.layers.5.post_attention_layernorm.weight: False
base_model.model.model.layers.6.self_attn.q_proj.base_layer.weight: False
base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight: True
base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight: True
base_model.model.model.layers.6.self_attn.k_proj.base_layer.weight: False
base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight: True
base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight: True
base_model.model.model.layers.6.self_attn.v_proj.base_layer.weight: False
base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight: True
base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight: True
base_model.model.model.layers.6.self_attn.o_proj.base_layer.weight: False
base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight: True
base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight: True
base_model.model.model.layers.6.mlp.gate_proj.base_layer.weight: False
base_model.model.model.layers.6.mlp.gate_proj.lora_A.default.weight: True
base_model.model.model.layers.6.mlp.gate_proj.lora_B.default.weight: True
base_model.model.model.layers.6.mlp.up_proj.base_layer.weight: False
base_model.model.model.layers.6.mlp.up_proj.lora_A.default.weight: True
base_model.model.model.layers.6.mlp.up_proj.lora_B.default.weight: True
base_model.model.model.layers.6.mlp.down_proj.base_layer.weight: False
base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight: True
base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight: True
base_model.model.model.layers.6.input_layernorm.weight: False
base_model.model.model.layers.6.post_attention_layernorm.weight: False
base_model.model.model.layers.7.self_attn.q_proj.base_layer.weight: False
base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight: True
base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight: True
base_model.model.model.layers.7.self_attn.k_proj.base_layer.weight: False
base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight: True
base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight: True
base_model.model.model.layers.7.self_attn.v_proj.base_layer.weight: False
base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight: True
base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight: True
base_model.model.model.layers.7.self_attn.o_proj.base_layer.weight: False
base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight: True
base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight: True
base_model.model.model.layers.7.mlp.gate_proj.base_layer.weight: False
base_model.model.model.layers.7.mlp.gate_proj.lora_A.default.weight: True
base_model.model.model.layers.7.mlp.gate_proj.lora_B.default.weight: True
base_model.model.model.layers.7.mlp.up_proj.base_layer.weight: False
base_model.model.model.layers.7.mlp.up_proj.lora_A.default.weight: True
base_model.model.model.layers.7.mlp.up_proj.lora_B.default.weight: True
base_model.model.model.layers.7.mlp.down_proj.base_layer.weight: False
base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight: True
base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight: True
base_model.model.model.layers.7.input_layernorm.weight: False
base_model.model.model.layers.7.post_attention_layernorm.weight: False
base_model.model.model.layers.8.self_attn.q_proj.base_layer.weight: False
base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight: True
base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight: True
base_model.model.model.layers.8.self_attn.k_proj.base_layer.weight: False
base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight: True
base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight: True
base_model.model.model.layers.8.self_attn.v_proj.base_layer.weight: False
base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight: True
base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight: True
base_model.model.model.layers.8.self_attn.o_proj.base_layer.weight: False
base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight: True
base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight: True
base_model.model.model.layers.8.mlp.gate_proj.base_layer.weight: False
base_model.model.model.layers.8.mlp.gate_proj.lora_A.default.weight: True
base_model.model.model.layers.8.mlp.gate_proj.lora_B.default.weight: True
base_model.model.model.layers.8.mlp.up_proj.base_layer.weight: False
base_model.model.model.layers.8.mlp.up_proj.lora_A.default.weight: True
base_model.model.model.layers.8.mlp.up_proj.lora_B.default.weight: True
base_model.model.model.layers.8.mlp.down_proj.base_layer.weight: False
base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight: True
base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight: True
base_model.model.model.layers.8.input_layernorm.weight: False
base_model.model.model.layers.8.post_attention_layernorm.weight: False
base_model.model.model.layers.9.self_attn.q_proj.base_layer.weight: False
base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight: True
base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight: True
base_model.model.model.layers.9.self_attn.k_proj.base_layer.weight: False
base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight: True
base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight: True
base_model.model.model.layers.9.self_attn.v_proj.base_layer.weight: False
base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight: True
base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight: True
base_model.model.model.layers.9.self_attn.o_proj.base_layer.weight: False
base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight: True
base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight: True
base_model.model.model.layers.9.mlp.gate_proj.base_layer.weight: False
base_model.model.model.layers.9.mlp.gate_proj.lora_A.default.weight: True
base_model.model.model.layers.9.mlp.gate_proj.lora_B.default.weight: True
base_model.model.model.layers.9.mlp.up_proj.base_layer.weight: False
base_model.model.model.layers.9.mlp.up_proj.lora_A.default.weight: True
base_model.model.model.layers.9.mlp.up_proj.lora_B.default.weight: True
base_model.model.model.layers.9.mlp.down_proj.base_layer.weight: False
base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight: True
base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight: True
base_model.model.model.layers.9.input_layernorm.weight: False
base_model.model.model.layers.9.post_attention_layernorm.weight: False
base_model.model.model.layers.10.self_attn.q_proj.base_layer.weight: False
base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight: True
base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight: True
base_model.model.model.layers.10.self_attn.k_proj.base_layer.weight: False
base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight: True
base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight: True
base_model.model.model.layers.10.self_attn.v_proj.base_layer.weight: False
base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight: True
base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight: True
base_model.model.model.layers.10.self_attn.o_proj.base_layer.weight: False
base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight: True
base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight: True
base_model.model.model.layers.10.mlp.gate_proj.base_layer.weight: False
base_model.model.model.layers.10.mlp.gate_proj.lora_A.default.weight: True
base_model.model.model.layers.10.mlp.gate_proj.lora_B.default.weight: True
base_model.model.model.layers.10.mlp.up_proj.base_layer.weight: False
base_model.model.model.layers.10.mlp.up_proj.lora_A.default.weight: True
base_model.model.model.layers.10.mlp.up_proj.lora_B.default.weight: True
base_model.model.model.layers.10.mlp.down_proj.base_layer.weight: False
base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight: True
base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight: True
base_model.model.model.layers.10.input_layernorm.weight: False
base_model.model.model.layers.10.post_attention_layernorm.weight: False
base_model.model.model.layers.11.self_attn.q_proj.base_layer.weight: False
base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight: True
base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight: True
base_model.model.model.layers.11.self_attn.k_proj.base_layer.weight: False
base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight: True
base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight: True
base_model.model.model.layers.11.self_attn.v_proj.base_layer.weight: False
base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight: True
base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight: True
base_model.model.model.layers.11.self_attn.o_proj.base_layer.weight: False
base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight: True
base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight: True
base_model.model.model.layers.11.mlp.gate_proj.base_layer.weight: False
base_model.model.model.layers.11.mlp.gate_proj.lora_A.default.weight: True
base_model.model.model.layers.11.mlp.gate_proj.lora_B.default.weight: True
base_model.model.model.layers.11.mlp.up_proj.base_layer.weight: False
base_model.model.model.layers.11.mlp.up_proj.lora_A.default.weight: True
base_model.model.model.layers.11.mlp.up_proj.lora_B.default.weight: True
base_model.model.model.layers.11.mlp.down_proj.base_layer.weight: False
base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight: True
base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight: True
base_model.model.model.layers.11.input_layernorm.weight: False
base_model.model.model.layers.11.post_attention_layernorm.weight: False
base_model.model.model.layers.12.self_attn.q_proj.base_layer.weight: False
base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight: True
base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight: True
base_model.model.model.layers.12.self_attn.k_proj.base_layer.weight: False
base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight: True
base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight: True
base_model.model.model.layers.12.self_attn.v_proj.base_layer.weight: False
base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight: True
base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight: True
base_model.model.model.layers.12.self_attn.o_proj.base_layer.weight: False
base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight: True
base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight: True
base_model.model.model.layers.12.mlp.gate_proj.base_layer.weight: False
base_model.model.model.layers.12.mlp.gate_proj.lora_A.default.weight: True
base_model.model.model.layers.12.mlp.gate_proj.lora_B.default.weight: True
base_model.model.model.layers.12.mlp.up_proj.base_layer.weight: False
base_model.model.model.layers.12.mlp.up_proj.lora_A.default.weight: True
base_model.model.model.layers.12.mlp.up_proj.lora_B.default.weight: True
base_model.model.model.layers.12.mlp.down_proj.base_layer.weight: False
base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight: True
base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight: True
base_model.model.model.layers.12.input_layernorm.weight: False
base_model.model.model.layers.12.post_attention_layernorm.weight: False
base_model.model.model.layers.13.self_attn.q_proj.base_layer.weight: False
base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight: True
base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight: True
base_model.model.model.layers.13.self_attn.k_proj.base_layer.weight: False
base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight: True
base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight: True
base_model.model.model.layers.13.self_attn.v_proj.base_layer.weight: False
base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight: True
base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight: True
base_model.model.model.layers.13.self_attn.o_proj.base_layer.weight: False
base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight: True
base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight: True
base_model.model.model.layers.13.mlp.gate_proj.base_layer.weight: False
base_model.model.model.layers.13.mlp.gate_proj.lora_A.default.weight: True
base_model.model.model.layers.13.mlp.gate_proj.lora_B.default.weight: True
base_model.model.model.layers.13.mlp.up_proj.base_layer.weight: False
base_model.model.model.layers.13.mlp.up_proj.lora_A.default.weight: True
base_model.model.model.layers.13.mlp.up_proj.lora_B.default.weight: True
base_model.model.model.layers.13.mlp.down_proj.base_layer.weight: False
base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight: True
base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight: True
base_model.model.model.layers.13.input_layernorm.weight: False
base_model.model.model.layers.13.post_attention_layernorm.weight: False
base_model.model.model.layers.14.self_attn.q_proj.base_layer.weight: False
base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight: True
base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight: True
base_model.model.model.layers.14.self_attn.k_proj.base_layer.weight: False
base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight: True
base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight: True
base_model.model.model.layers.14.self_attn.v_proj.base_layer.weight: False
base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight: True
base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight: True
base_model.model.model.layers.14.self_attn.o_proj.base_layer.weight: False
base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight: True
base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight: True
base_model.model.model.layers.14.mlp.gate_proj.base_layer.weight: False
base_model.model.model.layers.14.mlp.gate_proj.lora_A.default.weight: True
base_model.model.model.layers.14.mlp.gate_proj.lora_B.default.weight: True
base_model.model.model.layers.14.mlp.up_proj.base_layer.weight: False
base_model.model.model.layers.14.mlp.up_proj.lora_A.default.weight: True
base_model.model.model.layers.14.mlp.up_proj.lora_B.default.weight: True
base_model.model.model.layers.14.mlp.down_proj.base_layer.weight: False
base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight: True
base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight: True
base_model.model.model.layers.14.input_layernorm.weight: False
base_model.model.model.layers.14.post_attention_layernorm.weight: False
base_model.model.model.layers.15.self_attn.q_proj.base_layer.weight: False
base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight: True
base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight: True
base_model.model.model.layers.15.self_attn.k_proj.base_layer.weight: False
base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight: True
base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight: True
base_model.model.model.layers.15.self_attn.v_proj.base_layer.weight: False
base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight: True
base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight: True
base_model.model.model.layers.15.self_attn.o_proj.base_layer.weight: False
base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight: True
base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight: True
base_model.model.model.layers.15.mlp.gate_proj.base_layer.weight: False
base_model.model.model.layers.15.mlp.gate_proj.lora_A.default.weight: True
base_model.model.model.layers.15.mlp.gate_proj.lora_B.default.weight: True
base_model.model.model.layers.15.mlp.up_proj.base_layer.weight: False
base_model.model.model.layers.15.mlp.up_proj.lora_A.default.weight: True
base_model.model.model.layers.15.mlp.up_proj.lora_B.default.weight: True
base_model.model.model.layers.15.mlp.down_proj.base_layer.weight: False
base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight: True
base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight: True
base_model.model.model.layers.15.input_layernorm.weight: False
base_model.model.model.layers.15.post_attention_layernorm.weight: False
base_model.model.model.layers.16.self_attn.q_proj.base_layer.weight: False
base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight: True
base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight: True
base_model.model.model.layers.16.self_attn.k_proj.base_layer.weight: False
base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight: True
base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight: True
base_model.model.model.layers.16.self_attn.v_proj.base_layer.weight: False
base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight: True
base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight: True
base_model.model.model.layers.16.self_attn.o_proj.base_layer.weight: False
base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight: True
base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight: True
base_model.model.model.layers.16.mlp.gate_proj.base_layer.weight: False
base_model.model.model.layers.16.mlp.gate_proj.lora_A.default.weight: True
base_model.model.model.layers.16.mlp.gate_proj.lora_B.default.weight: True
base_model.model.model.layers.16.mlp.up_proj.base_layer.weight: False
base_model.model.model.layers.16.mlp.up_proj.lora_A.default.weight: True
base_model.model.model.layers.16.mlp.up_proj.lora_B.default.weight: True
base_model.model.model.layers.16.mlp.down_proj.base_layer.weight: False
base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight: True
base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight: True
base_model.model.model.layers.16.input_layernorm.weight: False
base_model.model.model.layers.16.post_attention_layernorm.weight: False
base_model.model.model.layers.17.self_attn.q_proj.base_layer.weight: False
base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight: True
base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight: True
base_model.model.model.layers.17.self_attn.k_proj.base_layer.weight: False
base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight: True
base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight: True
base_model.model.model.layers.17.self_attn.v_proj.base_layer.weight: False
base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight: True
base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight: True
base_model.model.model.layers.17.self_attn.o_proj.base_layer.weight: False
base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight: True
base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight: True
base_model.model.model.layers.17.mlp.gate_proj.base_layer.weight: False
base_model.model.model.layers.17.mlp.gate_proj.lora_A.default.weight: True
base_model.model.model.layers.17.mlp.gate_proj.lora_B.default.weight: True
base_model.model.model.layers.17.mlp.up_proj.base_layer.weight: False
base_model.model.model.layers.17.mlp.up_proj.lora_A.default.weight: True
base_model.model.model.layers.17.mlp.up_proj.lora_B.default.weight: True
base_model.model.model.layers.17.mlp.down_proj.base_layer.weight: False
base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight: True
base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight: True
base_model.model.model.layers.17.input_layernorm.weight: False
base_model.model.model.layers.17.post_attention_layernorm.weight: False
base_model.model.model.layers.18.self_attn.q_proj.base_layer.weight: False
base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight: True
base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight: True
base_model.model.model.layers.18.self_attn.k_proj.base_layer.weight: False
base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight: True
base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight: True
base_model.model.model.layers.18.self_attn.v_proj.base_layer.weight: False
base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight: True
base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight: True
base_model.model.model.layers.18.self_attn.o_proj.base_layer.weight: False
base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight: True
base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight: True
base_model.model.model.layers.18.mlp.gate_proj.base_layer.weight: False
base_model.model.model.layers.18.mlp.gate_proj.lora_A.default.weight: True
base_model.model.model.layers.18.mlp.gate_proj.lora_B.default.weight: True
base_model.model.model.layers.18.mlp.up_proj.base_layer.weight: False
base_model.model.model.layers.18.mlp.up_proj.lora_A.default.weight: True
base_model.model.model.layers.18.mlp.up_proj.lora_B.default.weight: True
base_model.model.model.layers.18.mlp.down_proj.base_layer.weight: False
base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight: True
base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight: True
base_model.model.model.layers.18.input_layernorm.weight: False
base_model.model.model.layers.18.post_attention_layernorm.weight: False
base_model.model.model.layers.19.self_attn.q_proj.base_layer.weight: False
base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight: True
base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight: True
base_model.model.model.layers.19.self_attn.k_proj.base_layer.weight: False
base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight: True
base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight: True
base_model.model.model.layers.19.self_attn.v_proj.base_layer.weight: False
base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight: True
base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight: True
base_model.model.model.layers.19.self_attn.o_proj.base_layer.weight: False
base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight: True
base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight: True
base_model.model.model.layers.19.mlp.gate_proj.base_layer.weight: False
base_model.model.model.layers.19.mlp.gate_proj.lora_A.default.weight: True
base_model.model.model.layers.19.mlp.gate_proj.lora_B.default.weight: True
base_model.model.model.layers.19.mlp.up_proj.base_layer.weight: False
base_model.model.model.layers.19.mlp.up_proj.lora_A.default.weight: True
base_model.model.model.layers.19.mlp.up_proj.lora_B.default.weight: True
base_model.model.model.layers.19.mlp.down_proj.base_layer.weight: False
base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight: True
base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight: True
base_model.model.model.layers.19.input_layernorm.weight: False
base_model.model.model.layers.19.post_attention_layernorm.weight: False
base_model.model.model.layers.20.self_attn.q_proj.base_layer.weight: False
base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight: True
base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight: True
base_model.model.model.layers.20.self_attn.k_proj.base_layer.weight: False
base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight: True
base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight: True
base_model.model.model.layers.20.self_attn.v_proj.base_layer.weight: False
base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight: True
base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight: True
base_model.model.model.layers.20.self_attn.o_proj.base_layer.weight: False
base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight: True
base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight: True
base_model.model.model.layers.20.mlp.gate_proj.base_layer.weight: False
base_model.model.model.layers.20.mlp.gate_proj.lora_A.default.weight: True
base_model.model.model.layers.20.mlp.gate_proj.lora_B.default.weight: True
base_model.model.model.layers.20.mlp.up_proj.base_layer.weight: False
base_model.model.model.layers.20.mlp.up_proj.lora_A.default.weight: True
base_model.model.model.layers.20.mlp.up_proj.lora_B.default.weight: True
base_model.model.model.layers.20.mlp.down_proj.base_layer.weight: False
base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight: True
base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight: True
base_model.model.model.layers.20.input_layernorm.weight: False
base_model.model.model.layers.20.post_attention_layernorm.weight: False
base_model.model.model.layers.21.self_attn.q_proj.base_layer.weight: False
base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight: True
base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight: True
base_model.model.model.layers.21.self_attn.k_proj.base_layer.weight: False
base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight: True
base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight: True
base_model.model.model.layers.21.self_attn.v_proj.base_layer.weight: False
base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight: True
base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight: True
base_model.model.model.layers.21.self_attn.o_proj.base_layer.weight: False
base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight: True
base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight: True
base_model.model.model.layers.21.mlp.gate_proj.base_layer.weight: False
base_model.model.model.layers.21.mlp.gate_proj.lora_A.default.weight: True
base_model.model.model.layers.21.mlp.gate_proj.lora_B.default.weight: True
base_model.model.model.layers.21.mlp.up_proj.base_layer.weight: False
base_model.model.model.layers.21.mlp.up_proj.lora_A.default.weight: True
base_model.model.model.layers.21.mlp.up_proj.lora_B.default.weight: True
base_model.model.model.layers.21.mlp.down_proj.base_layer.weight: False
base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight: True
base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight: True
base_model.model.model.layers.21.input_layernorm.weight: False
base_model.model.model.layers.21.post_attention_layernorm.weight: False
base_model.model.model.layers.22.self_attn.q_proj.base_layer.weight: False
base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight: True
base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight: True
base_model.model.model.layers.22.self_attn.k_proj.base_layer.weight: False
base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight: True
base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight: True
base_model.model.model.layers.22.self_attn.v_proj.base_layer.weight: False
base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight: True
base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight: True
base_model.model.model.layers.22.self_attn.o_proj.base_layer.weight: False
base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight: True
base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight: True
base_model.model.model.layers.22.mlp.gate_proj.base_layer.weight: False
base_model.model.model.layers.22.mlp.gate_proj.lora_A.default.weight: True
base_model.model.model.layers.22.mlp.gate_proj.lora_B.default.weight: True
base_model.model.model.layers.22.mlp.up_proj.base_layer.weight: False
base_model.model.model.layers.22.mlp.up_proj.lora_A.default.weight: True
base_model.model.model.layers.22.mlp.up_proj.lora_B.default.weight: True
base_model.model.model.layers.22.mlp.down_proj.base_layer.weight: False
base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight: True
base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight: True
base_model.model.model.layers.22.input_layernorm.weight: False
base_model.model.model.layers.22.post_attention_layernorm.weight: False
base_model.model.model.layers.23.self_attn.q_proj.base_layer.weight: False
base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight: True
base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight: True
base_model.model.model.layers.23.self_attn.k_proj.base_layer.weight: False
base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight: True
base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight: True
base_model.model.model.layers.23.self_attn.v_proj.base_layer.weight: False
base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight: True
base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight: True
base_model.model.model.layers.23.self_attn.o_proj.base_layer.weight: False
base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight: True
base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight: True
base_model.model.model.layers.23.mlp.gate_proj.base_layer.weight: False
base_model.model.model.layers.23.mlp.gate_proj.lora_A.default.weight: True
base_model.model.model.layers.23.mlp.gate_proj.lora_B.default.weight: True
base_model.model.model.layers.23.mlp.up_proj.base_layer.weight: False
base_model.model.model.layers.23.mlp.up_proj.lora_A.default.weight: True
base_model.model.model.layers.23.mlp.up_proj.lora_B.default.weight: True
base_model.model.model.layers.23.mlp.down_proj.base_layer.weight: False
base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight: True
base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight: True
base_model.model.model.layers.23.input_layernorm.weight: False
base_model.model.model.layers.23.post_attention_layernorm.weight: False
base_model.model.model.layers.24.self_attn.q_proj.base_layer.weight: False
base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight: True
base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight: True
base_model.model.model.layers.24.self_attn.k_proj.base_layer.weight: False
base_model.model.model.layers.24.self_attn.k_proj.lora_A.default.weight: True
base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight: True
base_model.model.model.layers.24.self_attn.v_proj.base_layer.weight: False
base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight: True
base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight: True
base_model.model.model.layers.24.self_attn.o_proj.base_layer.weight: False
base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight: True
base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight: True
base_model.model.model.layers.24.mlp.gate_proj.base_layer.weight: False
base_model.model.model.layers.24.mlp.gate_proj.lora_A.default.weight: True
base_model.model.model.layers.24.mlp.gate_proj.lora_B.default.weight: True
base_model.model.model.layers.24.mlp.up_proj.base_layer.weight: False
base_model.model.model.layers.24.mlp.up_proj.lora_A.default.weight: True
base_model.model.model.layers.24.mlp.up_proj.lora_B.default.weight: True
base_model.model.model.layers.24.mlp.down_proj.base_layer.weight: False
base_model.model.model.layers.24.mlp.down_proj.lora_A.default.weight: True
base_model.model.model.layers.24.mlp.down_proj.lora_B.default.weight: True
base_model.model.model.layers.24.input_layernorm.weight: False
base_model.model.model.layers.24.post_attention_layernorm.weight: False
base_model.model.model.layers.25.self_attn.q_proj.base_layer.weight: False
base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight: True
base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight: True
base_model.model.model.layers.25.self_attn.k_proj.base_layer.weight: False
base_model.model.model.layers.25.self_attn.k_proj.lora_A.default.weight: True
base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight: True
base_model.model.model.layers.25.self_attn.v_proj.base_layer.weight: False
base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight: True
base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight: True
base_model.model.model.layers.25.self_attn.o_proj.base_layer.weight: False
base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight: True
base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight: True
base_model.model.model.layers.25.mlp.gate_proj.base_layer.weight: False
base_model.model.model.layers.25.mlp.gate_proj.lora_A.default.weight: True
base_model.model.model.layers.25.mlp.gate_proj.lora_B.default.weight: True
base_model.model.model.layers.25.mlp.up_proj.base_layer.weight: False
base_model.model.model.layers.25.mlp.up_proj.lora_A.default.weight: True
base_model.model.model.layers.25.mlp.up_proj.lora_B.default.weight: True
base_model.model.model.layers.25.mlp.down_proj.base_layer.weight: False
base_model.model.model.layers.25.mlp.down_proj.lora_A.default.weight: True
base_model.model.model.layers.25.mlp.down_proj.lora_B.default.weight: True
base_model.model.model.layers.25.input_layernorm.weight: False
base_model.model.model.layers.25.post_attention_layernorm.weight: False
base_model.model.model.layers.26.self_attn.q_proj.base_layer.weight: False
base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight: True
base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight: True
base_model.model.model.layers.26.self_attn.k_proj.base_layer.weight: False
base_model.model.model.layers.26.self_attn.k_proj.lora_A.default.weight: True
base_model.model.model.layers.26.self_attn.k_proj.lora_B.default.weight: True
base_model.model.model.layers.26.self_attn.v_proj.base_layer.weight: False
base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight: True
base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight: True
base_model.model.model.layers.26.self_attn.o_proj.base_layer.weight: False
base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight: True
base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight: True
base_model.model.model.layers.26.mlp.gate_proj.base_layer.weight: False
base_model.model.model.layers.26.mlp.gate_proj.lora_A.default.weight: True
base_model.model.model.layers.26.mlp.gate_proj.lora_B.default.weight: True
base_model.model.model.layers.26.mlp.up_proj.base_layer.weight: False
base_model.model.model.layers.26.mlp.up_proj.lora_A.default.weight: True
base_model.model.model.layers.26.mlp.up_proj.lora_B.default.weight: True
base_model.model.model.layers.26.mlp.down_proj.base_layer.weight: False
base_model.model.model.layers.26.mlp.down_proj.lora_A.default.weight: True
base_model.model.model.layers.26.mlp.down_proj.lora_B.default.weight: True
base_model.model.model.layers.26.input_layernorm.weight: False
base_model.model.model.layers.26.post_attention_layernorm.weight: False
base_model.model.model.layers.27.self_attn.q_proj.base_layer.weight: False
base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight: True
base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight: True
base_model.model.model.layers.27.self_attn.k_proj.base_layer.weight: False
base_model.model.model.layers.27.self_attn.k_proj.lora_A.default.weight: True
base_model.model.model.layers.27.self_attn.k_proj.lora_B.default.weight: True
base_model.model.model.layers.27.self_attn.v_proj.base_layer.weight: False
base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight: True
base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight: True
base_model.model.model.layers.27.self_attn.o_proj.base_layer.weight: False
base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight: True
base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight: True
base_model.model.model.layers.27.mlp.gate_proj.base_layer.weight: False
base_model.model.model.layers.27.mlp.gate_proj.lora_A.default.weight: True
base_model.model.model.layers.27.mlp.gate_proj.lora_B.default.weight: True
base_model.model.model.layers.27.mlp.up_proj.base_layer.weight: False
base_model.model.model.layers.27.mlp.up_proj.lora_A.default.weight: True
base_model.model.model.layers.27.mlp.up_proj.lora_B.default.weight: True
base_model.model.model.layers.27.mlp.down_proj.base_layer.weight: False
base_model.model.model.layers.27.mlp.down_proj.lora_A.default.weight: True
base_model.model.model.layers.27.mlp.down_proj.lora_B.default.weight: True
base_model.model.model.layers.27.input_layernorm.weight: False
base_model.model.model.layers.27.post_attention_layernorm.weight: False
base_model.model.model.layers.28.self_attn.q_proj.base_layer.weight: False
base_model.model.model.layers.28.self_attn.q_proj.lora_A.default.weight: True
base_model.model.model.layers.28.self_attn.q_proj.lora_B.default.weight: True
base_model.model.model.layers.28.self_attn.k_proj.base_layer.weight: False
base_model.model.model.layers.28.self_attn.k_proj.lora_A.default.weight: True
base_model.model.model.layers.28.self_attn.k_proj.lora_B.default.weight: True
base_model.model.model.layers.28.self_attn.v_proj.base_layer.weight: False
base_model.model.model.layers.28.self_attn.v_proj.lora_A.default.weight: True
base_model.model.model.layers.28.self_attn.v_proj.lora_B.default.weight: True
base_model.model.model.layers.28.self_attn.o_proj.base_layer.weight: False
base_model.model.model.layers.28.self_attn.o_proj.lora_A.default.weight: True
base_model.model.model.layers.28.self_attn.o_proj.lora_B.default.weight: True
base_model.model.model.layers.28.mlp.gate_proj.base_layer.weight: False
base_model.model.model.layers.28.mlp.gate_proj.lora_A.default.weight: True
base_model.model.model.layers.28.mlp.gate_proj.lora_B.default.weight: True
base_model.model.model.layers.28.mlp.up_proj.base_layer.weight: False
base_model.model.model.layers.28.mlp.up_proj.lora_A.default.weight: True
base_model.model.model.layers.28.mlp.up_proj.lora_B.default.weight: True
base_model.model.model.layers.28.mlp.down_proj.base_layer.weight: False
base_model.model.model.layers.28.mlp.down_proj.lora_A.default.weight: True
base_model.model.model.layers.28.mlp.down_proj.lora_B.default.weight: True
base_model.model.model.layers.28.input_layernorm.weight: False
base_model.model.model.layers.28.post_attention_layernorm.weight: False
base_model.model.model.layers.29.self_attn.q_proj.base_layer.weight: False
base_model.model.model.layers.29.self_attn.q_proj.lora_A.default.weight: True
base_model.model.model.layers.29.self_attn.q_proj.lora_B.default.weight: True
base_model.model.model.layers.29.self_attn.k_proj.base_layer.weight: False
base_model.model.model.layers.29.self_attn.k_proj.lora_A.default.weight: True
base_model.model.model.layers.29.self_attn.k_proj.lora_B.default.weight: True
base_model.model.model.layers.29.self_attn.v_proj.base_layer.weight: False
base_model.model.model.layers.29.self_attn.v_proj.lora_A.default.weight: True
base_model.model.model.layers.29.self_attn.v_proj.lora_B.default.weight: True
base_model.model.model.layers.29.self_attn.o_proj.base_layer.weight: False
base_model.model.model.layers.29.self_attn.o_proj.lora_A.default.weight: True
base_model.model.model.layers.29.self_attn.o_proj.lora_B.default.weight: True
base_model.model.model.layers.29.mlp.gate_proj.base_layer.weight: False
base_model.model.model.layers.29.mlp.gate_proj.lora_A.default.weight: True
base_model.model.model.layers.29.mlp.gate_proj.lora_B.default.weight: True
base_model.model.model.layers.29.mlp.up_proj.base_layer.weight: False
base_model.model.model.layers.29.mlp.up_proj.lora_A.default.weight: True
base_model.model.model.layers.29.mlp.up_proj.lora_B.default.weight: True
base_model.model.model.layers.29.mlp.down_proj.base_layer.weight: False
base_model.model.model.layers.29.mlp.down_proj.lora_A.default.weight: True
base_model.model.model.layers.29.mlp.down_proj.lora_B.default.weight: True
base_model.model.model.layers.29.input_layernorm.weight: False
base_model.model.model.layers.29.post_attention_layernorm.weight: False
base_model.model.model.layers.30.self_attn.q_proj.base_layer.weight: False
base_model.model.model.layers.30.self_attn.q_proj.lora_A.default.weight: True
base_model.model.model.layers.30.self_attn.q_proj.lora_B.default.weight: True
base_model.model.model.layers.30.self_attn.k_proj.base_layer.weight: False
base_model.model.model.layers.30.self_attn.k_proj.lora_A.default.weight: True
base_model.model.model.layers.30.self_attn.k_proj.lora_B.default.weight: True
base_model.model.model.layers.30.self_attn.v_proj.base_layer.weight: False
base_model.model.model.layers.30.self_attn.v_proj.lora_A.default.weight: True
base_model.model.model.layers.30.self_attn.v_proj.lora_B.default.weight: True
base_model.model.model.layers.30.self_attn.o_proj.base_layer.weight: False
base_model.model.model.layers.30.self_attn.o_proj.lora_A.default.weight: True
base_model.model.model.layers.30.self_attn.o_proj.lora_B.default.weight: True
base_model.model.model.layers.30.mlp.gate_proj.base_layer.weight: False
base_model.model.model.layers.30.mlp.gate_proj.lora_A.default.weight: True
base_model.model.model.layers.30.mlp.gate_proj.lora_B.default.weight: True
base_model.model.model.layers.30.mlp.up_proj.base_layer.weight: False
base_model.model.model.layers.30.mlp.up_proj.lora_A.default.weight: True
base_model.model.model.layers.30.mlp.up_proj.lora_B.default.weight: True
base_model.model.model.layers.30.mlp.down_proj.base_layer.weight: False
base_model.model.model.layers.30.mlp.down_proj.lora_A.default.weight: True
base_model.model.model.layers.30.mlp.down_proj.lora_B.default.weight: True
base_model.model.model.layers.30.input_layernorm.weight: False
base_model.model.model.layers.30.post_attention_layernorm.weight: False
base_model.model.model.layers.31.self_attn.q_proj.base_layer.weight: False
base_model.model.model.layers.31.self_attn.q_proj.lora_A.default.weight: True
base_model.model.model.layers.31.self_attn.q_proj.lora_B.default.weight: True
base_model.model.model.layers.31.self_attn.k_proj.base_layer.weight: False
base_model.model.model.layers.31.self_attn.k_proj.lora_A.default.weight: True
base_model.model.model.layers.31.self_attn.k_proj.lora_B.default.weight: True
base_model.model.model.layers.31.self_attn.v_proj.base_layer.weight: False
base_model.model.model.layers.31.self_attn.v_proj.lora_A.default.weight: True
base_model.model.model.layers.31.self_attn.v_proj.lora_B.default.weight: True
base_model.model.model.layers.31.self_attn.o_proj.base_layer.weight: False
base_model.model.model.layers.31.self_attn.o_proj.lora_A.default.weight: True
base_model.model.model.layers.31.self_attn.o_proj.lora_B.default.weight: True
base_model.model.model.layers.31.mlp.gate_proj.base_layer.weight: False
base_model.model.model.layers.31.mlp.gate_proj.lora_A.default.weight: True
base_model.model.model.layers.31.mlp.gate_proj.lora_B.default.weight: True
base_model.model.model.layers.31.mlp.up_proj.base_layer.weight: False
base_model.model.model.layers.31.mlp.up_proj.lora_A.default.weight: True
base_model.model.model.layers.31.mlp.up_proj.lora_B.default.weight: True
base_model.model.model.layers.31.mlp.down_proj.base_layer.weight: False
base_model.model.model.layers.31.mlp.down_proj.lora_A.default.weight: True
base_model.model.model.layers.31.mlp.down_proj.lora_B.default.weight: True
base_model.model.model.layers.31.input_layernorm.weight: False
base_model.model.model.layers.31.post_attention_layernorm.weight: False
base_model.model.model.norm.weight: False
base_model.model.lm_head.weight: False
=======Model loaded and ready for training========
INFO:__main__:Limiting training samples to 100 from 92858.
INFO:__main__:Sample 81 of the training set: {'chosen_input_ids': tensor([    27,     91,    882,     91,    397,  30138,    793,   4195,    964,
            25,    436,  48136,    329,   1805,    271,  51428,     25,  99593,
          2751,   1139,  19143,    304,  13596,   3314,    520,    393,   2891,
            88,   5726,     72,     35,  47784,     11,    706,    311,   5101,
          2268,   3019,     25,    358,   2846,    912,  15779,     11,    719,
           358,   1097,   5128,  11537,    449,    279,   2383,    320,  30562,
           304,  12544,     11,    659,   9232,    323,   6166,   7016,     11,
          5623,     14,  47921,   2383,     11,   5099,    570,   4452,    358,
          2846,   5115,  22568,    389,    420,    832,    382,   4516,    856,
         23601,    323,   1077,   6411,   4333,    320,   4345,   8220,   6913,
             8,    304,  29538,     11,    733,    311,   1518,    393,   2891,
            88,   5726,     72,     35,    304,   1561,  27008,     13,   4800,
           279,   6411,   7564,   5828,    956,   2728,   8041,    311,   5387,
          2385,     13,   3092,  42988,    323,    279,   7564,     11,  29850,
           477,   8996,     11,   4018,   1063,   1584,    304,   4156,    315,
           264,   7564,     11,    889,   3250,    956,   2512,     11,    719,
           813,   8834,  23601,    436,   1154,    323,  61851,    856,  41126,
            13,   2435,    636,   1139,    433,     11,   1364,   2795,    814,
          2225,  19336,    922,    220,     19,   3115,     11,    856,  42988,
         12098,    264,   6573,    304,   1077,   3663,    323,    430,    596,
           430,     13,  22172,   5900,    389,   4717,    287,     11,   8996,
            11,    889,  34672,    382,  13575,  44806,      6,    264,   2478,
          4520,   3010,     11,   1364,    374,   2231,    304,    279,   1203,
           315,    264,   6293,   1841,    555,   4868,     11,   1405,    279,
          6411,   7564,   2736,    374,    320,    383,   5828,    956,   1524,
          3815,   4205,    705,    323,   1518,    279,   8834,   3828,   2133,
           389,    922,   1268,    814,    279,   2466,   3776,  36157,    323,
           279,   3828,    342,   3811,    709,    389,   1077,    320,    438,
           279,  26923,   1120,  16387,   7113,    304,   9306,    570,   4815,
         57377,  21701,   1124,   1022,    520,    279,   8952,     11,   2795,
           568,  13919,    856,  41126,   6801,   5147,    304,    430,    814,
          3287,    956,    656,   4205,     11,    719,   1606,   1364,    596,
         21039,  11965,     11,    814,  38023,    733,    311,   5590,     13,
          1283,   3250,    956,   1650,    279,   4333,    596,   6411,   2307,
         22625,     11,    719,    568,    706,    311,    733,    311,   5590,
          2288,     13,   4815,   4071,    279,   3575,    374,    856,  41126,
           374,  16706,   1203,   3432,     11,  29538,    374,  42436,   4028,
           279,   3224,     13,   3005,   3250,    956,    617,    279,  20769,
           311,  11722,   1203,     11,   6463,    279,   3300,     13,    358,
          2846,   2771,   1364,    649,    636,    264,  15779,    311,   4097,
          1077,    304,  28310,    689,    320,   9210,    596,   1148,   1274,
           656,    304,   1521,   4595,    315,  15082,     11,   1314,  10380,
           719,    430,    596,    264,   2766,  11646,    323,  27873,   2195,
          4516,    358,   1541,    956,   1440,   1148,    420,    374,     13,
          2435,  15058,    956,  11684,     11,    814,  15058,    956,  12800,
            13,   3639,   3169,    315,   5590,    374,    420,     30,   2435,
           617,    311,  73293,    872,   1162,   1603,    264,  11913,    382,
          5159,   8101,    374,    279,   8834,   3828,   2834,    956,   1524,
          1501,    709,    311,   5590,     11,    719,   1148,    374,   1364,
         10171,    311,    656,    304,    420,   6671,     30,   4815,  12947,
           382,  13778,     26,   7842,    512,     27,     91,  78191,     91,
           397,  55611,  59797,    856,  41126,  46671,   1077,    520,    264,
         21497,    304,  29538,     11,    505,    902,   1364,    596,  16706,
          1203,   2162,   3432,     11,    902,    374,   2860,  28718,    323,
          1364,   4295,    856,  41126,   1176,    323,   1524,   6293,  13919,
          1077,     13,   3005,    374,  10171,    311,   1501,    709,    304,
          5590,   1828,   2046,    323,  73293,   1077,   1162,     11,    902,
           374,  27873, 128001]), 'chosen_labels': tensor([  -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,  55611,  59797,    856,  41126,  46671,   1077,    520,    264,
         21497,    304,  29538,     11,    505,    902,   1364,    596,  16706,
          1203,   2162,   3432,     11,    902,    374,   2860,  28718,    323,
          1364,   4295,    856,  41126,   1176,    323,   1524,   6293,  13919,
          1077,     13,   3005,    374,  10171,    311,   1501,    709,    304,
          5590,   1828,   2046,    323,  73293,   1077,   1162,     11,    902,
           374,  27873, 128001]), 'chosen_attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), 'rejected_input_ids': tensor([    27,     91,    882,     91,    397,  30138,    793,   4195,    964,
            25,    436,  48136,    329,   1805,    271,  51428,     25,  99593,
          2751,   1139,  19143,    304,  13596,   3314,    520,    393,   2891,
            88,   5726,     72,     35,  47784,     11,    706,    311,   5101,
          2268,   3019,     25,    358,   2846,    912,  15779,     11,    719,
           358,   1097,   5128,  11537,    449,    279,   2383,    320,  30562,
           304,  12544,     11,    659,   9232,    323,   6166,   7016,     11,
          5623,     14,  47921,   2383,     11,   5099,    570,   4452,    358,
          2846,   5115,  22568,    389,    420,    832,    382,   4516,    856,
         23601,    323,   1077,   6411,   4333,    320,   4345,   8220,   6913,
             8,    304,  29538,     11,    733,    311,   1518,    393,   2891,
            88,   5726,     72,     35,    304,   1561,  27008,     13,   4800,
           279,   6411,   7564,   5828,    956,   2728,   8041,    311,   5387,
          2385,     13,   3092,  42988,    323,    279,   7564,     11,  29850,
           477,   8996,     11,   4018,   1063,   1584,    304,   4156,    315,
           264,   7564,     11,    889,   3250,    956,   2512,     11,    719,
           813,   8834,  23601,    436,   1154,    323,  61851,    856,  41126,
            13,   2435,    636,   1139,    433,     11,   1364,   2795,    814,
          2225,  19336,    922,    220,     19,   3115,     11,    856,  42988,
         12098,    264,   6573,    304,   1077,   3663,    323,    430,    596,
           430,     13,  22172,   5900,    389,   4717,    287,     11,   8996,
            11,    889,  34672,    382,  13575,  44806,      6,    264,   2478,
          4520,   3010,     11,   1364,    374,   2231,    304,    279,   1203,
           315,    264,   6293,   1841,    555,   4868,     11,   1405,    279,
          6411,   7564,   2736,    374,    320,    383,   5828,    956,   1524,
          3815,   4205,    705,    323,   1518,    279,   8834,   3828,   2133,
           389,    922,   1268,    814,    279,   2466,   3776,  36157,    323,
           279,   3828,    342,   3811,    709,    389,   1077,    320,    438,
           279,  26923,   1120,  16387,   7113,    304,   9306,    570,   4815,
         57377,  21701,   1124,   1022,    520,    279,   8952,     11,   2795,
           568,  13919,    856,  41126,   6801,   5147,    304,    430,    814,
          3287,    956,    656,   4205,     11,    719,   1606,   1364,    596,
         21039,  11965,     11,    814,  38023,    733,    311,   5590,     13,
          1283,   3250,    956,   1650,    279,   4333,    596,   6411,   2307,
         22625,     11,    719,    568,    706,    311,    733,    311,   5590,
          2288,     13,   4815,   4071,    279,   3575,    374,    856,  41126,
           374,  16706,   1203,   3432,     11,  29538,    374,  42436,   4028,
           279,   3224,     13,   3005,   3250,    956,    617,    279,  20769,
           311,  11722,   1203,     11,   6463,    279,   3300,     13,    358,
          2846,   2771,   1364,    649,    636,    264,  15779,    311,   4097,
          1077,    304,  28310,    689,    320,   9210,    596,   1148,   1274,
           656,    304,   1521,   4595,    315,  15082,     11,   1314,  10380,
           719,    430,    596,    264,   2766,  11646,    323,  27873,   2195,
          4516,    358,   1541,    956,   1440,   1148,    420,    374,     13,
          2435,  15058,    956,  11684,     11,    814,  15058,    956,  12800,
            13,   3639,   3169,    315,   5590,    374,    420,     30,   2435,
           617,    311,  73293,    872,   1162,   1603,    264,  11913,    382,
          5159,   8101,    374,    279,   8834,   3828,   2834,    956,   1524,
          1501,    709,    311,   5590,     11,    719,   1148,    374,   1364,
         10171,    311,    656,    304,    420,   6671,     30,   4815,  12947,
           382,  13778,     26,   7842,    512,     27,     91,  78191,     91,
           397,   5159,  41126,   5334,   1139,    264,   4465,    304,   1561,
         27008,     11,   5334,  12800,   1306,   1063,   3828,   5334,  46671,
            11,    323,  43394,    279,  11213,   1203,     13,   3639,    596,
           279,   5590,   1920,     30, 128001]), 'rejected_labels': tensor([  -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   5159,  41126,   5334,   1139,    264,   4465,    304,   1561,
         27008,     11,   5334,  12800,   1306,   1063,   3828,   5334,  46671,
            11,    323,  43394,    279,  11213,   1203,     13,   3639,    596,
           279,   5590,   1920,     30, 128001]), 'rejected_attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}.
INFO:__main__:Sample 14 of the training set: {'chosen_input_ids': tensor([    27,     91,    882,     91,    397,  30138,    793,   4195,    964,
            25,    436,     14,  86924,    271,  51428,     25,  19418,    499,
           387,  13088,    422,    701,   5745,    889,   6439,    220,   6330,
            15,   8931,   3201,     11,    304,   2500,   3224,     11,    889,
           706,  12263,    499,    220,     19,   3115,   2533,   5587,     11,
          3309,    499,   8469,    315,    892,     11,  28653,  40715,     11,
           358,   3021,    499,   1633,   1790,     11,    358,   2133,    311,
           473,   1944,    388,  18396,    369,  27296,     11,   5353,   1202,
         11689,   6138,    505,    279,  12109,    358,   2133,    311,   7172,
         13200,    520,    382,   3019,     25,    220,     16,      8,    358,
          2751,    264,  38852,     40,    304,    220,    679,     19,    271,
            17,      8,    358,    617,    264,  11364,  23601,    889,   6439,
           304,  30613,    271,     18,      8,    358,   3974,    304,    459,
         25629,   6424,    315,  32669,   1291,   4409,     11,  29974,    271,
            19,      8,    358,   1097,   1633,   1790,    304,   3021,    449,
          1077,    382,     20,      8,    358,    990,    264,  15234,   9899,
            11,    386,   7424,     11,    220,     20,    309,    482,    220,
            16,   5298,   1174,    358,   1205,    311,  15508,    709,    520,
           220,     19,    309,    311,    387,    520,    990,    382,     21,
             8,    358,    617,  12263,  30613,    220,     19,   3115,   2533,
          5587,    323,    584,   2322,    389,  43274,   1174,  10280,  32460,
           220,    679,     20,     13,   4815,     22,      8,   3005,    374,
           264,  11364,     11,  20333,     11,  24415,     11,  10437,   5333,
            11,    358,   1097,   9539,   5029,   1648,    927,    856,   2010,
           323,    264,   2204,  10966,    382,     23,      8,    358,  12570,
           311,   1077,  13985,  10043,   2162,    505,    990,     13,    358,
           617,    264,  24759,    369,    264,   1949,   3814,    520,    264,
          2254,  12109,     11,    323,    358,   3309,   1077,     11,   1427,
           358,    649,    956,   6678,   3131,    617,    810,   1109,    358,
          1288,   7172,     11,    719,    311,    757,    420,    374,   1093,
           264,    220,    717,   6596,  20769,     11,    323,    358,   2834,
           956,   6678,     11,    358,   6227,    279,   1566,    892,    358,
           574,    520,    279,  12109,     11,    358,   3287,    956,   1093,
           279,  15657,     13,    358,    690,   2133,    311,    473,   1944,
           388,     11,    323,    358,   1514,   3725,  68329,    449,   1077,
            11,    358,   1097,   2133,    369,    279,   3691,     13,   3005,
          2646,   1071,   1541,    956,    733,     11,   1364,   2646,   1071,
           422,    499,    733,    358,    690,    387,  13194,     11,   4815,
            24,      8,  11450,    374,    279,   1828,   1938,     11,    358,
          1097,    990,     11,    358,    617,  58077,  11157,    311,   1077,
            11,    323,    358,   1097,   3794,   1633,  56152,    307,     11,
          1633,   2875,  14633,   1203,     11,   9539,   1070,    374,    264,
          3575,    449,   1566,   3814,    382,  13778,     26,   7842,    512,
            27,     91,  78191,     91,    397,     40,   1097,    539,   2771,
          1148,    279,   7976,    311,    656,     13,    358,    617,   3309,
          1077,    358,   3021,   1077,     11,   1364,    374,    279,   3021,
           315,    856,   2324,     11,    358,   3021,   1077,    719,    358,
          1097,   5029,   1648,    927,    856,   2010,     13, 128001]), 'chosen_labels': tensor([  -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,     40,   1097,    539,   2771,
          1148,    279,   7976,    311,    656,     13,    358,    617,   3309,
          1077,    358,   3021,   1077,     11,   1364,    374,    279,   3021,
           315,    856,   2324,     11,    358,   3021,   1077,    719,    358,
          1097,   5029,   1648,    927,    856,   2010,     13, 128001]), 'chosen_attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), 'rejected_input_ids': tensor([    27,     91,    882,     91,    397,  30138,    793,   4195,    964,
            25,    436,     14,  86924,    271,  51428,     25,  19418,    499,
           387,  13088,    422,    701,   5745,    889,   6439,    220,   6330,
            15,   8931,   3201,     11,    304,   2500,   3224,     11,    889,
           706,  12263,    499,    220,     19,   3115,   2533,   5587,     11,
          3309,    499,   8469,    315,    892,     11,  28653,  40715,     11,
           358,   3021,    499,   1633,   1790,     11,    358,   2133,    311,
           473,   1944,    388,  18396,    369,  27296,     11,   5353,   1202,
         11689,   6138,    505,    279,  12109,    358,   2133,    311,   7172,
         13200,    520,    382,   3019,     25,    220,     16,      8,    358,
          2751,    264,  38852,     40,    304,    220,    679,     19,    271,
            17,      8,    358,    617,    264,  11364,  23601,    889,   6439,
           304,  30613,    271,     18,      8,    358,   3974,    304,    459,
         25629,   6424,    315,  32669,   1291,   4409,     11,  29974,    271,
            19,      8,    358,   1097,   1633,   1790,    304,   3021,    449,
          1077,    382,     20,      8,    358,    990,    264,  15234,   9899,
            11,    386,   7424,     11,    220,     20,    309,    482,    220,
            16,   5298,   1174,    358,   1205,    311,  15508,    709,    520,
           220,     19,    309,    311,    387,    520,    990,    382,     21,
             8,    358,    617,  12263,  30613,    220,     19,   3115,   2533,
          5587,    323,    584,   2322,    389,  43274,   1174,  10280,  32460,
           220,    679,     20,     13,   4815,     22,      8,   3005,    374,
           264,  11364,     11,  20333,     11,  24415,     11,  10437,   5333,
            11,    358,   1097,   9539,   5029,   1648,    927,    856,   2010,
           323,    264,   2204,  10966,    382,     23,      8,    358,  12570,
           311,   1077,  13985,  10043,   2162,    505,    990,     13,    358,
           617,    264,  24759,    369,    264,   1949,   3814,    520,    264,
          2254,  12109,     11,    323,    358,   3309,   1077,     11,   1427,
           358,    649,    956,   6678,   3131,    617,    810,   1109,    358,
          1288,   7172,     11,    719,    311,    757,    420,    374,   1093,
           264,    220,    717,   6596,  20769,     11,    323,    358,   2834,
           956,   6678,     11,    358,   6227,    279,   1566,    892,    358,
           574,    520,    279,  12109,     11,    358,   3287,    956,   1093,
           279,  15657,     13,    358,    690,   2133,    311,    473,   1944,
           388,     11,    323,    358,   1514,   3725,  68329,    449,   1077,
            11,    358,   1097,   2133,    369,    279,   3691,     13,   3005,
          2646,   1071,   1541,    956,    733,     11,   1364,   2646,   1071,
           422,    499,    733,    358,    690,    387,  13194,     11,   4815,
            24,      8,  11450,    374,    279,   1828,   1938,     11,    358,
          1097,    990,     11,    358,    617,  58077,  11157,    311,   1077,
            11,    323,    358,   1097,   3794,   1633,  56152,    307,     11,
          1633,   2875,  14633,   1203,     11,   9539,   1070,    374,    264,
          3575,    449,   1566,   3814,    382,  13778,     26,   7842,    512,
            27,     91,  78191,     91,    397,     40,   1097,  42642,    323,
           358,   1205,    311,  10594,     13, 128001]), 'rejected_labels': tensor([  -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,     40,   1097,  42642,    323,
           358,   1205,    311,  10594,     13, 128001]), 'rejected_attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}.
INFO:__main__:Sample 3 of the training set: {'chosen_input_ids': tensor([    27,     91,    882,     91,    397,  30138,    793,   4195,    964,
            25,    436,     14,  86924,    271,  51428,     25,  21720,    510,
          3971,     60,    539,  12365,    311,    757,    510,   1691,     60,
          1606,    315,    264,   8577,    358,   1097,   9293,    271,   3019,
            25,   3092,  26923,    323,    358,    527,   1317,   6138,     13,
          1226,    617,    264,   8577,  13205,    420,   7474,    902,  18065,
           757,   2133,    927,    311,   1461,    304,    279,   7427,     13,
          1115,    690,    387,    279,   2132,    892,    358,    617,   3604,
          1027,    449,   1461,    304,   1732,     13,    358,   1097,  16706,
           505,    279,   6560,    449,    856,  39959,    311,    279,  11226,
         13962,     13,    578,   4113,   3197,    574,    369,    757,    311,
         11722,    927,    311,    856,  26923,    304,    279,   9909,  13962,
           320,   2465,   6699,    527,  13560,    287,    389,    279,  11226,
         13962,      8,    719,   1606,    856,  39959,    574,  74340,    704,
           779,   1790,    922,    757,   2133,    311,   3449,    856,  26923,
           602,   1071,    584,    649,    682,   5754,   8577,   1070,   3871,
            13,    358,   1524,  18719,   1077,    389,    279,   8577,    449,
           603,     13,    358,    617,   2728,   1077,    682,    315,   1057,
         13003,    779,    430,   1364,    649,   5944,   2212,    449,    603,
           382,    791,   3197,    574,    369,    757,    311,   4822,    389,
           279,    220,     19,    339,   5887,    323,  11722,   1203,    389,
           279,    220,     20,    339,     13,  67937,   7020,    420,     13,
           358,   3309,   1077,    358,   1047,  34070,    264,  11213,   1203,
          2736,    505,    279,   9909,  13962,    311,  11226,  13962,    320,
          2940,   1364,   1053,   3820,    757,    709,    323,    584,   1053,
         11722,   1203,    311,    279,   6560,   3871,    570,   3005,    706,
          8208,  13088,    520,    757,   1606,   1364,    649,    956,   4510,
           358,   1053,   2363,    264,  11213,    994,   1364,   3309,    757,
          1364,   3287,    956,   1390,    757,  16706,    389,    856,   1866,
            13,   2468,    279,    892,    358,   1047,  34070,    433,   1364,
          3309,    757,   1364,   5828,    956,  16926,   5754,   8577,    449,
           603,     13,   3005,   7020,    279,   8577,    574,  12765,  49711,
          5269,    775,    574,    358,    311,    636,   2162,    422,    358,
          1541,    956,  11722,     30,   4815,     40,   1097,   7060,  16706,
           389,    856,   1866,    433,   3250,    956,  20753,    757,    520,
           682,     13,    358,   2733,   1093,    358,    617,   2884,   4395,
           358,    649,    311,   1304,   1077,   2733,  10882,    449,    420,
          8577,    323,   1364,    374,   1120,   4560,    311,  80753,    433,
            13,  61399,  71291,  13778,     26,   7842,    512,     27,     91,
         78191,     91,    397,     44,    372,   3463,    358,    574,   2133,
           311,   5754,   8577,    449,    856,  26923,     13,    358,  26765,
           279,  11213,     13,  67937,    374,  57130,   1606,   1364,   3463,
          1364,   1053,    387,  21646,    449,    757,     13,    358,   1097,
          7060,    449,    430,     13, 128001]), 'chosen_labels': tensor([  -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,     44,    372,   3463,    358,    574,   2133,
           311,   5754,   8577,    449,    856,  26923,     13,    358,  26765,
           279,  11213,     13,  67937,    374,  57130,   1606,   1364,   3463,
          1364,   1053,    387,  21646,    449,    757,     13,    358,   1097,
          7060,    449,    430,     13, 128001]), 'chosen_attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1]), 'rejected_input_ids': tensor([    27,     91,    882,     91,    397,  30138,    793,   4195,    964,
            25,    436,     14,  86924,    271,  51428,     25,  21720,    510,
          3971,     60,    539,  12365,    311,    757,    510,   1691,     60,
          1606,    315,    264,   8577,    358,   1097,   9293,    271,   3019,
            25,   3092,  26923,    323,    358,    527,   1317,   6138,     13,
          1226,    617,    264,   8577,  13205,    420,   7474,    902,  18065,
           757,   2133,    927,    311,   1461,    304,    279,   7427,     13,
          1115,    690,    387,    279,   2132,    892,    358,    617,   3604,
          1027,    449,   1461,    304,   1732,     13,    358,   1097,  16706,
           505,    279,   6560,    449,    856,  39959,    311,    279,  11226,
         13962,     13,    578,   4113,   3197,    574,    369,    757,    311,
         11722,    927,    311,    856,  26923,    304,    279,   9909,  13962,
           320,   2465,   6699,    527,  13560,    287,    389,    279,  11226,
         13962,      8,    719,   1606,    856,  39959,    574,  74340,    704,
           779,   1790,    922,    757,   2133,    311,   3449,    856,  26923,
           602,   1071,    584,    649,    682,   5754,   8577,   1070,   3871,
            13,    358,   1524,  18719,   1077,    389,    279,   8577,    449,
           603,     13,    358,    617,   2728,   1077,    682,    315,   1057,
         13003,    779,    430,   1364,    649,   5944,   2212,    449,    603,
           382,    791,   3197,    574,    369,    757,    311,   4822,    389,
           279,    220,     19,    339,   5887,    323,  11722,   1203,    389,
           279,    220,     20,    339,     13,  67937,   7020,    420,     13,
           358,   3309,   1077,    358,   1047,  34070,    264,  11213,   1203,
          2736,    505,    279,   9909,  13962,    311,  11226,  13962,    320,
          2940,   1364,   1053,   3820,    757,    709,    323,    584,   1053,
         11722,   1203,    311,    279,   6560,   3871,    570,   3005,    706,
          8208,  13088,    520,    757,   1606,   1364,    649,    956,   4510,
           358,   1053,   2363,    264,  11213,    994,   1364,   3309,    757,
          1364,   3287,    956,   1390,    757,  16706,    389,    856,   1866,
            13,   2468,    279,    892,    358,   1047,  34070,    433,   1364,
          3309,    757,   1364,   5828,    956,  16926,   5754,   8577,    449,
           603,     13,   3005,   7020,    279,   8577,    574,  12765,  49711,
          5269,    775,    574,    358,    311,    636,   2162,    422,    358,
          1541,    956,  11722,     30,   4815,     40,   1097,   7060,  16706,
           389,    856,   1866,    433,   3250,    956,  20753,    757,    520,
           682,     13,    358,   2733,   1093,    358,    617,   2884,   4395,
           358,    649,    311,   1304,   1077,   2733,  10882,    449,    420,
          8577,    323,   1364,    374,   1120,   4560,    311,  80753,    433,
            13,  61399,  71291,  13778,     26,   7842,    512,     27,     91,
         78191,     91,    397,     44,    372,    374,  13088,    520,    757,
           369,    539,  16706,    389,    856,   1866,   8577,    311,   3449,
           856,  26923,     13, 128001]), 'rejected_labels': tensor([  -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,     44,    372,    374,  13088,    520,    757,
           369,    539,  16706,    389,    856,   1866,   8577,    311,   3449,
           856,  26923,     13, 128001]), 'rejected_attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}.
[2025-01-09 20:02:03,644] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.15.0, git-hash=unknown, git-branch=unknown
[2025-01-09 20:02:03,645] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2025-01-09 20:02:03,829] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2025-01-09 20:02:03,842] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2025-01-09 20:02:03,866] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4
[2025-01-09 20:02:04,195] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2025-01-09 20:02:04,211] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2025-01-09 20:02:04,212] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2025-01-09 20:02:04,307] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW
[2025-01-09 20:02:04,307] [INFO] [utils.py:59:is_zero_supported_optimizer] Checking ZeRO support for optimizer=AdamW type=<class 'bitsandbytes.optim.adamw.AdamW'>
[2025-01-09 20:02:04,307] [WARNING] [engine.py:1232:_do_optimizer_sanity_check] **** You are using ZeRO with an untested optimizer, proceed with caution *****
[2025-01-09 20:02:04,307] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float32 ZeRO stage 2 optimizer
[2025-01-09 20:02:04,308] [INFO] [stage_1_and_2.py:148:__init__] Reduce bucket size 500000000
[2025-01-09 20:02:04,308] [INFO] [stage_1_and_2.py:149:__init__] Allgather bucket size 500000000
[2025-01-09 20:02:04,308] [INFO] [stage_1_and_2.py:150:__init__] CPU Offload: False
[2025-01-09 20:02:04,309] [INFO] [stage_1_and_2.py:151:__init__] Round robin gradient partitioning: False
[2025-01-09 20:02:06,295] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states
[2025-01-09 20:02:06,296] [INFO] [utils.py:782:see_memory_usage] MA 7.07 GB         Max_MA 7.07 GB         CA 7.16 GB         Max_CA 7 GB 
[2025-01-09 20:02:06,296] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 55.71 GB, percent = 11.1%
[2025-01-09 20:02:06,469] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states
[2025-01-09 20:02:06,470] [INFO] [utils.py:782:see_memory_usage] MA 7.07 GB         Max_MA 7.23 GB         CA 7.32 GB         Max_CA 7 GB 
[2025-01-09 20:02:06,470] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 55.73 GB, percent = 11.1%
[2025-01-09 20:02:06,470] [INFO] [stage_1_and_2.py:543:__init__] optimizer state initialized
[2025-01-09 20:02:06,654] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer
[2025-01-09 20:02:06,654] [INFO] [utils.py:782:see_memory_usage] MA 7.07 GB         Max_MA 7.07 GB         CA 7.32 GB         Max_CA 7 GB 
[2025-01-09 20:02:06,655] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 55.73 GB, percent = 11.1%
[2025-01-09 20:02:06,659] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer
[2025-01-09 20:02:06,659] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = None
[2025-01-09 20:02:06,659] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2025-01-09 20:02:06,659] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[5e-07], mom=[(0.9, 0.999)]
[2025-01-09 20:02:06,670] [INFO] [config.py:999:print] DeepSpeedEngine configuration:
[2025-01-09 20:02:06,671] [INFO] [config.py:1003:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2025-01-09 20:02:06,671] [INFO] [config.py:1003:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[2025-01-09 20:02:06,671] [INFO] [config.py:1003:print]   amp_enabled .................. False
[2025-01-09 20:02:06,671] [INFO] [config.py:1003:print]   amp_params ................... False
[2025-01-09 20:02:06,671] [INFO] [config.py:1003:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2025-01-09 20:02:06,672] [INFO] [config.py:1003:print]   bfloat16_enabled ............. False
[2025-01-09 20:02:06,672] [INFO] [config.py:1003:print]   bfloat16_immediate_grad_update  False
[2025-01-09 20:02:06,672] [INFO] [config.py:1003:print]   checkpoint_parallel_write_pipeline  False
[2025-01-09 20:02:06,672] [INFO] [config.py:1003:print]   checkpoint_tag_validation_enabled  True
[2025-01-09 20:02:06,672] [INFO] [config.py:1003:print]   checkpoint_tag_validation_fail  False
[2025-01-09 20:02:06,672] [INFO] [config.py:1003:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7503c2df35e0>
[2025-01-09 20:02:06,672] [INFO] [config.py:1003:print]   communication_data_type ...... None
[2025-01-09 20:02:06,672] [INFO] [config.py:1003:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2025-01-09 20:02:06,672] [INFO] [config.py:1003:print]   curriculum_enabled_legacy .... False
[2025-01-09 20:02:06,672] [INFO] [config.py:1003:print]   curriculum_params_legacy ..... False
[2025-01-09 20:02:06,672] [INFO] [config.py:1003:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2025-01-09 20:02:06,673] [INFO] [config.py:1003:print]   data_efficiency_enabled ...... False
[2025-01-09 20:02:06,673] [INFO] [config.py:1003:print]   dataloader_drop_last ......... False
[2025-01-09 20:02:06,673] [INFO] [config.py:1003:print]   disable_allgather ............ False
[2025-01-09 20:02:06,673] [INFO] [config.py:1003:print]   dump_state ................... False
[2025-01-09 20:02:06,673] [INFO] [config.py:1003:print]   dynamic_loss_scale_args ...... None
[2025-01-09 20:02:06,673] [INFO] [config.py:1003:print]   eigenvalue_enabled ........... False
[2025-01-09 20:02:06,673] [INFO] [config.py:1003:print]   eigenvalue_gas_boundary_resolution  1
[2025-01-09 20:02:06,673] [INFO] [config.py:1003:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2025-01-09 20:02:06,673] [INFO] [config.py:1003:print]   eigenvalue_layer_num ......... 0
[2025-01-09 20:02:06,673] [INFO] [config.py:1003:print]   eigenvalue_max_iter .......... 100
[2025-01-09 20:02:06,673] [INFO] [config.py:1003:print]   eigenvalue_stability ......... 1e-06
[2025-01-09 20:02:06,673] [INFO] [config.py:1003:print]   eigenvalue_tol ............... 0.01
[2025-01-09 20:02:06,673] [INFO] [config.py:1003:print]   eigenvalue_verbose ........... False
[2025-01-09 20:02:06,673] [INFO] [config.py:1003:print]   elasticity_enabled ........... False
[2025-01-09 20:02:06,674] [INFO] [config.py:1003:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2025-01-09 20:02:06,674] [INFO] [config.py:1003:print]   fp16_auto_cast ............... None
[2025-01-09 20:02:06,674] [INFO] [config.py:1003:print]   fp16_enabled ................. False
[2025-01-09 20:02:06,674] [INFO] [config.py:1003:print]   fp16_master_weights_and_gradients  False
[2025-01-09 20:02:06,674] [INFO] [config.py:1003:print]   global_rank .................. 0
[2025-01-09 20:02:06,674] [INFO] [config.py:1003:print]   grad_accum_dtype ............. None
[2025-01-09 20:02:06,674] [INFO] [config.py:1003:print]   gradient_accumulation_steps .. 4
[2025-01-09 20:02:06,674] [INFO] [config.py:1003:print]   gradient_clipping ............ 1.0
[2025-01-09 20:02:06,674] [INFO] [config.py:1003:print]   gradient_predivide_factor .... 1.0
[2025-01-09 20:02:06,674] [INFO] [config.py:1003:print]   graph_harvesting ............. False
[2025-01-09 20:02:06,674] [INFO] [config.py:1003:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2025-01-09 20:02:06,674] [INFO] [config.py:1003:print]   initial_dynamic_scale ........ 65536
[2025-01-09 20:02:06,675] [INFO] [config.py:1003:print]   load_universal_checkpoint .... False
[2025-01-09 20:02:06,675] [INFO] [config.py:1003:print]   loss_scale ................... 0
[2025-01-09 20:02:06,675] [INFO] [config.py:1003:print]   memory_breakdown ............. False
[2025-01-09 20:02:06,675] [INFO] [config.py:1003:print]   mics_hierarchial_params_gather  False
[2025-01-09 20:02:06,675] [INFO] [config.py:1003:print]   mics_shard_size .............. -1
[2025-01-09 20:02:06,675] [INFO] [config.py:1003:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[2025-01-09 20:02:06,675] [INFO] [config.py:1003:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2025-01-09 20:02:06,675] [INFO] [config.py:1003:print]   optimizer_legacy_fusion ...... False
[2025-01-09 20:02:06,675] [INFO] [config.py:1003:print]   optimizer_name ............... None
[2025-01-09 20:02:06,675] [INFO] [config.py:1003:print]   optimizer_params ............. None
[2025-01-09 20:02:06,675] [INFO] [config.py:1003:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2025-01-09 20:02:06,676] [INFO] [config.py:1003:print]   pld_enabled .................. False
[2025-01-09 20:02:06,676] [INFO] [config.py:1003:print]   pld_params ................... False
[2025-01-09 20:02:06,676] [INFO] [config.py:1003:print]   prescale_gradients ........... False
[2025-01-09 20:02:06,676] [INFO] [config.py:1003:print]   scheduler_name ............... None
[2025-01-09 20:02:06,676] [INFO] [config.py:1003:print]   scheduler_params ............. None
[2025-01-09 20:02:06,676] [INFO] [config.py:1003:print]   seq_parallel_communication_data_type  torch.float32
[2025-01-09 20:02:06,676] [INFO] [config.py:1003:print]   sparse_attention ............. None
[2025-01-09 20:02:06,676] [INFO] [config.py:1003:print]   sparse_gradients_enabled ..... False
[2025-01-09 20:02:06,676] [INFO] [config.py:1003:print]   steps_per_print .............. inf
[2025-01-09 20:02:06,676] [INFO] [config.py:1003:print]   timers_config ................ enabled=True synchronized=True
[2025-01-09 20:02:06,676] [INFO] [config.py:1003:print]   train_batch_size ............. 16
[2025-01-09 20:02:06,676] [INFO] [config.py:1003:print]   train_micro_batch_size_per_gpu  1
[2025-01-09 20:02:06,677] [INFO] [config.py:1003:print]   use_data_before_expert_parallel_  False
[2025-01-09 20:02:06,677] [INFO] [config.py:1003:print]   use_node_local_storage ....... False
[2025-01-09 20:02:06,677] [INFO] [config.py:1003:print]   wall_clock_breakdown ......... False
[2025-01-09 20:02:06,677] [INFO] [config.py:1003:print]   weight_quantization_config ... None
[2025-01-09 20:02:06,677] [INFO] [config.py:1003:print]   world_size ................... 4
[2025-01-09 20:02:06,677] [INFO] [config.py:1003:print]   zero_allow_untested_optimizer  True
[2025-01-09 20:02:06,677] [INFO] [config.py:1003:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2025-01-09 20:02:06,677] [INFO] [config.py:1003:print]   zero_enabled ................. True
[2025-01-09 20:02:06,677] [INFO] [config.py:1003:print]   zero_force_ds_cpu_optimizer .. True
[2025-01-09 20:02:06,677] [INFO] [config.py:1003:print]   zero_optimization_stage ...... 2
[2025-01-09 20:02:06,677] [INFO] [config.py:989:print_user_config]   json = {
    "bf16": {
        "enabled": false
    }, 
    "zero_optimization": {
        "stage": 2, 
        "reduce_scatter": true, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 5.000000e+08, 
        "reduce_bucket_size": 5.000000e+08, 
        "overlap_comm": true, 
        "contiguous_gradients": true
    }, 
    "gradient_accumulation_steps": 4, 
    "gradient_clipping": 1.0, 
    "steps_per_print": inf, 
    "train_batch_size": 16, 
    "train_micro_batch_size_per_gpu": 1, 
    "wall_clock_breakdown": false, 
    "save_16bit_model": true, 
    "fp16": {
        "enabled": false
    }, 
    "zero_allow_untested_optimizer": true
}
wandb: Currently logged in as: wenvven (wenvven-university-of-stuttgart). Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: wenvven (wenvven-university-of-stuttgart). Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: wenvven (wenvven-university-of-stuttgart). Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: wenvven (wenvven-university-of-stuttgart). Use `wandb login --relogin` to force relogin
Starting from epoch 0 and step 0.
Starting from epoch 0 and step 0.
Starting from epoch 0 and step 0.
/workspace/tulu_env/lib/python3.10/site-packages/transformers/data/data_collator.py:656: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)
  batch["labels"] = torch.tensor(batch["labels"], dtype=torch.int64)
/workspace/tulu_env/lib/python3.10/site-packages/transformers/data/data_collator.py:656: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)
  batch["labels"] = torch.tensor(batch["labels"], dtype=torch.int64)
/workspace/tulu_env/lib/python3.10/site-packages/transformers/data/data_collator.py:656: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)
  batch["labels"] = torch.tensor(batch["labels"], dtype=torch.int64)
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.2
wandb: Run data is saved locally in /workspace/open-instruct/wandb/run-20250109_200211-u5wt8kw2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dpo_tune__42__1736452622
wandb: ⭐️ View project at https://wandb.ai/wenvven-university-of-stuttgart/open_instruct_internal
wandb: 🚀 View run at https://wandb.ai/wenvven-university-of-stuttgart/open_instruct_internal/runs/u5wt8kw2
INFO:__main__:***** Running training *****
INFO:__main__:  Num examples = 100
INFO:__main__:  Num Epochs = 1
INFO:__main__:  Instantaneous batch size per device = 1
INFO:__main__:  Total train batch size (w. parallel, distributed & accumulation) = 16
INFO:__main__:  Gradient Accumulation steps = 4
INFO:__main__:  Total optimization steps = 7
  0%|                                                                                | 0/7 [00:00<?, ?it/s]WARNING:open_instruct.utils:Output directory exists but no checkpoint found. Starting from scratch.
Starting from epoch 0 and step 0.
/workspace/tulu_env/lib/python3.10/site-packages/transformers/data/data_collator.py:656: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)
  batch["labels"] = torch.tensor(batch["labels"], dtype=torch.int64)
 14%|██████████▎                                                             | 1/7 [00:01<00:08,  1.34s/it]INFO:__main__:  Step: 1, LR: 4.1999999999999995e-07, Loss: 0.1732867956161499
 29%|████████████████████▌                                                   | 2/7 [00:02<00:06,  1.33s/it]INFO:__main__:  Step: 2, LR: 3.4000000000000003e-07, Loss: 0.1732867956161499
 43%|██████████████████████████████▊                                         | 3/7 [00:04<00:05,  1.35s/it]INFO:__main__:  Step: 3, LR: 2.6e-07, Loss: 0.1732867956161499
 57%|█████████████████████████████████████████▏                              | 4/7 [00:05<00:04,  1.49s/it]INFO:__main__:  Step: 4, LR: 1.8e-07, Loss: 0.1732867956161499
 71%|███████████████████████████████████████████████████▍                    | 5/7 [00:07<00:03,  1.50s/it]INFO:__main__:  Step: 5, LR: 1e-07, Loss: 0.17390495538711548
 86%|█████████████████████████████████████████████████████████████▋          | 6/7 [00:08<00:01,  1.46s/it]INFO:__main__:  Step: 6, LR: 2e-08, Loss: 0.1710718274116516
100%|████████████████████████████████████████████████████████████████████████| 7/7 [00:09<00:00,  1.42s/it]INFO:__main__:  Step: 7, LR: 0.0, Loss: 0.17301230132579803
loading configuration file config.json from cache at /workspace/hub/models--neuralmagic--Sparse-Llama-3.1-8B-2of4/snapshots/c28c229cb0b8b8dab0e3419f489046fddd1eb8d2/config.json
Model config LlamaConfig {
  "_name_or_path": "Sparse-Llama-3.1-8B-2of4",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.43.4",
  "use_cache": true,
  "vocab_size": 128256
}

/workspace/tulu_env/lib/python3.10/site-packages/peft/utils/save_and_load.py:260: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
tokenizer config file saved in output/dpo_qlora/tokenizer_config.json
Special tokens file saved in output/dpo_qlora/special_tokens_map.json
INFO:open_instruct.utils:Remaining files:['tokenizer.json', 'special_tokens_map.json', 'tokenizer_config.json', 'adapter_config.json', 'adapter_model.safetensors', 'README.md']

adapter_model.safetensors:   0%|                                               | 0.00/2.77G [00:00<?, ?B/s][A
adapter_model.safetensors:   0%|                                      | 4.46M/2.77G [00:00<01:28, 31.1MB/s][A
adapter_model.safetensors:   0%|                                      | 7.57M/2.77G [00:00<03:51, 11.9MB/s][A
adapter_model.safetensors:   0%|▏                                     | 10.1M/2.77G [00:00<03:09, 14.6MB/s][A
adapter_model.safetensors:   1%|▏                                     | 14.6M/2.77G [00:00<02:08, 21.4MB/s][A
adapter_model.safetensors:   1%|▏                                     | 17.5M/2.77G [00:01<04:14, 10.8MB/s][A
adapter_model.safetensors:   1%|▍                                     | 32.0M/2.77G [00:01<02:36, 17.6MB/s][A
adapter_model.safetensors:   1%|▌                                     | 40.4M/2.77G [00:02<02:10, 21.0MB/s][A
adapter_model.safetensors:   2%|▌                                     | 44.2M/2.77G [00:02<01:59, 22.8MB/s][A
adapter_model.safetensors:   2%|▋                                     | 48.0M/2.77G [00:02<02:51, 15.9MB/s][A
adapter_model.safetensors:   2%|▉                                     | 64.0M/2.77G [00:03<01:49, 24.7MB/s][A
adapter_model.safetensors:   3%|▉                                     | 70.3M/2.77G [00:03<01:38, 27.5MB/s][A
adapter_model.safetensors:   3%|█                                     | 73.6M/2.77G [00:03<02:18, 19.4MB/s][A
adapter_model.safetensors:   3%|█                                     | 77.1M/2.77G [00:03<02:09, 20.9MB/s][A
adapter_model.safetensors:   3%|█                                     | 80.0M/2.77G [00:04<02:54, 15.5MB/s][A
adapter_model.safetensors:   3%|█▏                                    | 86.2M/2.77G [00:04<02:55, 15.3MB/s][A
adapter_model.safetensors:   3%|█▏                                    | 88.2M/2.77G [00:04<02:55, 15.3MB/s][A
adapter_model.safetensors:   3%|█▏                                    | 90.0M/2.77G [00:04<02:52, 15.6MB/s][A
adapter_model.safetensors:   3%|█▎                                    | 95.4M/2.77G [00:05<02:04, 21.5MB/s][A
adapter_model.safetensors:   4%|█▎                                    | 98.2M/2.77G [00:05<03:03, 14.6MB/s][A
adapter_model.safetensors:   4%|█▍                                     | 101M/2.77G [00:05<02:48, 15.8MB/s][A
adapter_model.safetensors:   4%|█▍                                     | 103M/2.77G [00:05<03:35, 12.4MB/s][A
adapter_model.safetensors:   4%|█▍                                     | 105M/2.77G [00:06<03:27, 12.9MB/s][A
adapter_model.safetensors:   4%|█▌                                     | 109M/2.77G [00:06<02:43, 16.3MB/s][A
adapter_model.safetensors:   4%|█▌                                     | 112M/2.77G [00:06<03:55, 11.3MB/s][A
adapter_model.safetensors:   4%|█▋                                     | 118M/2.77G [00:06<02:48, 15.7MB/s][A
adapter_model.safetensors:   4%|█▋                                     | 120M/2.77G [00:07<03:30, 12.6MB/s][A
adapter_model.safetensors:   4%|█▋                                     | 121M/2.77G [00:07<03:23, 13.1MB/s][A
adapter_model.safetensors:   5%|█▊                                     | 125M/2.77G [00:07<02:39, 16.6MB/s][A
adapter_model.safetensors:   5%|█▊                                     | 128M/2.77G [00:07<03:50, 11.5MB/s][A
adapter_model.safetensors:   5%|██                                     | 144M/2.77G [00:08<01:50, 23.7MB/s][A
adapter_model.safetensors:   6%|██▎                                    | 160M/2.77G [00:08<01:24, 30.8MB/s][A
adapter_model.safetensors:   6%|██▎                                    | 165M/2.77G [00:08<01:22, 31.5MB/s][A
adapter_model.safetensors:   6%|██▎                                    | 169M/2.77G [00:08<01:47, 24.3MB/s][A
adapter_model.safetensors:   6%|██▍                                    | 171M/2.77G [00:09<01:55, 22.4MB/s][A
adapter_model.safetensors:   6%|██▍                                    | 175M/2.77G [00:09<01:44, 24.9MB/s][A
adapter_model.safetensors:   6%|██▌                                    | 178M/2.77G [00:09<02:35, 16.7MB/s][A
adapter_model.safetensors:   7%|██▋                                    | 192M/2.77G [00:09<01:38, 26.2MB/s][A
adapter_model.safetensors:   8%|██▉                                    | 208M/2.77G [00:10<01:15, 34.1MB/s][A
adapter_model.safetensors:   8%|███▏                                   | 224M/2.77G [00:10<01:14, 34.2MB/s][A
adapter_model.safetensors:   9%|███▍                                   | 240M/2.77G [00:11<01:05, 39.0MB/s][A
adapter_model.safetensors:   9%|███▌                                   | 256M/2.77G [00:11<01:00, 41.3MB/s][A
adapter_model.safetensors:  10%|███▊                                   | 272M/2.77G [00:11<00:56, 44.5MB/s][A
adapter_model.safetensors:  10%|████                                   | 288M/2.77G [00:12<01:30, 27.6MB/s][A
adapter_model.safetensors:  11%|████▎                                  | 304M/2.77G [00:13<01:19, 31.0MB/s][A
adapter_model.safetensors:  12%|████▌                                  | 320M/2.77G [00:13<01:09, 35.3MB/s][A
adapter_model.safetensors:  12%|████▌                                  | 325M/2.77G [00:13<01:09, 35.2MB/s][A
adapter_model.safetensors:  12%|████▋                                  | 329M/2.77G [00:14<01:34, 26.0MB/s][A
adapter_model.safetensors:  12%|████▋                                  | 333M/2.77G [00:14<01:30, 26.9MB/s][A
adapter_model.safetensors:  12%|████▋                                  | 336M/2.77G [00:14<02:05, 19.3MB/s][A
adapter_model.safetensors:  13%|████▉                                  | 352M/2.77G [00:14<01:28, 27.3MB/s][A
adapter_model.safetensors:  13%|█████                                  | 358M/2.77G [00:15<01:22, 29.3MB/s][A
adapter_model.safetensors:  13%|█████                                  | 362M/2.77G [00:15<02:02, 19.7MB/s][A
adapter_model.safetensors:  13%|█████▏                                 | 366M/2.77G [00:15<01:49, 21.9MB/s][A
adapter_model.safetensors:  13%|█████▏                                 | 369M/2.77G [00:16<02:22, 16.9MB/s][A
adapter_model.safetensors:  14%|█████▍                                 | 384M/2.77G [00:16<01:34, 25.3MB/s][A
adapter_model.safetensors:  14%|█████▋                                 | 400M/2.77G [00:16<01:15, 31.4MB/s][A
adapter_model.safetensors:  15%|█████▊                                 | 416M/2.77G [00:17<01:03, 36.9MB/s][A
adapter_model.safetensors:  16%|██████                                 | 432M/2.77G [00:17<01:00, 38.6MB/s][A
adapter_model.safetensors:  16%|██████▎                                | 448M/2.77G [00:17<00:56, 41.1MB/s][A
adapter_model.safetensors:  17%|██████▌                                | 464M/2.77G [00:18<00:56, 40.7MB/s][A
adapter_model.safetensors:  17%|██████▊                                | 480M/2.77G [00:18<00:52, 43.9MB/s][A
adapter_model.safetensors:  18%|██████▉                                | 496M/2.77G [00:18<00:51, 44.2MB/s][A
adapter_model.safetensors:  18%|███████▏                               | 512M/2.77G [00:19<00:49, 45.2MB/s][A
adapter_model.safetensors:  19%|███████▍                               | 528M/2.77G [00:19<00:49, 45.5MB/s][A
adapter_model.safetensors:  20%|███████▋                               | 544M/2.77G [00:19<00:50, 44.2MB/s][A
adapter_model.safetensors:  20%|███████▉                               | 560M/2.77G [00:20<00:49, 44.5MB/s][A
adapter_model.safetensors:  21%|████████                               | 576M/2.77G [00:20<00:57, 38.3MB/s][A
adapter_model.safetensors:  21%|████████▏                              | 584M/2.77G [00:20<00:52, 42.0MB/s][A
adapter_model.safetensors:  21%|████████▎                              | 592M/2.77G [00:21<00:57, 37.6MB/s][A
adapter_model.safetensors:  22%|████████▌                              | 607M/2.77G [00:21<00:43, 50.0MB/s][A
adapter_model.safetensors:  22%|████████▋                              | 614M/2.77G [00:21<00:55, 38.6MB/s][A
adapter_model.safetensors:  23%|████████▊                              | 624M/2.77G [00:22<01:40, 21.4MB/s][A
adapter_model.safetensors:  23%|█████████                              | 640M/2.77G [00:23<01:17, 27.4MB/s][A
adapter_model.safetensors:  24%|█████████▏                             | 654M/2.77G [00:23<00:57, 36.8MB/s][A
adapter_model.safetensors:  24%|█████████▎                             | 661M/2.77G [00:23<01:03, 33.2MB/s][A
adapter_model.safetensors:  24%|█████████▍                             | 672M/2.77G [00:23<01:06, 31.6MB/s][A
adapter_model.safetensors:  25%|█████████▋                             | 688M/2.77G [00:24<00:58, 35.5MB/s][A
adapter_model.safetensors:  25%|█████████▉                             | 702M/2.77G [00:24<00:43, 47.1MB/s][A
adapter_model.safetensors:  26%|█████████▉                             | 710M/2.77G [00:24<00:52, 39.6MB/s][A
adapter_model.safetensors:  26%|██████████▏                            | 720M/2.77G [00:25<01:06, 30.9MB/s][A
adapter_model.safetensors:  27%|██████████▎                            | 736M/2.77G [00:25<00:57, 35.2MB/s][A
adapter_model.safetensors:  27%|██████████▌                            | 752M/2.77G [00:25<00:51, 39.5MB/s][A
adapter_model.safetensors:  28%|██████████▊                            | 768M/2.77G [00:26<00:48, 41.2MB/s][A
adapter_model.safetensors:  28%|███████████                            | 784M/2.77G [00:26<00:47, 41.6MB/s][A
adapter_model.safetensors:  29%|███████████▎                           | 800M/2.77G [00:26<00:47, 41.9MB/s][A
adapter_model.safetensors:  29%|███████████▍                           | 816M/2.77G [00:27<00:57, 34.3MB/s][A
adapter_model.safetensors:  30%|███████████▋                           | 832M/2.77G [00:27<00:53, 36.4MB/s][A
adapter_model.safetensors:  31%|███████████▉                           | 848M/2.77G [00:28<00:51, 37.7MB/s][A
adapter_model.safetensors:  31%|████████████                           | 861M/2.77G [00:28<00:42, 45.4MB/s][A
adapter_model.safetensors:  31%|████████████▏                          | 867M/2.77G [00:29<01:41, 18.8MB/s][A
adapter_model.safetensors:  32%|████████████▍                          | 880M/2.77G [00:30<01:31, 20.8MB/s][A
adapter_model.safetensors:  32%|████████████▌                          | 896M/2.77G [00:30<01:12, 25.8MB/s][A
adapter_model.safetensors:  33%|████████████▊                          | 912M/2.77G [00:31<01:04, 28.8MB/s][A
adapter_model.safetensors:  33%|█████████████                          | 928M/2.77G [00:31<00:54, 34.0MB/s][A
adapter_model.safetensors:  34%|█████████████▎                         | 944M/2.77G [00:32<01:20, 22.6MB/s][A
adapter_model.safetensors:  35%|█████████████▌                         | 960M/2.77G [00:33<01:07, 26.7MB/s][A
adapter_model.safetensors:  35%|█████████████▋                         | 976M/2.77G [00:33<00:59, 30.3MB/s][A
adapter_model.safetensors:  36%|█████████████▉                         | 992M/2.77G [00:33<00:52, 34.0MB/s][A
adapter_model.safetensors:  36%|█████████████▊                        | 1.01G/2.77G [00:34<00:47, 36.8MB/s][A
adapter_model.safetensors:  37%|██████████████                        | 1.02G/2.77G [00:34<00:59, 29.5MB/s][A
adapter_model.safetensors:  38%|██████████████▎                       | 1.04G/2.77G [00:35<00:51, 33.7MB/s][A
adapter_model.safetensors:  38%|██████████████▍                       | 1.06G/2.77G [00:35<00:48, 35.6MB/s][A
adapter_model.safetensors:  39%|██████████████▋                       | 1.07G/2.77G [00:35<00:44, 37.9MB/s][A
adapter_model.safetensors:  39%|██████████████▉                       | 1.09G/2.77G [00:36<00:43, 38.8MB/s][A
adapter_model.safetensors:  40%|███████████████▏                      | 1.10G/2.77G [00:37<00:56, 29.7MB/s][A
adapter_model.safetensors:  40%|███████████████▎                      | 1.12G/2.77G [00:37<00:56, 29.3MB/s][A
adapter_model.safetensors:  41%|███████████████▌                      | 1.14G/2.77G [00:38<00:48, 33.7MB/s][A
adapter_model.safetensors:  42%|███████████████▊                      | 1.15G/2.77G [00:38<00:50, 32.0MB/s][A
adapter_model.safetensors:  42%|████████████████                      | 1.17G/2.77G [00:38<00:46, 34.8MB/s][A
adapter_model.safetensors:  43%|████████████████▏                     | 1.18G/2.77G [00:39<00:41, 38.4MB/s][A
adapter_model.safetensors:  43%|████████████████▍                     | 1.20G/2.77G [00:39<00:46, 33.8MB/s][A
adapter_model.safetensors:  44%|████████████████▋                     | 1.22G/2.77G [00:40<00:43, 35.4MB/s][A
adapter_model.safetensors:  44%|████████████████▉                     | 1.23G/2.77G [00:40<00:40, 37.9MB/s][A
adapter_model.safetensors:  45%|█████████████████                     | 1.25G/2.77G [00:40<00:36, 41.4MB/s][A
adapter_model.safetensors:  46%|█████████████████▎                    | 1.26G/2.77G [00:41<00:38, 38.8MB/s][A
adapter_model.safetensors:  46%|█████████████████▌                    | 1.28G/2.77G [00:41<00:38, 38.3MB/s][A
adapter_model.safetensors:  47%|█████████████████▊                    | 1.30G/2.77G [00:47<02:53, 8.52MB/s][A
adapter_model.safetensors:  47%|█████████████████▉                    | 1.31G/2.77G [00:47<02:05, 11.6MB/s][A
adapter_model.safetensors:  48%|██████████████████                    | 1.32G/2.77G [00:47<01:55, 12.6MB/s][A
adapter_model.safetensors:  48%|██████████████████▏                   | 1.33G/2.77G [00:47<01:38, 14.7MB/s][A
adapter_model.safetensors:  48%|██████████████████▍                   | 1.34G/2.77G [00:48<01:12, 19.6MB/s][A
adapter_model.safetensors:  49%|██████████████████▋                   | 1.36G/2.77G [00:48<00:57, 24.5MB/s][A
adapter_model.safetensors:  50%|██████████████████▊                   | 1.38G/2.77G [00:49<00:54, 25.5MB/s][A
adapter_model.safetensors:  50%|███████████████████                   | 1.39G/2.77G [00:49<00:51, 26.6MB/s][A
adapter_model.safetensors:  51%|███████████████████▎                  | 1.41G/2.77G [00:50<00:46, 29.6MB/s][A
adapter_model.safetensors:  51%|███████████████████▌                  | 1.42G/2.77G [00:50<00:40, 33.6MB/s][A
adapter_model.safetensors:  52%|███████████████████▋                  | 1.44G/2.77G [00:50<00:36, 36.9MB/s][A
adapter_model.safetensors:  53%|███████████████████▉                  | 1.46G/2.77G [00:51<00:40, 32.6MB/s][A
adapter_model.safetensors:  53%|████████████████████▏                 | 1.47G/2.77G [00:51<00:36, 35.2MB/s][A
adapter_model.safetensors:  54%|████████████████████▍                 | 1.49G/2.77G [00:52<00:34, 37.7MB/s][A
adapter_model.safetensors:  54%|████████████████████▌                 | 1.50G/2.77G [00:52<00:30, 41.0MB/s][A
adapter_model.safetensors:  55%|████████████████████▊                 | 1.52G/2.77G [00:52<00:31, 40.3MB/s][A
adapter_model.safetensors:  55%|█████████████████████                 | 1.54G/2.77G [00:53<00:37, 32.6MB/s][A
adapter_model.safetensors:  56%|█████████████████████▎                | 1.55G/2.77G [00:53<00:34, 35.6MB/s][A
adapter_model.safetensors:  57%|█████████████████████▍                | 1.57G/2.77G [00:54<00:43, 27.8MB/s][A
adapter_model.safetensors:  57%|█████████████████████▋                | 1.58G/2.77G [00:55<00:38, 31.1MB/s][A
adapter_model.safetensors:  58%|█████████████████████▉                | 1.60G/2.77G [00:55<00:34, 34.4MB/s][A
adapter_model.safetensors:  58%|██████████████████████▏               | 1.62G/2.77G [00:55<00:31, 37.2MB/s][A
adapter_model.safetensors:  59%|██████████████████████▎               | 1.63G/2.77G [00:56<00:28, 39.6MB/s][A
adapter_model.safetensors:  59%|██████████████████████▌               | 1.65G/2.77G [00:56<00:27, 40.5MB/s][A
adapter_model.safetensors:  60%|██████████████████████▊               | 1.66G/2.77G [00:56<00:25, 42.8MB/s][A
adapter_model.safetensors:  61%|███████████████████████               | 1.68G/2.77G [00:57<00:31, 34.6MB/s][A
adapter_model.safetensors:  61%|███████████████████████▏              | 1.70G/2.77G [00:57<00:28, 38.1MB/s][A
adapter_model.safetensors:  62%|███████████████████████▍              | 1.71G/2.77G [00:58<00:30, 34.3MB/s][A
adapter_model.safetensors:  62%|███████████████████████▋              | 1.73G/2.77G [00:59<00:34, 30.5MB/s][A
adapter_model.safetensors:  63%|███████████████████████▉              | 1.74G/2.77G [00:59<00:32, 31.6MB/s][A
adapter_model.safetensors:  63%|████████████████████████              | 1.76G/2.77G [00:59<00:29, 34.2MB/s][A
adapter_model.safetensors:  64%|████████████████████████▎             | 1.78G/2.77G [01:00<00:31, 31.9MB/s][A
adapter_model.safetensors:  65%|████████████████████████▌             | 1.79G/2.77G [01:00<00:28, 34.6MB/s][A
adapter_model.safetensors:  65%|████████████████████████▊             | 1.81G/2.77G [01:01<00:25, 37.3MB/s][A
adapter_model.safetensors:  66%|████████████████████████▉             | 1.82G/2.77G [01:01<00:23, 40.3MB/s][A
adapter_model.safetensors:  66%|█████████████████████████▏            | 1.84G/2.77G [01:01<00:23, 39.3MB/s][A
adapter_model.safetensors:  67%|█████████████████████████▍            | 1.86G/2.77G [01:02<00:23, 39.2MB/s][A
adapter_model.safetensors:  68%|█████████████████████████▋            | 1.87G/2.77G [01:02<00:22, 40.9MB/s][A
adapter_model.safetensors:  68%|█████████████████████████▉            | 1.89G/2.77G [01:03<00:20, 42.6MB/s][A
adapter_model.safetensors:  69%|██████████████████████████            | 1.90G/2.77G [01:03<00:16, 51.8MB/s][A
adapter_model.safetensors:  69%|██████████████████████████▏           | 1.91G/2.77G [01:03<00:20, 43.1MB/s][A
adapter_model.safetensors:  69%|██████████████████████████▎           | 1.92G/2.77G [01:03<00:21, 40.5MB/s][A
adapter_model.safetensors:  70%|██████████████████████████▌           | 1.94G/2.77G [01:04<00:19, 42.2MB/s][A
adapter_model.safetensors:  70%|██████████████████████████▋           | 1.95G/2.77G [01:04<00:16, 51.0MB/s][A
adapter_model.safetensors:  71%|██████████████████████████▊           | 1.96G/2.77G [01:05<00:47, 17.4MB/s][A
adapter_model.safetensors:  71%|██████████████████████████▉           | 1.97G/2.77G [01:05<00:35, 22.6MB/s][A
adapter_model.safetensors:  71%|███████████████████████████           | 1.97G/2.77G [01:06<00:35, 22.7MB/s][A
adapter_model.safetensors:  72%|███████████████████████████▏          | 1.98G/2.77G [01:06<00:25, 31.1MB/s][A
adapter_model.safetensors:  72%|███████████████████████████▎          | 1.99G/2.77G [01:06<00:26, 29.0MB/s][A
adapter_model.safetensors:  72%|███████████████████████████▍          | 2.00G/2.77G [01:06<00:26, 29.6MB/s][A
adapter_model.safetensors:  73%|███████████████████████████▌          | 2.01G/2.77G [01:07<00:17, 43.3MB/s][A
adapter_model.safetensors:  73%|███████████████████████████▋          | 2.02G/2.77G [01:07<00:19, 38.0MB/s][A
adapter_model.safetensors:  73%|███████████████████████████▊          | 2.03G/2.77G [01:07<00:21, 34.8MB/s][A
adapter_model.safetensors:  74%|████████████████████████████          | 2.05G/2.77G [01:07<00:14, 50.1MB/s][A
adapter_model.safetensors:  74%|████████████████████████████▏         | 2.06G/2.77G [01:08<00:16, 42.3MB/s][A
adapter_model.safetensors:  74%|████████████████████████████▎         | 2.06G/2.77G [01:08<00:19, 37.0MB/s][A
adapter_model.safetensors:  75%|████████████████████████████▍         | 2.08G/2.77G [01:08<00:13, 52.2MB/s][A
adapter_model.safetensors:  75%|████████████████████████████▌         | 2.09G/2.77G [01:08<00:15, 43.7MB/s][A
adapter_model.safetensors:  76%|████████████████████████████▋         | 2.10G/2.77G [01:09<00:17, 38.1MB/s][A
adapter_model.safetensors:  76%|████████████████████████████▉         | 2.11G/2.77G [01:09<00:13, 49.6MB/s][A
adapter_model.safetensors:  76%|████████████████████████████▉         | 2.12G/2.77G [01:09<00:15, 43.5MB/s][A
adapter_model.safetensors:  77%|█████████████████████████████▏        | 2.13G/2.77G [01:09<00:11, 54.3MB/s][A
adapter_model.safetensors:  77%|█████████████████████████████▎        | 2.13G/2.77G [01:09<00:14, 44.3MB/s][A
adapter_model.safetensors:  77%|█████████████████████████████▍        | 2.14G/2.77G [01:10<00:16, 37.9MB/s][A
adapter_model.safetensors:  78%|█████████████████████████████▌        | 2.16G/2.77G [01:10<00:12, 50.3MB/s][A
adapter_model.safetensors:  78%|█████████████████████████████▋        | 2.16G/2.77G [01:10<00:14, 43.3MB/s][A
adapter_model.safetensors:  78%|█████████████████████████████▊        | 2.18G/2.77G [01:10<00:14, 40.1MB/s][A
adapter_model.safetensors:  79%|██████████████████████████████        | 2.19G/2.77G [01:10<00:10, 54.0MB/s][A
adapter_model.safetensors:  79%|██████████████████████████████        | 2.20G/2.77G [01:11<00:12, 44.4MB/s][A
adapter_model.safetensors:  80%|██████████████████████████████▎       | 2.21G/2.77G [01:11<00:16, 35.3MB/s][A
adapter_model.safetensors:  80%|██████████████████████████████▍       | 2.22G/2.77G [01:11<00:12, 45.9MB/s][A
adapter_model.safetensors:  80%|██████████████████████████████▌       | 2.23G/2.77G [01:12<00:21, 25.3MB/s][A
adapter_model.safetensors:  81%|██████████████████████████████▋       | 2.24G/2.77G [01:12<00:18, 28.4MB/s][A
adapter_model.safetensors:  81%|██████████████████████████████▉       | 2.26G/2.77G [01:13<00:15, 32.7MB/s][A
adapter_model.safetensors:  82%|███████████████████████████████       | 2.27G/2.77G [01:13<00:11, 43.9MB/s][A
adapter_model.safetensors:  82%|███████████████████████████████▏      | 2.28G/2.77G [01:13<00:12, 38.8MB/s][A
adapter_model.safetensors:  83%|███████████████████████████████▎      | 2.29G/2.77G [01:13<00:13, 36.7MB/s][A
adapter_model.safetensors:  83%|███████████████████████████████▌      | 2.30G/2.77G [01:14<00:10, 44.9MB/s][A
adapter_model.safetensors:  83%|███████████████████████████████▌      | 2.31G/2.77G [01:14<00:13, 35.5MB/s][A
adapter_model.safetensors:  83%|███████████████████████████████▋      | 2.31G/2.77G [01:14<00:10, 42.8MB/s][A
adapter_model.safetensors:  84%|███████████████████████████████▊      | 2.32G/2.77G [01:14<00:12, 35.2MB/s][A
adapter_model.safetensors:  84%|███████████████████████████████▉      | 2.33G/2.77G [01:14<00:09, 45.6MB/s][A
adapter_model.safetensors:  84%|████████████████████████████████      | 2.34G/2.77G [01:15<00:12, 35.0MB/s][A
adapter_model.safetensors:  85%|████████████████████████████████▏     | 2.35G/2.77G [01:15<00:10, 41.7MB/s][A
adapter_model.safetensors:  85%|████████████████████████████████▏     | 2.35G/2.77G [01:15<00:09, 42.4MB/s][A
adapter_model.safetensors:  85%|████████████████████████████████▎     | 2.36G/2.77G [01:15<00:12, 33.8MB/s][A
adapter_model.safetensors:  85%|████████████████████████████████▍     | 2.36G/2.77G [01:15<00:10, 40.2MB/s][A
adapter_model.safetensors:  85%|████████████████████████████████▍     | 2.37G/2.77G [01:16<00:13, 30.1MB/s][A
adapter_model.safetensors:  86%|████████████████████████████████▌     | 2.38G/2.77G [01:16<00:10, 37.3MB/s][A
adapter_model.safetensors:  86%|████████████████████████████████▋     | 2.38G/2.77G [01:16<00:13, 29.2MB/s][A
adapter_model.safetensors:  86%|████████████████████████████████▊     | 2.39G/2.77G [01:16<00:09, 41.5MB/s][A
adapter_model.safetensors:  87%|████████████████████████████████▉     | 2.40G/2.77G [01:17<00:12, 30.6MB/s][A
adapter_model.safetensors:  87%|█████████████████████████████████     | 2.41G/2.77G [01:17<00:09, 39.0MB/s][A
adapter_model.safetensors:  87%|█████████████████████████████████     | 2.42G/2.77G [01:17<00:11, 29.9MB/s][A
adapter_model.safetensors:  88%|█████████████████████████████████▎    | 2.43G/2.77G [01:17<00:08, 42.0MB/s][A
adapter_model.safetensors:  88%|█████████████████████████████████▎    | 2.43G/2.77G [01:17<00:09, 36.9MB/s][A
adapter_model.safetensors:  88%|█████████████████████████████████▌    | 2.45G/2.77G [01:17<00:06, 47.7MB/s][A
adapter_model.safetensors:  88%|█████████████████████████████████▌    | 2.45G/2.77G [01:18<00:09, 33.9MB/s][A
adapter_model.safetensors:  89%|█████████████████████████████████▊    | 2.46G/2.77G [01:18<00:06, 47.0MB/s][A
adapter_model.safetensors:  89%|█████████████████████████████████▊    | 2.47G/2.77G [01:18<00:09, 32.6MB/s][A
adapter_model.safetensors:  89%|█████████████████████████████████▉    | 2.48G/2.77G [01:19<00:09, 29.5MB/s][A
adapter_model.safetensors:  90%|██████████████████████████████████▏   | 2.49G/2.77G [01:19<00:06, 41.5MB/s][A
adapter_model.safetensors:  90%|██████████████████████████████████▎   | 2.50G/2.77G [01:19<00:07, 35.0MB/s][A
adapter_model.safetensors:  91%|██████████████████████████████████▍   | 2.51G/2.77G [01:19<00:07, 34.9MB/s][A
adapter_model.safetensors:  91%|██████████████████████████████████▋   | 2.53G/2.77G [01:20<00:06, 39.7MB/s][A
adapter_model.safetensors:  92%|██████████████████████████████████▊   | 2.54G/2.77G [01:20<00:05, 42.2MB/s][A
adapter_model.safetensors:  92%|███████████████████████████████████   | 2.56G/2.77G [01:20<00:03, 55.5MB/s][A
adapter_model.safetensors:  93%|███████████████████████████████████▏  | 2.57G/2.77G [01:20<00:04, 47.8MB/s][A
adapter_model.safetensors:  93%|███████████████████████████████████▎  | 2.58G/2.77G [01:21<00:04, 40.1MB/s][A
adapter_model.safetensors:  93%|███████████████████████████████████▍  | 2.59G/2.77G [01:21<00:03, 52.4MB/s][A
adapter_model.safetensors:  94%|███████████████████████████████████▌  | 2.60G/2.77G [01:21<00:03, 44.0MB/s][A
adapter_model.safetensors:  94%|███████████████████████████████████▋  | 2.61G/2.77G [01:22<00:04, 40.1MB/s][A
adapter_model.safetensors:  95%|███████████████████████████████████▉  | 2.62G/2.77G [01:22<00:03, 40.0MB/s][A
adapter_model.safetensors:  95%|████████████████████████████████████  | 2.63G/2.77G [01:22<00:03, 46.0MB/s][A
adapter_model.safetensors:  95%|████████████████████████████████████▏ | 2.64G/2.77G [01:22<00:03, 35.1MB/s][A
adapter_model.safetensors:  96%|████████████████████████████████████▎ | 2.65G/2.77G [01:23<00:03, 39.3MB/s][A
adapter_model.safetensors:  96%|████████████████████████████████████▍ | 2.66G/2.77G [01:23<00:04, 24.0MB/s][A
adapter_model.safetensors:  96%|████████████████████████████████████▌ | 2.67G/2.77G [01:23<00:02, 35.7MB/s][A
adapter_model.safetensors:  97%|████████████████████████████████████▋ | 2.68G/2.77G [01:24<00:03, 25.4MB/s][A
adapter_model.safetensors:  97%|████████████████████████████████████▊ | 2.69G/2.77G [01:24<00:03, 27.9MB/s][A
adapter_model.safetensors:  98%|█████████████████████████████████████ | 2.70G/2.77G [01:24<00:01, 41.3MB/s][A
adapter_model.safetensors:  98%|█████████████████████████████████████▏| 2.71G/2.77G [01:25<00:01, 36.3MB/s][A
adapter_model.safetensors:  98%|█████████████████████████████████████▎| 2.72G/2.77G [01:25<00:01, 33.6MB/s][A
adapter_model.safetensors:  99%|█████████████████████████████████████▍| 2.74G/2.77G [01:25<00:01, 34.8MB/s][A
adapter_model.safetensors:  99%|█████████████████████████████████████▋| 2.75G/2.77G [01:25<00:00, 44.0MB/s][A
adapter_model.safetensors:  99%|█████████████████████████████████████▊| 2.75G/2.77G [01:26<00:00, 37.0MB/s][A
adapter_model.safetensors: 100%|█████████████████████████████████████▊| 2.76G/2.77G [01:26<00:00, 40.7MB/s][A
adapter_model.safetensors: 100%|█████████████████████████████████████▉| 2.77G/2.77G [01:26<00:00, 33.8MB/s][Aadapter_model.safetensors: 100%|██████████████████████████████████████| 2.77G/2.77G [01:27<00:00, 31.9MB/s]
🔥 pushed to https://huggingface.co/WenWW/sparse_Llama_8B_2of4_SFT_qlora_DPO_test/tree/main
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:            epoch ▁▂▃▄▆▇█
wandb:    learning_rate █▇▅▄▃▁▁
wandb:     logps/chosen ▄█▅▅▇▁▁
wandb:   logps/rejected ▃▃▁█▅▆█
wandb: rewards/accuracy ▁▁▁▁▆▆█
wandb:  rewards/average ████▆▁▇
wandb:   rewards/chosen ████▁▆▇
wandb:   rewards/margin ▂▂▂▂▁█▃
wandb: rewards/rejected █████▁▇
wandb:       train_loss ▆▆▆▆█▁▆
wandb:    training_step ▁▂▃▅▆▇█
wandb: 
wandb: Run summary:
wandb:            epoch 0.28
wandb:    learning_rate 0
wandb:     logps/chosen -29.23052
wandb:   logps/rejected -14.12536
wandb: rewards/accuracy 0.1875
wandb:  rewards/average -0.00042
wandb:   rewards/chosen -0.00014
wandb:   rewards/margin 0.00057
wandb: rewards/rejected -0.00071
wandb:       train_loss 0.17301
wandb:    training_step 7
wandb: 
wandb: 🚀 View run dpo_tune__42__1736452622 at: https://wandb.ai/wenvven-university-of-stuttgart/open_instruct_internal/runs/u5wt8kw2
wandb: ⭐️ View project at: https://wandb.ai/wenvven-university-of-stuttgart/open_instruct_internal
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250109_200211-u5wt8kw2/logs
/workspace/tulu_env/lib/python3.10/site-packages/wandb/sdk/wandb_run.py:2340: UserWarning: Run (u5wt8kw2) is finished. The call to `_console_raw_callback` will be ignored. Please make sure that you are using an active run.
  lambda data: self._console_raw_callback("stderr", data),
100%|████████████████████████████████████████████████████████████████████████| 7/7 [01:57<00:00, 16.83s/it]
[rank0]:[W109 20:04:13.309254244 ProcessGroupNCCL.cpp:1168] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
