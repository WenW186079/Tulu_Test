Number of GPUs: 2
Using config file: configs/train_configs/dpo/mini.yaml
CUDA_VISIBLE_DEVICES: 0,1
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
[2025-01-07 20:17:15,002] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
df: /root/.triton/autotune: No such file or directory
df: /root/.triton/autotune: No such file or directory
W0107 20:17:18.822000 140621285412864 torch/distributed/run.py:779] 
W0107 20:17:18.822000 140621285412864 torch/distributed/run.py:779] *****************************************
W0107 20:17:18.822000 140621285412864 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0107 20:17:18.822000 140621285412864 torch/distributed/run.py:779] *****************************************
[2025-01-07 20:17:30,780] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-01-07 20:17:31,571] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
df: /root/.triton/autotune: No such file or directory
df: /root/.triton/autotune: No such file or directory
df: /root/.triton/autotune: No such file or directory
df: /root/.triton/autotune: No such file or directory
[2025-01-07 20:17:48,033] [INFO] [comm.py:652:init_distributed] cdb=None
/workspace/tulu_env/lib/python3.10/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(use_seedable_sampler=True)
  warnings.warn(
INFO:__main__:Distributed environment: DEEPSPEED  Backend: nccl
Num processes: 2
Process index: 1
Local process index: 1
Device: cuda:1

Mixed precision type: bf16
ds_config: {'bf16': {'enabled': True}, 'zero_optimization': {'stage': 3, 'overlap_comm': True, 'contiguous_gradients': True, 'sub_group_size': 1000000000.0, 'reduce_bucket_size': 'auto', 'stage3_prefetch_bucket_size': 'auto', 'stage3_param_persistence_threshold': 'auto', 'stage3_max_live_parameters': 1000000000.0, 'stage3_max_reuse_distance': 1000000000.0, 'stage3_gather_16bit_weights_on_model_save': True}, 'gradient_accumulation_steps': 'auto', 'gradient_clipping': 'auto', 'steps_per_print': inf, 'train_batch_size': 'auto', 'train_micro_batch_size_per_gpu': 'auto', 'wall_clock_breakdown': False, 'fp16': {'enabled': False}}

[2025-01-07 20:17:48,887] [INFO] [comm.py:652:init_distributed] cdb=None
[2025-01-07 20:17:48,887] [INFO] [comm.py:683:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
/workspace/tulu_env/lib/python3.10/site-packages/accelerate/accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(use_seedable_sampler=True)
  warnings.warn(
INFO:__main__:Distributed environment: DEEPSPEED  Backend: nccl
Num processes: 2
Process index: 0
Local process index: 0
Device: cuda:0

Mixed precision type: bf16
ds_config: {'bf16': {'enabled': True}, 'zero_optimization': {'stage': 3, 'overlap_comm': True, 'contiguous_gradients': True, 'sub_group_size': 1000000000.0, 'reduce_bucket_size': 'auto', 'stage3_prefetch_bucket_size': 'auto', 'stage3_param_persistence_threshold': 'auto', 'stage3_max_live_parameters': 1000000000.0, 'stage3_max_reuse_distance': 1000000000.0, 'stage3_gather_16bit_weights_on_model_save': True}, 'gradient_accumulation_steps': 'auto', 'gradient_clipping': 'auto', 'steps_per_print': inf, 'train_batch_size': 'auto', 'train_micro_batch_size_per_gpu': 'auto', 'wall_clock_breakdown': False, 'fp16': {'enabled': False}}

loading configuration file config.json from cache at /workspace/hub/models--WenWW--sparse_Llama_8B_2of4_SFT_test/snapshots/4c6f3a2e57976a4879b6f34b6f400bc198179197/config.json
Model config LlamaConfig {
  "_name_or_path": "WenWW/sparse_Llama_8B_2of4_SFT_test",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.43.4",
  "use_cache": true,
  "vocab_size": 128264
}

loading file tokenizer.json from cache at /workspace/hub/models--WenWW--sparse_Llama_8B_2of4_SFT_test/snapshots/4c6f3a2e57976a4879b6f34b6f400bc198179197/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /workspace/hub/models--WenWW--sparse_Llama_8B_2of4_SFT_test/snapshots/4c6f3a2e57976a4879b6f34b6f400bc198179197/special_tokens_map.json
loading file tokenizer_config.json from cache at /workspace/hub/models--WenWW--sparse_Llama_8B_2of4_SFT_test/snapshots/4c6f3a2e57976a4879b6f34b6f400bc198179197/tokenizer_config.json
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
loading weights file pytorch_model.bin from cache at /workspace/hub/models--WenWW--sparse_Llama_8B_2of4_SFT_test/snapshots/4c6f3a2e57976a4879b6f34b6f400bc198179197/pytorch_model.bin.index.json
Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[2025-01-07 20:17:51,445] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 2
The model was loaded with use_flash_attention_2=True, which is deprecated and may be removed in a future release. Please use `attn_implementation="flash_attention_2"` instead.
You are attempting to use Flash Attention 2.0 without specifying a torch dtype. This might lead to unexpected behaviour
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in LlamaForCausalLM is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`
Generate config GenerationConfig {
  "bos_token_id": 128000,
  "eos_token_id": 128001
}

Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in LlamaModel is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`
[2025-01-07 20:17:51,687] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 2
[2025-01-07 20:18:03,281] [INFO] [partition_parameters.py:345:__exit__] finished initializing model - num_params = 291, num_elems = 8.03B

Loading checkpoint shards:   0%|                                                                          | 0/4 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|                                                                          | 0/4 [00:00<?, ?it/s]
Loading checkpoint shards:  25%|████████████████▌                                                 | 1/4 [00:04<00:13,  4.35s/it]
Loading checkpoint shards:  50%|█████████████████████████████████                                 | 2/4 [00:08<00:08,  4.31s/it]
Loading checkpoint shards:  25%|████████████████▌                                                 | 1/4 [00:10<00:30, 10.12s/it]
Loading checkpoint shards:  75%|█████████████████████████████████████████████████▌                | 3/4 [00:11<00:03,  3.87s/it]
Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████| 4/4 [00:12<00:00,  2.73s/it]
Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████| 4/4 [00:12<00:00,  3.24s/it]
[2025-01-07 20:18:16,476] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 2

Loading checkpoint shards:  50%|█████████████████████████████████                                 | 2/4 [00:16<00:15,  7.93s/it]
Loading checkpoint shards:  75%|█████████████████████████████████████████████████▌                | 3/4 [00:22<00:07,  7.08s/it]
Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████| 4/4 [00:23<00:00,  4.70s/it]
Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████| 4/4 [00:23<00:00,  5.91s/it]
All model checkpoint weights were used when initializing LlamaForCausalLM.

All the weights of LlamaForCausalLM were initialized from the model checkpoint at WenWW/sparse_Llama_8B_2of4_SFT_test.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
loading configuration file generation_config.json from cache at /workspace/hub/models--WenWW--sparse_Llama_8B_2of4_SFT_test/snapshots/4c6f3a2e57976a4879b6f34b6f400bc198179197/generation_config.json
Generate config GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "temperature": 0.6,
  "top_p": 0.9
}

loading weights file pytorch_model.bin from cache at /workspace/hub/models--WenWW--sparse_Llama_8B_2of4_SFT_test/snapshots/4c6f3a2e57976a4879b6f34b6f400bc198179197/pytorch_model.bin.index.json
Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[2025-01-07 20:18:27,124] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 2
Generate config GenerationConfig {
  "bos_token_id": 128000,
  "eos_token_id": 128001
}

[2025-01-07 20:18:39,330] [INFO] [partition_parameters.py:345:__exit__] finished initializing model - num_params = 582, num_elems = 16.06B

Loading checkpoint shards:   0%|                                                                          | 0/4 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|                                                                          | 0/4 [00:00<?, ?it/s]
Loading checkpoint shards:  25%|████████████████▌                                                 | 1/4 [00:04<00:12,  4.11s/it]
Loading checkpoint shards:  50%|█████████████████████████████████                                 | 2/4 [00:08<00:08,  4.09s/it]
Loading checkpoint shards:  25%|████████████████▌                                                 | 1/4 [00:09<00:29,  9.76s/it]
Loading checkpoint shards:  75%|█████████████████████████████████████████████████▌                | 3/4 [00:11<00:03,  3.94s/it]
Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████| 4/4 [00:13<00:00,  2.83s/it]
Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████| 4/4 [00:13<00:00,  3.27s/it]

Loading checkpoint shards:  50%|█████████████████████████████████                                 | 2/4 [00:15<00:15,  7.68s/it]
Loading checkpoint shards:  75%|█████████████████████████████████████████████████▌                | 3/4 [00:22<00:07,  7.00s/it]
Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████| 4/4 [00:23<00:00,  4.63s/it]
Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████| 4/4 [00:23<00:00,  5.79s/it]
All model checkpoint weights were used when initializing LlamaForCausalLM.

All the weights of LlamaForCausalLM were initialized from the model checkpoint at WenWW/sparse_Llama_8B_2of4_SFT_test.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
loading configuration file generation_config.json from cache at /workspace/hub/models--WenWW--sparse_Llama_8B_2of4_SFT_test/snapshots/4c6f3a2e57976a4879b6f34b6f400bc198179197/generation_config.json
Generate config GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "temperature": 0.6,
  "top_p": 0.9
}

INFO:__main__:Limiting training samples to 100 from 92858.

Tokenizing and reformatting instruction data (num_proc=16):   0%|                                | 0/100 [00:00<?, ? examples/s]
Tokenizing and reformatting instruction data (num_proc=16):   1%|▏                       | 1/100 [00:00<01:10,  1.40 examples/s]
Tokenizing and reformatting instruction data (num_proc=16):   8%|█▉                      | 8/100 [00:00<00:08, 11.10 examples/s]
Tokenizing and reformatting instruction data (num_proc=16):  15%|███▍                   | 15/100 [00:01<00:04, 17.23 examples/s]
Tokenizing and reformatting instruction data (num_proc=16):  22%|█████                  | 22/100 [00:01<00:03, 21.78 examples/s]
Tokenizing and reformatting instruction data (num_proc=16):  29%|██████▋                | 29/100 [00:01<00:02, 27.06 examples/s]
Tokenizing and reformatting instruction data (num_proc=16):  35%|████████               | 35/100 [00:01<00:02, 29.83 examples/s]
Tokenizing and reformatting instruction data (num_proc=16):  41%|█████████▍             | 41/100 [00:01<00:01, 31.71 examples/s]
Tokenizing and reformatting instruction data (num_proc=16):  47%|██████████▊            | 47/100 [00:01<00:01, 33.56 examples/s]
Tokenizing and reformatting instruction data (num_proc=16):  52%|███████████▉           | 52/100 [00:02<00:01, 32.60 examples/s]
Tokenizing and reformatting instruction data (num_proc=16):  59%|█████████████▌         | 59/100 [00:02<00:01, 28.18 examples/s]
Tokenizing and reformatting instruction data (num_proc=16):  70%|████████████████       | 70/100 [00:02<00:00, 37.23 examples/s]
Tokenizing and reformatting instruction data (num_proc=16):  77%|█████████████████▋     | 77/100 [00:02<00:00, 38.59 examples/s]
Tokenizing and reformatting instruction data (num_proc=16):  83%|███████████████████    | 83/100 [00:02<00:00, 40.73 examples/s]
Tokenizing and reformatting instruction data (num_proc=16):  89%|████████████████████▍  | 89/100 [00:03<00:00, 36.40 examples/s]
Tokenizing and reformatting instruction data (num_proc=16):  95%|█████████████████████▊ | 95/100 [00:03<00:00, 38.09 examples/s]
Tokenizing and reformatting instruction data (num_proc=16): 100%|██████████████████████| 100/100 [00:03<00:00, 29.27 examples/s]

Filter:   0%|                                                                                    | 0/100 [00:00<?, ? examples/s]
Filter: 100%|████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 1752.04 examples/s]

Filter:   0%|                                                                                    | 0/100 [00:00<?, ? examples/s]
Filter: 100%|████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 3588.59 examples/s]
INFO:__main__:Sample 81 of the training set: {'chosen_input_ids': tensor([    27,     91,    882,     91,    397,  30138,    793,   4195,    964,
            25,    436,  48136,    329,   1805,    271,  51428,     25,  99593,
          2751,   1139,  19143,    304,  13596,   3314,    520,    393,   2891,
            88,   5726,     72,     35,  47784,     11,    706,    311,   5101,
          2268,   3019,     25,    358,   2846,    912,  15779,     11,    719,
           358,   1097,   5128,  11537,    449,    279,   2383,    320,  30562,
           304,  12544,     11,    659,   9232,    323,   6166,   7016,     11,
          5623,     14,  47921,   2383,     11,   5099,    570,   4452,    358,
          2846,   5115,  22568,    389,    420,    832,    382,   4516,    856,
         23601,    323,   1077,   6411,   4333,    320,   4345,   8220,   6913,
             8,    304,  29538,     11,    733,    311,   1518,    393,   2891,
            88,   5726,     72,     35,    304,   1561,  27008,     13,   4800,
           279,   6411,   7564,   5828,    956,   2728,   8041,    311,   5387,
          2385,     13,   3092,  42988,    323,    279,   7564,     11,  29850,
           477,   8996,     11,   4018,   1063,   1584,    304,   4156,    315,
           264,   7564,     11,    889,   3250,    956,   2512,     11,    719,
           813,   8834,  23601,    436,   1154,    323,  61851,    856,  41126,
            13,   2435,    636,   1139,    433,     11,   1364,   2795,    814,
          2225,  19336,    922,    220,     19,   3115,     11,    856,  42988,
         12098,    264,   6573,    304,   1077,   3663,    323,    430,    596,
           430,     13,  22172,   5900,    389,   4717,    287,     11,   8996,
            11,    889,  34672,    382,  13575,  44806,      6,    264,   2478,
          4520,   3010,     11,   1364,    374,   2231,    304,    279,   1203,
           315,    264,   6293,   1841,    555,   4868,     11,   1405,    279,
          6411,   7564,   2736,    374,    320,    383,   5828,    956,   1524,
          3815,   4205,    705,    323,   1518,    279,   8834,   3828,   2133,
           389,    922,   1268,    814,    279,   2466,   3776,  36157,    323,
           279,   3828,    342,   3811,    709,    389,   1077,    320,    438,
           279,  26923,   1120,  16387,   7113,    304,   9306,    570,   4815,
         57377,  21701,   1124,   1022,    520,    279,   8952,     11,   2795,
           568,  13919,    856,  41126,   6801,   5147,    304,    430,    814,
          3287,    956,    656,   4205,     11,    719,   1606,   1364,    596,
         21039,  11965,     11,    814,  38023,    733,    311,   5590,     13,
          1283,   3250,    956,   1650,    279,   4333,    596,   6411,   2307,
         22625,     11,    719,    568,    706,    311,    733,    311,   5590,
          2288,     13,   4815,   4071,    279,   3575,    374,    856,  41126,
           374,  16706,   1203,   3432,     11,  29538,    374,  42436,   4028,
           279,   3224,     13,   3005,   3250,    956,    617,    279,  20769,
           311,  11722,   1203,     11,   6463,    279,   3300,     13,    358,
          2846,   2771,   1364,    649,    636,    264,  15779,    311,   4097,
          1077,    304,  28310,    689,    320,   9210,    596,   1148,   1274,
           656,    304,   1521,   4595,    315,  15082,     11,   1314,  10380,
           719,    430,    596,    264,   2766,  11646,    323,  27873,   2195,
          4516,    358,   1541,    956,   1440,   1148,    420,    374,     13,
          2435,  15058,    956,  11684,     11,    814,  15058,    956,  12800,
            13,   3639,   3169,    315,   5590,    374,    420,     30,   2435,
           617,    311,  73293,    872,   1162,   1603,    264,  11913,    382,
          5159,   8101,    374,    279,   8834,   3828,   2834,    956,   1524,
          1501,    709,    311,   5590,     11,    719,   1148,    374,   1364,
         10171,    311,    656,    304,    420,   6671,     30,   4815,  12947,
           382,  13778,     26,   7842,    512,     27,     91,  78191,     91,
           397,  55611,  59797,    856,  41126,  46671,   1077,    520,    264,
         21497,    304,  29538,     11,    505,    902,   1364,    596,  16706,
          1203,   2162,   3432,     11,    902,    374,   2860,  28718,    323,
          1364,   4295,    856,  41126,   1176,    323,   1524,   6293,  13919,
          1077,     13,   3005,    374,  10171,    311,   1501,    709,    304,
          5590,   1828,   2046,    323,  73293,   1077,   1162,     11,    902,
           374,  27873, 128001]), 'chosen_labels': tensor([  -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,  55611,  59797,    856,  41126,  46671,   1077,    520,    264,
         21497,    304,  29538,     11,    505,    902,   1364,    596,  16706,
          1203,   2162,   3432,     11,    902,    374,   2860,  28718,    323,
          1364,   4295,    856,  41126,   1176,    323,   1524,   6293,  13919,
          1077,     13,   3005,    374,  10171,    311,   1501,    709,    304,
          5590,   1828,   2046,    323,  73293,   1077,   1162,     11,    902,
           374,  27873, 128001]), 'chosen_attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), 'rejected_input_ids': tensor([    27,     91,    882,     91,    397,  30138,    793,   4195,    964,
            25,    436,  48136,    329,   1805,    271,  51428,     25,  99593,
          2751,   1139,  19143,    304,  13596,   3314,    520,    393,   2891,
            88,   5726,     72,     35,  47784,     11,    706,    311,   5101,
          2268,   3019,     25,    358,   2846,    912,  15779,     11,    719,
           358,   1097,   5128,  11537,    449,    279,   2383,    320,  30562,
           304,  12544,     11,    659,   9232,    323,   6166,   7016,     11,
          5623,     14,  47921,   2383,     11,   5099,    570,   4452,    358,
          2846,   5115,  22568,    389,    420,    832,    382,   4516,    856,
         23601,    323,   1077,   6411,   4333,    320,   4345,   8220,   6913,
             8,    304,  29538,     11,    733,    311,   1518,    393,   2891,
            88,   5726,     72,     35,    304,   1561,  27008,     13,   4800,
           279,   6411,   7564,   5828,    956,   2728,   8041,    311,   5387,
          2385,     13,   3092,  42988,    323,    279,   7564,     11,  29850,
           477,   8996,     11,   4018,   1063,   1584,    304,   4156,    315,
           264,   7564,     11,    889,   3250,    956,   2512,     11,    719,
           813,   8834,  23601,    436,   1154,    323,  61851,    856,  41126,
            13,   2435,    636,   1139,    433,     11,   1364,   2795,    814,
          2225,  19336,    922,    220,     19,   3115,     11,    856,  42988,
         12098,    264,   6573,    304,   1077,   3663,    323,    430,    596,
           430,     13,  22172,   5900,    389,   4717,    287,     11,   8996,
            11,    889,  34672,    382,  13575,  44806,      6,    264,   2478,
          4520,   3010,     11,   1364,    374,   2231,    304,    279,   1203,
           315,    264,   6293,   1841,    555,   4868,     11,   1405,    279,
          6411,   7564,   2736,    374,    320,    383,   5828,    956,   1524,
          3815,   4205,    705,    323,   1518,    279,   8834,   3828,   2133,
           389,    922,   1268,    814,    279,   2466,   3776,  36157,    323,
           279,   3828,    342,   3811,    709,    389,   1077,    320,    438,
           279,  26923,   1120,  16387,   7113,    304,   9306,    570,   4815,
         57377,  21701,   1124,   1022,    520,    279,   8952,     11,   2795,
           568,  13919,    856,  41126,   6801,   5147,    304,    430,    814,
          3287,    956,    656,   4205,     11,    719,   1606,   1364,    596,
         21039,  11965,     11,    814,  38023,    733,    311,   5590,     13,
          1283,   3250,    956,   1650,    279,   4333,    596,   6411,   2307,
         22625,     11,    719,    568,    706,    311,    733,    311,   5590,
          2288,     13,   4815,   4071,    279,   3575,    374,    856,  41126,
           374,  16706,   1203,   3432,     11,  29538,    374,  42436,   4028,
           279,   3224,     13,   3005,   3250,    956,    617,    279,  20769,
           311,  11722,   1203,     11,   6463,    279,   3300,     13,    358,
          2846,   2771,   1364,    649,    636,    264,  15779,    311,   4097,
          1077,    304,  28310,    689,    320,   9210,    596,   1148,   1274,
           656,    304,   1521,   4595,    315,  15082,     11,   1314,  10380,
           719,    430,    596,    264,   2766,  11646,    323,  27873,   2195,
          4516,    358,   1541,    956,   1440,   1148,    420,    374,     13,
          2435,  15058,    956,  11684,     11,    814,  15058,    956,  12800,
            13,   3639,   3169,    315,   5590,    374,    420,     30,   2435,
           617,    311,  73293,    872,   1162,   1603,    264,  11913,    382,
          5159,   8101,    374,    279,   8834,   3828,   2834,    956,   1524,
          1501,    709,    311,   5590,     11,    719,   1148,    374,   1364,
         10171,    311,    656,    304,    420,   6671,     30,   4815,  12947,
           382,  13778,     26,   7842,    512,     27,     91,  78191,     91,
           397,   5159,  41126,   5334,   1139,    264,   4465,    304,   1561,
         27008,     11,   5334,  12800,   1306,   1063,   3828,   5334,  46671,
            11,    323,  43394,    279,  11213,   1203,     13,   3639,    596,
           279,   5590,   1920,     30, 128001]), 'rejected_labels': tensor([  -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   5159,  41126,   5334,   1139,    264,   4465,    304,   1561,
         27008,     11,   5334,  12800,   1306,   1063,   3828,   5334,  46671,
            11,    323,  43394,    279,  11213,   1203,     13,   3639,    596,
           279,   5590,   1920,     30, 128001]), 'rejected_attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}.
INFO:__main__:Sample 14 of the training set: {'chosen_input_ids': tensor([    27,     91,    882,     91,    397,  30138,    793,   4195,    964,
            25,    436,     14,  86924,    271,  51428,     25,  19418,    499,
           387,  13088,    422,    701,   5745,    889,   6439,    220,   6330,
            15,   8931,   3201,     11,    304,   2500,   3224,     11,    889,
           706,  12263,    499,    220,     19,   3115,   2533,   5587,     11,
          3309,    499,   8469,    315,    892,     11,  28653,  40715,     11,
           358,   3021,    499,   1633,   1790,     11,    358,   2133,    311,
           473,   1944,    388,  18396,    369,  27296,     11,   5353,   1202,
         11689,   6138,    505,    279,  12109,    358,   2133,    311,   7172,
         13200,    520,    382,   3019,     25,    220,     16,      8,    358,
          2751,    264,  38852,     40,    304,    220,    679,     19,    271,
            17,      8,    358,    617,    264,  11364,  23601,    889,   6439,
           304,  30613,    271,     18,      8,    358,   3974,    304,    459,
         25629,   6424,    315,  32669,   1291,   4409,     11,  29974,    271,
            19,      8,    358,   1097,   1633,   1790,    304,   3021,    449,
          1077,    382,     20,      8,    358,    990,    264,  15234,   9899,
            11,    386,   7424,     11,    220,     20,    309,    482,    220,
            16,   5298,   1174,    358,   1205,    311,  15508,    709,    520,
           220,     19,    309,    311,    387,    520,    990,    382,     21,
             8,    358,    617,  12263,  30613,    220,     19,   3115,   2533,
          5587,    323,    584,   2322,    389,  43274,   1174,  10280,  32460,
           220,    679,     20,     13,   4815,     22,      8,   3005,    374,
           264,  11364,     11,  20333,     11,  24415,     11,  10437,   5333,
            11,    358,   1097,   9539,   5029,   1648,    927,    856,   2010,
           323,    264,   2204,  10966,    382,     23,      8,    358,  12570,
           311,   1077,  13985,  10043,   2162,    505,    990,     13,    358,
           617,    264,  24759,    369,    264,   1949,   3814,    520,    264,
          2254,  12109,     11,    323,    358,   3309,   1077,     11,   1427,
           358,    649,    956,   6678,   3131,    617,    810,   1109,    358,
          1288,   7172,     11,    719,    311,    757,    420,    374,   1093,
           264,    220,    717,   6596,  20769,     11,    323,    358,   2834,
           956,   6678,     11,    358,   6227,    279,   1566,    892,    358,
           574,    520,    279,  12109,     11,    358,   3287,    956,   1093,
           279,  15657,     13,    358,    690,   2133,    311,    473,   1944,
           388,     11,    323,    358,   1514,   3725,  68329,    449,   1077,
            11,    358,   1097,   2133,    369,    279,   3691,     13,   3005,
          2646,   1071,   1541,    956,    733,     11,   1364,   2646,   1071,
           422,    499,    733,    358,    690,    387,  13194,     11,   4815,
            24,      8,  11450,    374,    279,   1828,   1938,     11,    358,
          1097,    990,     11,    358,    617,  58077,  11157,    311,   1077,
            11,    323,    358,   1097,   3794,   1633,  56152,    307,     11,
          1633,   2875,  14633,   1203,     11,   9539,   1070,    374,    264,
          3575,    449,   1566,   3814,    382,  13778,     26,   7842,    512,
            27,     91,  78191,     91,    397,     40,   1097,    539,   2771,
          1148,    279,   7976,    311,    656,     13,    358,    617,   3309,
          1077,    358,   3021,   1077,     11,   1364,    374,    279,   3021,
           315,    856,   2324,     11,    358,   3021,   1077,    719,    358,
          1097,   5029,   1648,    927,    856,   2010,     13, 128001]), 'chosen_labels': tensor([  -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,     40,   1097,    539,   2771,
          1148,    279,   7976,    311,    656,     13,    358,    617,   3309,
          1077,    358,   3021,   1077,     11,   1364,    374,    279,   3021,
           315,    856,   2324,     11,    358,   3021,   1077,    719,    358,
          1097,   5029,   1648,    927,    856,   2010,     13, 128001]), 'chosen_attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), 'rejected_input_ids': tensor([    27,     91,    882,     91,    397,  30138,    793,   4195,    964,
            25,    436,     14,  86924,    271,  51428,     25,  19418,    499,
           387,  13088,    422,    701,   5745,    889,   6439,    220,   6330,
            15,   8931,   3201,     11,    304,   2500,   3224,     11,    889,
           706,  12263,    499,    220,     19,   3115,   2533,   5587,     11,
          3309,    499,   8469,    315,    892,     11,  28653,  40715,     11,
           358,   3021,    499,   1633,   1790,     11,    358,   2133,    311,
           473,   1944,    388,  18396,    369,  27296,     11,   5353,   1202,
         11689,   6138,    505,    279,  12109,    358,   2133,    311,   7172,
         13200,    520,    382,   3019,     25,    220,     16,      8,    358,
          2751,    264,  38852,     40,    304,    220,    679,     19,    271,
            17,      8,    358,    617,    264,  11364,  23601,    889,   6439,
           304,  30613,    271,     18,      8,    358,   3974,    304,    459,
         25629,   6424,    315,  32669,   1291,   4409,     11,  29974,    271,
            19,      8,    358,   1097,   1633,   1790,    304,   3021,    449,
          1077,    382,     20,      8,    358,    990,    264,  15234,   9899,
            11,    386,   7424,     11,    220,     20,    309,    482,    220,
            16,   5298,   1174,    358,   1205,    311,  15508,    709,    520,
           220,     19,    309,    311,    387,    520,    990,    382,     21,
             8,    358,    617,  12263,  30613,    220,     19,   3115,   2533,
          5587,    323,    584,   2322,    389,  43274,   1174,  10280,  32460,
           220,    679,     20,     13,   4815,     22,      8,   3005,    374,
           264,  11364,     11,  20333,     11,  24415,     11,  10437,   5333,
            11,    358,   1097,   9539,   5029,   1648,    927,    856,   2010,
           323,    264,   2204,  10966,    382,     23,      8,    358,  12570,
           311,   1077,  13985,  10043,   2162,    505,    990,     13,    358,
           617,    264,  24759,    369,    264,   1949,   3814,    520,    264,
          2254,  12109,     11,    323,    358,   3309,   1077,     11,   1427,
           358,    649,    956,   6678,   3131,    617,    810,   1109,    358,
          1288,   7172,     11,    719,    311,    757,    420,    374,   1093,
           264,    220,    717,   6596,  20769,     11,    323,    358,   2834,
           956,   6678,     11,    358,   6227,    279,   1566,    892,    358,
           574,    520,    279,  12109,     11,    358,   3287,    956,   1093,
           279,  15657,     13,    358,    690,   2133,    311,    473,   1944,
           388,     11,    323,    358,   1514,   3725,  68329,    449,   1077,
            11,    358,   1097,   2133,    369,    279,   3691,     13,   3005,
          2646,   1071,   1541,    956,    733,     11,   1364,   2646,   1071,
           422,    499,    733,    358,    690,    387,  13194,     11,   4815,
            24,      8,  11450,    374,    279,   1828,   1938,     11,    358,
          1097,    990,     11,    358,    617,  58077,  11157,    311,   1077,
            11,    323,    358,   1097,   3794,   1633,  56152,    307,     11,
          1633,   2875,  14633,   1203,     11,   9539,   1070,    374,    264,
          3575,    449,   1566,   3814,    382,  13778,     26,   7842,    512,
            27,     91,  78191,     91,    397,     40,   1097,  42642,    323,
           358,   1205,    311,  10594,     13, 128001]), 'rejected_labels': tensor([  -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,     40,   1097,  42642,    323,
           358,   1205,    311,  10594,     13, 128001]), 'rejected_attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}.
INFO:__main__:Sample 3 of the training set: {'chosen_input_ids': tensor([    27,     91,    882,     91,    397,  30138,    793,   4195,    964,
            25,    436,     14,  86924,    271,  51428,     25,  21720,    510,
          3971,     60,    539,  12365,    311,    757,    510,   1691,     60,
          1606,    315,    264,   8577,    358,   1097,   9293,    271,   3019,
            25,   3092,  26923,    323,    358,    527,   1317,   6138,     13,
          1226,    617,    264,   8577,  13205,    420,   7474,    902,  18065,
           757,   2133,    927,    311,   1461,    304,    279,   7427,     13,
          1115,    690,    387,    279,   2132,    892,    358,    617,   3604,
          1027,    449,   1461,    304,   1732,     13,    358,   1097,  16706,
           505,    279,   6560,    449,    856,  39959,    311,    279,  11226,
         13962,     13,    578,   4113,   3197,    574,    369,    757,    311,
         11722,    927,    311,    856,  26923,    304,    279,   9909,  13962,
           320,   2465,   6699,    527,  13560,    287,    389,    279,  11226,
         13962,      8,    719,   1606,    856,  39959,    574,  74340,    704,
           779,   1790,    922,    757,   2133,    311,   3449,    856,  26923,
           602,   1071,    584,    649,    682,   5754,   8577,   1070,   3871,
            13,    358,   1524,  18719,   1077,    389,    279,   8577,    449,
           603,     13,    358,    617,   2728,   1077,    682,    315,   1057,
         13003,    779,    430,   1364,    649,   5944,   2212,    449,    603,
           382,    791,   3197,    574,    369,    757,    311,   4822,    389,
           279,    220,     19,    339,   5887,    323,  11722,   1203,    389,
           279,    220,     20,    339,     13,  67937,   7020,    420,     13,
           358,   3309,   1077,    358,   1047,  34070,    264,  11213,   1203,
          2736,    505,    279,   9909,  13962,    311,  11226,  13962,    320,
          2940,   1364,   1053,   3820,    757,    709,    323,    584,   1053,
         11722,   1203,    311,    279,   6560,   3871,    570,   3005,    706,
          8208,  13088,    520,    757,   1606,   1364,    649,    956,   4510,
           358,   1053,   2363,    264,  11213,    994,   1364,   3309,    757,
          1364,   3287,    956,   1390,    757,  16706,    389,    856,   1866,
            13,   2468,    279,    892,    358,   1047,  34070,    433,   1364,
          3309,    757,   1364,   5828,    956,  16926,   5754,   8577,    449,
           603,     13,   3005,   7020,    279,   8577,    574,  12765,  49711,
          5269,    775,    574,    358,    311,    636,   2162,    422,    358,
          1541,    956,  11722,     30,   4815,     40,   1097,   7060,  16706,
           389,    856,   1866,    433,   3250,    956,  20753,    757,    520,
           682,     13,    358,   2733,   1093,    358,    617,   2884,   4395,
           358,    649,    311,   1304,   1077,   2733,  10882,    449,    420,
          8577,    323,   1364,    374,   1120,   4560,    311,  80753,    433,
            13,  61399,  71291,  13778,     26,   7842,    512,     27,     91,
         78191,     91,    397,     44,    372,   3463,    358,    574,   2133,
           311,   5754,   8577,    449,    856,  26923,     13,    358,  26765,
           279,  11213,     13,  67937,    374,  57130,   1606,   1364,   3463,
          1364,   1053,    387,  21646,    449,    757,     13,    358,   1097,
          7060,    449,    430,     13, 128001]), 'chosen_labels': tensor([  -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,     44,    372,   3463,    358,    574,   2133,
           311,   5754,   8577,    449,    856,  26923,     13,    358,  26765,
           279,  11213,     13,  67937,    374,  57130,   1606,   1364,   3463,
          1364,   1053,    387,  21646,    449,    757,     13,    358,   1097,
          7060,    449,    430,     13, 128001]), 'chosen_attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1]), 'rejected_input_ids': tensor([    27,     91,    882,     91,    397,  30138,    793,   4195,    964,
            25,    436,     14,  86924,    271,  51428,     25,  21720,    510,
          3971,     60,    539,  12365,    311,    757,    510,   1691,     60,
          1606,    315,    264,   8577,    358,   1097,   9293,    271,   3019,
            25,   3092,  26923,    323,    358,    527,   1317,   6138,     13,
          1226,    617,    264,   8577,  13205,    420,   7474,    902,  18065,
           757,   2133,    927,    311,   1461,    304,    279,   7427,     13,
          1115,    690,    387,    279,   2132,    892,    358,    617,   3604,
          1027,    449,   1461,    304,   1732,     13,    358,   1097,  16706,
           505,    279,   6560,    449,    856,  39959,    311,    279,  11226,
         13962,     13,    578,   4113,   3197,    574,    369,    757,    311,
         11722,    927,    311,    856,  26923,    304,    279,   9909,  13962,
           320,   2465,   6699,    527,  13560,    287,    389,    279,  11226,
         13962,      8,    719,   1606,    856,  39959,    574,  74340,    704,
           779,   1790,    922,    757,   2133,    311,   3449,    856,  26923,
           602,   1071,    584,    649,    682,   5754,   8577,   1070,   3871,
            13,    358,   1524,  18719,   1077,    389,    279,   8577,    449,
           603,     13,    358,    617,   2728,   1077,    682,    315,   1057,
         13003,    779,    430,   1364,    649,   5944,   2212,    449,    603,
           382,    791,   3197,    574,    369,    757,    311,   4822,    389,
           279,    220,     19,    339,   5887,    323,  11722,   1203,    389,
           279,    220,     20,    339,     13,  67937,   7020,    420,     13,
           358,   3309,   1077,    358,   1047,  34070,    264,  11213,   1203,
          2736,    505,    279,   9909,  13962,    311,  11226,  13962,    320,
          2940,   1364,   1053,   3820,    757,    709,    323,    584,   1053,
         11722,   1203,    311,    279,   6560,   3871,    570,   3005,    706,
          8208,  13088,    520,    757,   1606,   1364,    649,    956,   4510,
           358,   1053,   2363,    264,  11213,    994,   1364,   3309,    757,
          1364,   3287,    956,   1390,    757,  16706,    389,    856,   1866,
            13,   2468,    279,    892,    358,   1047,  34070,    433,   1364,
          3309,    757,   1364,   5828,    956,  16926,   5754,   8577,    449,
           603,     13,   3005,   7020,    279,   8577,    574,  12765,  49711,
          5269,    775,    574,    358,    311,    636,   2162,    422,    358,
          1541,    956,  11722,     30,   4815,     40,   1097,   7060,  16706,
           389,    856,   1866,    433,   3250,    956,  20753,    757,    520,
           682,     13,    358,   2733,   1093,    358,    617,   2884,   4395,
           358,    649,    311,   1304,   1077,   2733,  10882,    449,    420,
          8577,    323,   1364,    374,   1120,   4560,    311,  80753,    433,
            13,  61399,  71291,  13778,     26,   7842,    512,     27,     91,
         78191,     91,    397,     44,    372,    374,  13088,    520,    757,
           369,    539,  16706,    389,    856,   1866,   8577,    311,   3449,
           856,  26923,     13, 128001]), 'rejected_labels': tensor([  -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,
          -100,   -100,   -100,     44,    372,    374,  13088,    520,    757,
           369,    539,  16706,    389,    856,   1866,   8577,    311,   3449,
           856,  26923,     13, 128001]), 'rejected_attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}.
[2025-01-07 20:19:06,832] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.15.0, git-hash=unknown, git-branch=unknown
[2025-01-07 20:19:06,832] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 2
[2025-01-07 20:19:06,843] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2025-01-07 20:19:06,845] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2025-01-07 20:19:06,846] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2025-01-07 20:19:06,861] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW
[2025-01-07 20:19:06,861] [INFO] [utils.py:59:is_zero_supported_optimizer] Checking ZeRO support for optimizer=AdamW type=<class 'torch.optim.adamw.AdamW'>
[2025-01-07 20:19:06,862] [INFO] [logging.py:96:log_dist] [Rank 0] Creating fp16 ZeRO stage 3 optimizer, MiCS is enabled False, Hierarchical params gather False
[2025-01-07 20:19:06,862] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 3 optimizer
[2025-01-07 20:19:07,103] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 2
[2025-01-07 20:19:07,167] [INFO] [utils.py:781:see_memory_usage] Stage 3 initialize beginning
[2025-01-07 20:19:07,168] [INFO] [utils.py:782:see_memory_usage] MA 14.96 GB         Max_MA 17.41 GB         CA 18.76 GB         Max_CA 19 GB 
[2025-01-07 20:19:07,168] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 29.61 GB, percent = 3.1%
[2025-01-07 20:19:07,171] [INFO] [stage3.py:164:__init__] Reduce bucket size 16777216
[2025-01-07 20:19:07,171] [INFO] [stage3.py:165:__init__] Prefetch bucket size 15099494
[2025-01-07 20:19:07,437] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]
[2025-01-07 20:19:07,438] [INFO] [utils.py:782:see_memory_usage] MA 14.96 GB         Max_MA 14.96 GB         CA 18.76 GB         Max_CA 19 GB 
[2025-01-07 20:19:07,439] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 29.61 GB, percent = 3.1%
Parameter Offload: Total persistent parameters: 266240 in 65 params
[2025-01-07 20:19:07,714] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [end]
[2025-01-07 20:19:07,715] [INFO] [utils.py:782:see_memory_usage] MA 14.96 GB         Max_MA 14.96 GB         CA 18.76 GB         Max_CA 19 GB 
[2025-01-07 20:19:07,716] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 29.61 GB, percent = 3.1%
[2025-01-07 20:19:07,965] [INFO] [utils.py:781:see_memory_usage] Before creating fp16 partitions
[2025-01-07 20:19:07,966] [INFO] [utils.py:782:see_memory_usage] MA 14.96 GB         Max_MA 14.96 GB         CA 18.76 GB         Max_CA 19 GB 
[2025-01-07 20:19:07,967] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 29.61 GB, percent = 3.1%
[2025-01-07 20:19:14,685] [INFO] [utils.py:781:see_memory_usage] After creating fp16 partitions: 4
[2025-01-07 20:19:14,688] [INFO] [utils.py:782:see_memory_usage] MA 14.96 GB         Max_MA 14.96 GB         CA 17.95 GB         Max_CA 19 GB 
[2025-01-07 20:19:14,688] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 30.49 GB, percent = 3.2%
[2025-01-07 20:19:14,914] [INFO] [utils.py:781:see_memory_usage] Before creating fp32 partitions
[2025-01-07 20:19:14,915] [INFO] [utils.py:782:see_memory_usage] MA 14.96 GB         Max_MA 14.96 GB         CA 17.95 GB         Max_CA 18 GB 
[2025-01-07 20:19:14,916] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 29.62 GB, percent = 3.1%
[2025-01-07 20:19:15,166] [INFO] [utils.py:781:see_memory_usage] After creating fp32 partitions
[2025-01-07 20:19:15,167] [INFO] [utils.py:782:see_memory_usage] MA 29.92 GB         Max_MA 31.73 GB         CA 34.83 GB         Max_CA 35 GB 
[2025-01-07 20:19:15,168] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 29.62 GB, percent = 3.1%
[2025-01-07 20:19:15,483] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states
[2025-01-07 20:19:15,484] [INFO] [utils.py:782:see_memory_usage] MA 29.92 GB         Max_MA 29.92 GB         CA 34.83 GB         Max_CA 35 GB 
[2025-01-07 20:19:15,485] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 29.62 GB, percent = 3.1%
[2025-01-07 20:19:15,730] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states
[2025-01-07 20:19:15,731] [INFO] [utils.py:782:see_memory_usage] MA 29.92 GB         Max_MA 33.74 GB         CA 38.65 GB         Max_CA 39 GB 
[2025-01-07 20:19:15,732] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 29.62 GB, percent = 3.1%
[2025-01-07 20:19:15,733] [INFO] [stage3.py:517:_setup_for_real_optimizer] optimizer state initialized
[2025-01-07 20:19:15,837] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 2
[2025-01-07 20:19:16,083] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer
[2025-01-07 20:19:16,084] [INFO] [utils.py:782:see_memory_usage] MA 37.43 GB         Max_MA 39.38 GB         CA 46.13 GB         Max_CA 46 GB 
[2025-01-07 20:19:16,085] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 29.62 GB, percent = 3.1%
[2025-01-07 20:19:16,085] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer_Stage3
[2025-01-07 20:19:16,085] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = None
[2025-01-07 20:19:16,086] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2025-01-07 20:19:16,086] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0], mom=[(0.9, 0.999)]
[2025-01-07 20:19:16,087] [INFO] [config.py:999:print] DeepSpeedEngine configuration:
[2025-01-07 20:19:16,088] [INFO] [config.py:1003:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2025-01-07 20:19:16,088] [INFO] [config.py:1003:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[2025-01-07 20:19:16,089] [INFO] [config.py:1003:print]   amp_enabled .................. False
[2025-01-07 20:19:16,089] [INFO] [config.py:1003:print]   amp_params ................... False
[2025-01-07 20:19:16,089] [INFO] [config.py:1003:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2025-01-07 20:19:16,089] [INFO] [config.py:1003:print]   bfloat16_enabled ............. True
[2025-01-07 20:19:16,089] [INFO] [config.py:1003:print]   bfloat16_immediate_grad_update  False
[2025-01-07 20:19:16,089] [INFO] [config.py:1003:print]   checkpoint_parallel_write_pipeline  False
[2025-01-07 20:19:16,090] [INFO] [config.py:1003:print]   checkpoint_tag_validation_enabled  True
[2025-01-07 20:19:16,090] [INFO] [config.py:1003:print]   checkpoint_tag_validation_fail  False
[2025-01-07 20:19:16,090] [INFO] [config.py:1003:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f537422d0c0>
[2025-01-07 20:19:16,090] [INFO] [config.py:1003:print]   communication_data_type ...... None
[2025-01-07 20:19:16,090] [INFO] [config.py:1003:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2025-01-07 20:19:16,090] [INFO] [config.py:1003:print]   curriculum_enabled_legacy .... False
[2025-01-07 20:19:16,090] [INFO] [config.py:1003:print]   curriculum_params_legacy ..... False
[2025-01-07 20:19:16,091] [INFO] [config.py:1003:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2025-01-07 20:19:16,091] [INFO] [config.py:1003:print]   data_efficiency_enabled ...... False
[2025-01-07 20:19:16,091] [INFO] [config.py:1003:print]   dataloader_drop_last ......... False
[2025-01-07 20:19:16,091] [INFO] [config.py:1003:print]   disable_allgather ............ False
[2025-01-07 20:19:16,091] [INFO] [config.py:1003:print]   dump_state ................... False
[2025-01-07 20:19:16,091] [INFO] [config.py:1003:print]   dynamic_loss_scale_args ...... None
[2025-01-07 20:19:16,091] [INFO] [config.py:1003:print]   eigenvalue_enabled ........... False
[2025-01-07 20:19:16,091] [INFO] [config.py:1003:print]   eigenvalue_gas_boundary_resolution  1
[2025-01-07 20:19:16,092] [INFO] [config.py:1003:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2025-01-07 20:19:16,092] [INFO] [config.py:1003:print]   eigenvalue_layer_num ......... 0
[2025-01-07 20:19:16,092] [INFO] [config.py:1003:print]   eigenvalue_max_iter .......... 100
[2025-01-07 20:19:16,092] [INFO] [config.py:1003:print]   eigenvalue_stability ......... 1e-06
[2025-01-07 20:19:16,092] [INFO] [config.py:1003:print]   eigenvalue_tol ............... 0.01
[2025-01-07 20:19:16,092] [INFO] [config.py:1003:print]   eigenvalue_verbose ........... False
[2025-01-07 20:19:16,092] [INFO] [config.py:1003:print]   elasticity_enabled ........... False
[2025-01-07 20:19:16,092] [INFO] [config.py:1003:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2025-01-07 20:19:16,092] [INFO] [config.py:1003:print]   fp16_auto_cast ............... None
[2025-01-07 20:19:16,093] [INFO] [config.py:1003:print]   fp16_enabled ................. False
[2025-01-07 20:19:16,093] [INFO] [config.py:1003:print]   fp16_master_weights_and_gradients  False
[2025-01-07 20:19:16,093] [INFO] [config.py:1003:print]   global_rank .................. 0
[2025-01-07 20:19:16,093] [INFO] [config.py:1003:print]   grad_accum_dtype ............. None
[2025-01-07 20:19:16,093] [INFO] [config.py:1003:print]   gradient_accumulation_steps .. 8
[2025-01-07 20:19:16,093] [INFO] [config.py:1003:print]   gradient_clipping ............ 1.0
[2025-01-07 20:19:16,093] [INFO] [config.py:1003:print]   gradient_predivide_factor .... 1.0
[2025-01-07 20:19:16,094] [INFO] [config.py:1003:print]   graph_harvesting ............. False
[2025-01-07 20:19:16,094] [INFO] [config.py:1003:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2025-01-07 20:19:16,094] [INFO] [config.py:1003:print]   initial_dynamic_scale ........ 1
[2025-01-07 20:19:16,094] [INFO] [config.py:1003:print]   load_universal_checkpoint .... False
[2025-01-07 20:19:16,094] [INFO] [config.py:1003:print]   loss_scale ................... 1.0
[2025-01-07 20:19:16,094] [INFO] [config.py:1003:print]   memory_breakdown ............. False
[2025-01-07 20:19:16,094] [INFO] [config.py:1003:print]   mics_hierarchial_params_gather  False
[2025-01-07 20:19:16,094] [INFO] [config.py:1003:print]   mics_shard_size .............. -1
[2025-01-07 20:19:16,095] [INFO] [config.py:1003:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[2025-01-07 20:19:16,095] [INFO] [config.py:1003:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2025-01-07 20:19:16,095] [INFO] [config.py:1003:print]   optimizer_legacy_fusion ...... False
[2025-01-07 20:19:16,095] [INFO] [config.py:1003:print]   optimizer_name ............... None
[2025-01-07 20:19:16,095] [INFO] [config.py:1003:print]   optimizer_params ............. None
[2025-01-07 20:19:16,095] [INFO] [config.py:1003:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2025-01-07 20:19:16,095] [INFO] [config.py:1003:print]   pld_enabled .................. False
[2025-01-07 20:19:16,096] [INFO] [config.py:1003:print]   pld_params ................... False
[2025-01-07 20:19:16,096] [INFO] [config.py:1003:print]   prescale_gradients ........... False
[2025-01-07 20:19:16,096] [INFO] [config.py:1003:print]   scheduler_name ............... None
[2025-01-07 20:19:16,096] [INFO] [config.py:1003:print]   scheduler_params ............. None
[2025-01-07 20:19:16,096] [INFO] [config.py:1003:print]   seq_parallel_communication_data_type  torch.float32
[2025-01-07 20:19:16,096] [INFO] [config.py:1003:print]   sparse_attention ............. None
[2025-01-07 20:19:16,096] [INFO] [config.py:1003:print]   sparse_gradients_enabled ..... False
[2025-01-07 20:19:16,096] [INFO] [config.py:1003:print]   steps_per_print .............. inf
[2025-01-07 20:19:16,096] [INFO] [config.py:1003:print]   timers_config ................ enabled=True synchronized=True
[2025-01-07 20:19:16,097] [INFO] [config.py:1003:print]   train_batch_size ............. 16
[2025-01-07 20:19:16,097] [INFO] [config.py:1003:print]   train_micro_batch_size_per_gpu  1
[2025-01-07 20:19:16,097] [INFO] [config.py:1003:print]   use_data_before_expert_parallel_  False
[2025-01-07 20:19:16,097] [INFO] [config.py:1003:print]   use_node_local_storage ....... False
[2025-01-07 20:19:16,097] [INFO] [config.py:1003:print]   wall_clock_breakdown ......... False
[2025-01-07 20:19:16,097] [INFO] [config.py:1003:print]   weight_quantization_config ... None
[2025-01-07 20:19:16,097] [INFO] [config.py:1003:print]   world_size ................... 2
[2025-01-07 20:19:16,097] [INFO] [config.py:1003:print]   zero_allow_untested_optimizer  True
[2025-01-07 20:19:16,098] [INFO] [config.py:1003:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=16777216 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=15099494 param_persistence_threshold=40960 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=True use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2025-01-07 20:19:16,098] [INFO] [config.py:1003:print]   zero_enabled ................. True
[2025-01-07 20:19:16,098] [INFO] [config.py:1003:print]   zero_force_ds_cpu_optimizer .. True
[2025-01-07 20:19:16,098] [INFO] [config.py:1003:print]   zero_optimization_stage ...... 3
[2025-01-07 20:19:16,098] [INFO] [config.py:989:print_user_config]   json = {
    "bf16": {
        "enabled": true
    }, 
    "zero_optimization": {
        "stage": 3, 
        "overlap_comm": true, 
        "contiguous_gradients": true, 
        "sub_group_size": 1.000000e+09, 
        "reduce_bucket_size": 1.677722e+07, 
        "stage3_prefetch_bucket_size": 1.509949e+07, 
        "stage3_param_persistence_threshold": 4.096000e+04, 
        "stage3_max_live_parameters": 1.000000e+09, 
        "stage3_max_reuse_distance": 1.000000e+09, 
        "stage3_gather_16bit_weights_on_model_save": true
    }, 
    "gradient_accumulation_steps": 8, 
    "gradient_clipping": 1.0, 
    "steps_per_print": inf, 
    "train_batch_size": 16, 
    "train_micro_batch_size_per_gpu": 1, 
    "wall_clock_breakdown": false, 
    "fp16": {
        "enabled": false
    }, 
    "zero_allow_untested_optimizer": true
}
[2025-01-07 20:19:16,099] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.15.0, git-hash=unknown, git-branch=unknown
[2025-01-07 20:19:16,099] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 2
[2025-01-07 20:19:16,108] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2025-01-07 20:19:16,110] [INFO] [logging.py:96:log_dist] [Rank 0] Creating ZeRO Offload
[2025-01-07 20:19:16,339] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]
[2025-01-07 20:19:16,340] [INFO] [utils.py:782:see_memory_usage] MA 37.43 GB         Max_MA 37.43 GB         CA 46.13 GB         Max_CA 46 GB 
[2025-01-07 20:19:16,341] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 29.62 GB, percent = 3.1%
Parameter Offload: Total persistent parameters: 266240 in 65 params
[2025-01-07 20:19:16,592] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [end]
[2025-01-07 20:19:16,593] [INFO] [utils.py:782:see_memory_usage] MA 37.43 GB         Max_MA 37.43 GB         CA 46.13 GB         Max_CA 46 GB 
[2025-01-07 20:19:16,594] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 29.62 GB, percent = 3.1%
[2025-01-07 20:19:16,595] [INFO] [config.py:999:print] DeepSpeedEngine configuration:
[2025-01-07 20:19:16,595] [INFO] [config.py:1003:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2025-01-07 20:19:16,596] [INFO] [config.py:1003:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[2025-01-07 20:19:16,596] [INFO] [config.py:1003:print]   amp_enabled .................. False
[2025-01-07 20:19:16,596] [INFO] [config.py:1003:print]   amp_params ................... False
[2025-01-07 20:19:16,596] [INFO] [config.py:1003:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2025-01-07 20:19:16,596] [INFO] [config.py:1003:print]   bfloat16_enabled ............. True
[2025-01-07 20:19:16,596] [INFO] [config.py:1003:print]   bfloat16_immediate_grad_update  False
[2025-01-07 20:19:16,597] [INFO] [config.py:1003:print]   checkpoint_parallel_write_pipeline  False
[2025-01-07 20:19:16,597] [INFO] [config.py:1003:print]   checkpoint_tag_validation_enabled  True
[2025-01-07 20:19:16,597] [INFO] [config.py:1003:print]   checkpoint_tag_validation_fail  False
[2025-01-07 20:19:16,597] [INFO] [config.py:1003:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f53b421b850>
[2025-01-07 20:19:16,597] [INFO] [config.py:1003:print]   communication_data_type ...... None
[2025-01-07 20:19:16,597] [INFO] [config.py:1003:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2025-01-07 20:19:16,598] [INFO] [config.py:1003:print]   curriculum_enabled_legacy .... False
[2025-01-07 20:19:16,598] [INFO] [config.py:1003:print]   curriculum_params_legacy ..... False
[2025-01-07 20:19:16,598] [INFO] [config.py:1003:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2025-01-07 20:19:16,598] [INFO] [config.py:1003:print]   data_efficiency_enabled ...... False
[2025-01-07 20:19:16,598] [INFO] [config.py:1003:print]   dataloader_drop_last ......... False
[2025-01-07 20:19:16,598] [INFO] [config.py:1003:print]   disable_allgather ............ False
[2025-01-07 20:19:16,599] [INFO] [config.py:1003:print]   dump_state ................... False
[2025-01-07 20:19:16,599] [INFO] [config.py:1003:print]   dynamic_loss_scale_args ...... None
[2025-01-07 20:19:16,599] [INFO] [config.py:1003:print]   eigenvalue_enabled ........... False
[2025-01-07 20:19:16,599] [INFO] [config.py:1003:print]   eigenvalue_gas_boundary_resolution  1
[2025-01-07 20:19:16,599] [INFO] [config.py:1003:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2025-01-07 20:19:16,599] [INFO] [config.py:1003:print]   eigenvalue_layer_num ......... 0
[2025-01-07 20:19:16,600] [INFO] [config.py:1003:print]   eigenvalue_max_iter .......... 100
[2025-01-07 20:19:16,600] [INFO] [config.py:1003:print]   eigenvalue_stability ......... 1e-06
[2025-01-07 20:19:16,600] [INFO] [config.py:1003:print]   eigenvalue_tol ............... 0.01
[2025-01-07 20:19:16,600] [INFO] [config.py:1003:print]   eigenvalue_verbose ........... False
[2025-01-07 20:19:16,600] [INFO] [config.py:1003:print]   elasticity_enabled ........... False
[2025-01-07 20:19:16,601] [INFO] [config.py:1003:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2025-01-07 20:19:16,601] [INFO] [config.py:1003:print]   fp16_auto_cast ............... None
[2025-01-07 20:19:16,601] [INFO] [config.py:1003:print]   fp16_enabled ................. False
[2025-01-07 20:19:16,601] [INFO] [config.py:1003:print]   fp16_master_weights_and_gradients  False
[2025-01-07 20:19:16,601] [INFO] [config.py:1003:print]   global_rank .................. 0
[2025-01-07 20:19:16,602] [INFO] [config.py:1003:print]   grad_accum_dtype ............. None
[2025-01-07 20:19:16,602] [INFO] [config.py:1003:print]   gradient_accumulation_steps .. 8
[2025-01-07 20:19:16,602] [INFO] [config.py:1003:print]   gradient_clipping ............ 1.0
[2025-01-07 20:19:16,602] [INFO] [config.py:1003:print]   gradient_predivide_factor .... 1.0
[2025-01-07 20:19:16,602] [INFO] [config.py:1003:print]   graph_harvesting ............. False
[2025-01-07 20:19:16,603] [INFO] [config.py:1003:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2025-01-07 20:19:16,603] [INFO] [config.py:1003:print]   initial_dynamic_scale ........ 1
[2025-01-07 20:19:16,603] [INFO] [config.py:1003:print]   load_universal_checkpoint .... False
[2025-01-07 20:19:16,603] [INFO] [config.py:1003:print]   loss_scale ................... 1.0
[2025-01-07 20:19:16,603] [INFO] [config.py:1003:print]   memory_breakdown ............. False
[2025-01-07 20:19:16,604] [INFO] [config.py:1003:print]   mics_hierarchial_params_gather  False
[2025-01-07 20:19:16,604] [INFO] [config.py:1003:print]   mics_shard_size .............. -1
[2025-01-07 20:19:16,604] [INFO] [config.py:1003:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[2025-01-07 20:19:16,604] [INFO] [config.py:1003:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2025-01-07 20:19:16,604] [INFO] [config.py:1003:print]   optimizer_legacy_fusion ...... False
[2025-01-07 20:19:16,604] [INFO] [config.py:1003:print]   optimizer_name ............... None
[2025-01-07 20:19:16,605] [INFO] [config.py:1003:print]   optimizer_params ............. None
[2025-01-07 20:19:16,605] [INFO] [config.py:1003:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2025-01-07 20:19:16,605] [INFO] [config.py:1003:print]   pld_enabled .................. False
[2025-01-07 20:19:16,605] [INFO] [config.py:1003:print]   pld_params ................... False
[2025-01-07 20:19:16,605] [INFO] [config.py:1003:print]   prescale_gradients ........... False
[2025-01-07 20:19:16,605] [INFO] [config.py:1003:print]   scheduler_name ............... None
[2025-01-07 20:19:16,606] [INFO] [config.py:1003:print]   scheduler_params ............. None
[2025-01-07 20:19:16,606] [INFO] [config.py:1003:print]   seq_parallel_communication_data_type  torch.float32
[2025-01-07 20:19:16,606] [INFO] [config.py:1003:print]   sparse_attention ............. None
[2025-01-07 20:19:16,606] [INFO] [config.py:1003:print]   sparse_gradients_enabled ..... False
[2025-01-07 20:19:16,606] [INFO] [config.py:1003:print]   steps_per_print .............. inf
[2025-01-07 20:19:16,606] [INFO] [config.py:1003:print]   timers_config ................ enabled=True synchronized=True
[2025-01-07 20:19:16,606] [INFO] [config.py:1003:print]   train_batch_size ............. 16
[2025-01-07 20:19:16,606] [INFO] [config.py:1003:print]   train_micro_batch_size_per_gpu  1
[2025-01-07 20:19:16,607] [INFO] [config.py:1003:print]   use_data_before_expert_parallel_  False
[2025-01-07 20:19:16,607] [INFO] [config.py:1003:print]   use_node_local_storage ....... False
[2025-01-07 20:19:16,607] [INFO] [config.py:1003:print]   wall_clock_breakdown ......... False
[2025-01-07 20:19:16,607] [INFO] [config.py:1003:print]   weight_quantization_config ... None
[2025-01-07 20:19:16,607] [INFO] [config.py:1003:print]   world_size ................... 2
[2025-01-07 20:19:16,607] [INFO] [config.py:1003:print]   zero_allow_untested_optimizer  True
[2025-01-07 20:19:16,607] [INFO] [config.py:1003:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=16777216 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=15099494 param_persistence_threshold=40960 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=True use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2025-01-07 20:19:16,607] [INFO] [config.py:1003:print]   zero_enabled ................. True
[2025-01-07 20:19:16,608] [INFO] [config.py:1003:print]   zero_force_ds_cpu_optimizer .. True
[2025-01-07 20:19:16,608] [INFO] [config.py:1003:print]   zero_optimization_stage ...... 3
[2025-01-07 20:19:16,608] [INFO] [config.py:989:print_user_config]   json = {
    "bf16": {
        "enabled": true
    }, 
    "zero_optimization": {
        "stage": 3, 
        "overlap_comm": true, 
        "contiguous_gradients": true, 
        "sub_group_size": 1.000000e+09, 
        "reduce_bucket_size": 1.677722e+07, 
        "stage3_prefetch_bucket_size": 1.509949e+07, 
        "stage3_param_persistence_threshold": 4.096000e+04, 
        "stage3_max_live_parameters": 1.000000e+09, 
        "stage3_max_reuse_distance": 1.000000e+09, 
        "stage3_gather_16bit_weights_on_model_save": true
    }, 
    "gradient_accumulation_steps": 8, 
    "gradient_clipping": 1.0, 
    "steps_per_print": inf, 
    "train_batch_size": 16, 
    "train_micro_batch_size_per_gpu": 1, 
    "wall_clock_breakdown": false, 
    "fp16": {
        "enabled": false
    }, 
    "zero_allow_untested_optimizer": true, 
    "zero_optimization.reduce_bucket_size": 1.677722e+07, 
    "zero_optimization.stage3_param_persistence_threshold": 4.096000e+04, 
    "zero_optimization.stage3_prefetch_bucket_size": 1.509949e+07
}
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: wenvven (wenvven-university-of-stuttgart). Use `wandb login --relogin` to force relogin
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: wenvven (wenvven-university-of-stuttgart). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /workspace/open-instruct/wandb/run-20250107_201919-vujib8jk
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dpo_tune__42__1736281068
wandb: ⭐️ View project at https://wandb.ai/wenvven-university-of-stuttgart/open_instruct_internal
wandb: 🚀 View run at https://wandb.ai/wenvven-university-of-stuttgart/open_instruct_internal/runs/vujib8jk
INFO:__main__:Attaching masks to the model...
[Rank 0] Attached mask to module.model.layers.0.self_attn.q_proj:
  - Parameters: 16,777,216
  - Masked (zero) params: 8,388,608
  - Sparsity: 50.0%
[Rank 1] Attached mask to module.model.layers.0.self_attn.q_proj:
  - Parameters: 16,777,216
  - Masked (zero) params: 8,388,608
  - Sparsity: 50.0%
[Rank 0] Attached mask to module.model.layers.0.self_attn.k_proj:[Rank 1] Attached mask to module.model.layers.0.self_attn.k_proj:

  - Parameters: 4,194,304  - Parameters: 4,194,304

  - Masked (zero) params: 2,097,152
  - Masked (zero) params: 2,097,152  - Sparsity: 50.0%

  - Sparsity: 50.0%
[Rank 1] Attached mask to module.model.layers.0.self_attn.v_proj:
[Rank 0] Attached mask to module.model.layers.0.self_attn.v_proj:  - Parameters: 4,194,304

  - Masked (zero) params: 2,097,152
  - Parameters: 4,194,304  - Sparsity: 50.0%

  - Masked (zero) params: 2,097,152
  - Sparsity: 50.0%
[Rank 1] Attached mask to module.model.layers.0.self_attn.o_proj:
  - Parameters: 16,777,216[Rank 0] Attached mask to module.model.layers.0.self_attn.o_proj:
  - Masked (zero) params: 8,388,608

  - Sparsity: 50.0%  - Parameters: 16,777,216

  - Masked (zero) params: 8,388,608
  - Sparsity: 50.0%
[Rank 1] Attached mask to module.model.layers.0.mlp.gate_proj:
  - Parameters: 58,720,256
[Rank 0] Attached mask to module.model.layers.0.mlp.gate_proj:  - Masked (zero) params: 29,360,128
  - Sparsity: 50.0%

  - Parameters: 58,720,256
  - Masked (zero) params: 29,360,128
  - Sparsity: 50.0%
[Rank 0] Attached mask to module.model.layers.0.mlp.up_proj:[Rank 1] Attached mask to module.model.layers.0.mlp.up_proj:

  - Parameters: 58,720,256
  - Parameters: 58,720,256  - Masked (zero) params: 29,360,128

  - Sparsity: 50.0%
  - Masked (zero) params: 29,360,128
  - Sparsity: 50.0%
[Rank 0] Attached mask to module.model.layers.0.mlp.down_proj:[Rank 1] Attached mask to module.model.layers.0.mlp.down_proj:

  - Parameters: 58,720,256
  - Parameters: 58,720,256  - Masked (zero) params: 29,360,128

  - Sparsity: 50.0%
  - Masked (zero) params: 29,360,128
  - Sparsity: 50.0%
[Rank 0] Attached mask to module.model.layers.1.self_attn.q_proj:[Rank 1] Attached mask to module.model.layers.1.self_attn.q_proj:

  - Parameters: 16,777,216
  - Parameters: 16,777,216  - Masked (zero) params: 8,388,608

  - Sparsity: 50.0%
  - Masked (zero) params: 8,388,608
  - Sparsity: 50.0%
[Rank 1] Attached mask to module.model.layers.1.self_attn.k_proj:
[Rank 0] Attached mask to module.model.layers.1.self_attn.k_proj:  - Parameters: 4,194,304

  - Masked (zero) params: 2,097,152
  - Parameters: 4,194,304  - Sparsity: 50.0%

  - Masked (zero) params: 2,097,152
  - Sparsity: 50.0%
[Rank 1] Attached mask to module.model.layers.1.self_attn.v_proj:
[Rank 0] Attached mask to module.model.layers.1.self_attn.v_proj:  - Parameters: 4,194,304

  - Masked (zero) params: 2,097,152
  - Parameters: 4,194,304  - Sparsity: 50.0%

  - Masked (zero) params: 2,097,152
  - Sparsity: 50.0%
[Rank 0] Attached mask to module.model.layers.1.self_attn.o_proj:[Rank 1] Attached mask to module.model.layers.1.self_attn.o_proj:

  - Parameters: 16,777,216
  - Parameters: 16,777,216  - Masked (zero) params: 8,388,608

  - Sparsity: 50.0%
  - Masked (zero) params: 8,388,608
  - Sparsity: 50.0%
[Rank 0] Attached mask to module.model.layers.1.mlp.gate_proj:[Rank 1] Attached mask to module.model.layers.1.mlp.gate_proj:

  - Parameters: 58,720,256
  - Parameters: 58,720,256  - Masked (zero) params: 29,360,128

  - Sparsity: 50.0%
  - Masked (zero) params: 29,360,128
  - Sparsity: 50.0%
[Rank 0] Attached mask to module.model.layers.1.mlp.up_proj:[Rank 1] Attached mask to module.model.layers.1.mlp.up_proj:

  - Parameters: 58,720,256
  - Parameters: 58,720,256  - Masked (zero) params: 29,360,128

  - Sparsity: 50.0%
  - Masked (zero) params: 29,360,128
  - Sparsity: 50.0%
[Rank 0] Attached mask to module.model.layers.1.mlp.down_proj:
  - Parameters: 58,720,256[Rank 1] Attached mask to module.model.layers.1.mlp.down_proj:

  - Parameters: 58,720,256
  - Masked (zero) params: 29,360,128  - Masked (zero) params: 29,360,128

  - Sparsity: 50.0%
  - Sparsity: 50.0%
[Rank 0] Attached mask to module.model.layers.2.self_attn.q_proj:[Rank 1] Attached mask to module.model.layers.2.self_attn.q_proj:

  - Parameters: 16,777,216
  - Parameters: 16,777,216  - Masked (zero) params: 8,388,608

  - Sparsity: 50.0%
  - Masked (zero) params: 8,388,608
  - Sparsity: 50.0%
[Rank 1] Attached mask to module.model.layers.2.self_attn.k_proj:
[Rank 0] Attached mask to module.model.layers.2.self_attn.k_proj:  - Parameters: 4,194,304

  - Masked (zero) params: 2,097,152
  - Parameters: 4,194,304  - Sparsity: 50.0%

  - Masked (zero) params: 2,097,152
  - Sparsity: 50.0%
[Rank 1] Attached mask to module.model.layers.2.self_attn.v_proj:
[Rank 0] Attached mask to module.model.layers.2.self_attn.v_proj:  - Parameters: 4,194,304

  - Masked (zero) params: 2,097,152
  - Parameters: 4,194,304  - Sparsity: 50.0%

  - Masked (zero) params: 2,097,152
  - Sparsity: 50.0%
[Rank 1] Attached mask to module.model.layers.2.self_attn.o_proj:
  - Parameters: 16,777,216[Rank 0] Attached mask to module.model.layers.2.self_attn.o_proj:
  - Masked (zero) params: 8,388,608

  - Sparsity: 50.0%  - Parameters: 16,777,216

  - Masked (zero) params: 8,388,608
  - Sparsity: 50.0%
[Rank 0] Attached mask to module.model.layers.2.mlp.gate_proj:[Rank 1] Attached mask to module.model.layers.2.mlp.gate_proj:

  - Parameters: 58,720,256
  - Parameters: 58,720,256  - Masked (zero) params: 29,360,128

  - Sparsity: 50.0%
  - Masked (zero) params: 29,360,128
  - Sparsity: 50.0%
[Rank 0] Attached mask to module.model.layers.2.mlp.up_proj:[Rank 1] Attached mask to module.model.layers.2.mlp.up_proj:

  - Parameters: 58,720,256
  - Parameters: 58,720,256  - Masked (zero) params: 29,360,128

  - Sparsity: 50.0%
  - Masked (zero) params: 29,360,128
  - Sparsity: 50.0%
[Rank 0] Attached mask to module.model.layers.2.mlp.down_proj:[Rank 1] Attached mask to module.model.layers.2.mlp.down_proj:

  - Parameters: 58,720,256
  - Parameters: 58,720,256  - Masked (zero) params: 29,360,128

  - Sparsity: 50.0%
  - Masked (zero) params: 29,360,128
  - Sparsity: 50.0%
[Rank 1] Attached mask to module.model.layers.3.self_attn.q_proj:
[Rank 0] Attached mask to module.model.layers.3.self_attn.q_proj:  - Parameters: 16,777,216

  - Masked (zero) params: 8,388,608
  - Parameters: 16,777,216  - Sparsity: 50.0%

  - Masked (zero) params: 8,388,608
  - Sparsity: 50.0%
[Rank 1] Attached mask to module.model.layers.3.self_attn.k_proj:
[Rank 0] Attached mask to module.model.layers.3.self_attn.k_proj:  - Parameters: 4,194,304

  - Masked (zero) params: 2,097,152
  - Parameters: 4,194,304  - Sparsity: 50.0%

  - Masked (zero) params: 2,097,152
  - Sparsity: 50.0%
[Rank 1] Attached mask to module.model.layers.3.self_attn.v_proj:
[Rank 0] Attached mask to module.model.layers.3.self_attn.v_proj:  - Parameters: 4,194,304

  - Masked (zero) params: 2,097,152
  - Parameters: 4,194,304  - Sparsity: 50.0%

  - Masked (zero) params: 2,097,152
  - Sparsity: 50.0%
[Rank 1] Attached mask to module.model.layers.3.self_attn.o_proj:
[Rank 0] Attached mask to module.model.layers.3.self_attn.o_proj:  - Parameters: 16,777,216

  - Masked (zero) params: 8,388,608
  - Parameters: 16,777,216  - Sparsity: 50.0%

  - Masked (zero) params: 8,388,608
  - Sparsity: 50.0%
[Rank 0] Attached mask to module.model.layers.3.mlp.gate_proj:[Rank 1] Attached mask to module.model.layers.3.mlp.gate_proj:

  - Parameters: 58,720,256
  - Parameters: 58,720,256  - Masked (zero) params: 29,360,128

  - Sparsity: 50.0%
  - Masked (zero) params: 29,360,128
  - Sparsity: 50.0%
[Rank 0] Attached mask to module.model.layers.3.mlp.up_proj:[Rank 1] Attached mask to module.model.layers.3.mlp.up_proj:

  - Parameters: 58,720,256
  - Parameters: 58,720,256  - Masked (zero) params: 29,360,128

  - Sparsity: 50.0%
  - Masked (zero) params: 29,360,128
  - Sparsity: 50.0%
[Rank 0] Attached mask to module.model.layers.3.mlp.down_proj:[Rank 1] Attached mask to module.model.layers.3.mlp.down_proj:

  - Parameters: 58,720,256
  - Parameters: 58,720,256  - Masked (zero) params: 29,360,128

  - Sparsity: 50.0%
  - Masked (zero) params: 29,360,128
  - Sparsity: 50.0%
[Rank 1] Attached mask to module.model.layers.4.self_attn.q_proj:
  - Parameters: 16,777,216[Rank 0] Attached mask to module.model.layers.4.self_attn.q_proj:
  - Masked (zero) params: 8,388,608

  - Sparsity: 50.0%  - Parameters: 16,777,216

  - Masked (zero) params: 8,388,608
  - Sparsity: 50.0%
[Rank 1] Attached mask to module.model.layers.4.self_attn.k_proj:
[Rank 0] Attached mask to module.model.layers.4.self_attn.k_proj:  - Parameters: 4,194,304

  - Masked (zero) params: 2,097,152
  - Parameters: 4,194,304  - Sparsity: 50.0%

  - Masked (zero) params: 2,097,152
  - Sparsity: 50.0%
[Rank 1] Attached mask to module.model.layers.4.self_attn.v_proj:
[Rank 0] Attached mask to module.model.layers.4.self_attn.v_proj:  - Parameters: 4,194,304

  - Masked (zero) params: 2,097,152
  - Parameters: 4,194,304
  - Sparsity: 50.0%  - Masked (zero) params: 2,097,152

  - Sparsity: 50.0%
[Rank 0] Attached mask to module.model.layers.4.self_attn.o_proj:
[Rank 1] Attached mask to module.model.layers.4.self_attn.o_proj:
  - Parameters: 16,777,216  - Parameters: 16,777,216

  - Masked (zero) params: 8,388,608
  - Masked (zero) params: 8,388,608  - Sparsity: 50.0%

  - Sparsity: 50.0%
[Rank 1] Attached mask to module.model.layers.4.mlp.gate_proj:
[Rank 0] Attached mask to module.model.layers.4.mlp.gate_proj:  - Parameters: 58,720,256

  - Masked (zero) params: 29,360,128
  - Parameters: 58,720,256  - Sparsity: 50.0%

  - Masked (zero) params: 29,360,128
  - Sparsity: 50.0%
[Rank 0] Attached mask to module.model.layers.4.mlp.up_proj:[Rank 1] Attached mask to module.model.layers.4.mlp.up_proj:

  - Parameters: 58,720,256
  - Parameters: 58,720,256  - Masked (zero) params: 29,360,128

  - Sparsity: 50.0%
  - Masked (zero) params: 29,360,128
  - Sparsity: 50.0%
[Rank 0] Attached mask to module.model.layers.4.mlp.down_proj:[Rank 1] Attached mask to module.model.layers.4.mlp.down_proj:

  - Parameters: 58,720,256
  - Parameters: 58,720,256  - Masked (zero) params: 29,360,128

  - Sparsity: 50.0%
  - Masked (zero) params: 29,360,128
  - Sparsity: 50.0%
[Rank 1] Attached mask to module.model.layers.5.self_attn.q_proj:
  - Parameters: 16,777,216[Rank 0] Attached mask to module.model.layers.5.self_attn.q_proj:
  - Masked (zero) params: 8,388,608

  - Sparsity: 50.0%  - Parameters: 16,777,216

  - Masked (zero) params: 8,388,608
  - Sparsity: 50.0%
[Rank 1] Attached mask to module.model.layers.5.self_attn.k_proj:
[Rank 0] Attached mask to module.model.layers.5.self_attn.k_proj:  - Parameters: 4,194,304

  - Masked (zero) params: 2,097,152
  - Parameters: 4,194,304  - Sparsity: 50.0%

  - Masked (zero) params: 2,097,152
  - Sparsity: 50.0%
[Rank 1] Attached mask to module.model.layers.5.self_attn.v_proj:
[Rank 0] Attached mask to module.model.layers.5.self_attn.v_proj:  - Parameters: 4,194,304

  - Masked (zero) params: 2,097,152
  - Parameters: 4,194,304  - Sparsity: 50.0%

  - Masked (zero) params: 2,097,152
  - Sparsity: 50.0%
[Rank 1] Attached mask to module.model.layers.5.self_attn.o_proj:
[Rank 0] Attached mask to module.model.layers.5.self_attn.o_proj:  - Parameters: 16,777,216

  - Masked (zero) params: 8,388,608
  - Parameters: 16,777,216  - Sparsity: 50.0%

  - Masked (zero) params: 8,388,608
  - Sparsity: 50.0%
[Rank 0] Attached mask to module.model.layers.5.mlp.gate_proj:[Rank 1] Attached mask to module.model.layers.5.mlp.gate_proj:

  - Parameters: 58,720,256
  - Parameters: 58,720,256  - Masked (zero) params: 29,360,128

  - Sparsity: 50.0%
  - Masked (zero) params: 29,360,128
  - Sparsity: 50.0%
[Rank 0] Attached mask to module.model.layers.5.mlp.up_proj:[Rank 1] Attached mask to module.model.layers.5.mlp.up_proj:

  - Parameters: 58,720,256
  - Parameters: 58,720,256  - Masked (zero) params: 29,360,128

  - Sparsity: 50.0%
  - Masked (zero) params: 29,360,128
  - Sparsity: 50.0%
[Rank 0] Attached mask to module.model.layers.5.mlp.down_proj:[Rank 1] Attached mask to module.model.layers.5.mlp.down_proj:

  - Parameters: 58,720,256
  - Parameters: 58,720,256  - Masked (zero) params: 29,360,128

  - Sparsity: 50.0%
  - Masked (zero) params: 29,360,128
  - Sparsity: 50.0%
[Rank 1] Attached mask to module.model.layers.6.self_attn.q_proj:
  - Parameters: 16,777,216
[Rank 0] Attached mask to module.model.layers.6.self_attn.q_proj:  - Masked (zero) params: 8,388,608

  - Sparsity: 50.0%
  - Parameters: 16,777,216
  - Masked (zero) params: 8,388,608
  - Sparsity: 50.0%
[Rank 1] Attached mask to module.model.layers.6.self_attn.k_proj:
[Rank 0] Attached mask to module.model.layers.6.self_attn.k_proj:  - Parameters: 4,194,304

  - Masked (zero) params: 2,097,152
  - Parameters: 4,194,304  - Sparsity: 50.0%

  - Masked (zero) params: 2,097,152
  - Sparsity: 50.0%
[Rank 1] Attached mask to module.model.layers.6.self_attn.v_proj:
  - Parameters: 4,194,304[Rank 0] Attached mask to module.model.layers.6.self_attn.v_proj:
  - Masked (zero) params: 2,097,152

  - Sparsity: 50.0%  - Parameters: 4,194,304

  - Masked (zero) params: 2,097,152
  - Sparsity: 50.0%
[Rank 1] Attached mask to module.model.layers.6.self_attn.o_proj:
  - Parameters: 16,777,216[Rank 0] Attached mask to module.model.layers.6.self_attn.o_proj:
  - Masked (zero) params: 8,388,608

  - Sparsity: 50.0%  - Parameters: 16,777,216

  - Masked (zero) params: 8,388,608
  - Sparsity: 50.0%
[Rank 0] Attached mask to module.model.layers.6.mlp.gate_proj:[Rank 1] Attached mask to module.model.layers.6.mlp.gate_proj:

  - Parameters: 58,720,256
  - Parameters: 58,720,256  - Masked (zero) params: 29,360,128

  - Sparsity: 50.0%
  - Masked (zero) params: 29,360,128
  - Sparsity: 50.0%
[Rank 0] Attached mask to module.model.layers.6.mlp.up_proj:
  - Parameters: 58,720,256[Rank 1] Attached mask to module.model.layers.6.mlp.up_proj:

  - Parameters: 58,720,256
  - Masked (zero) params: 29,360,128  - Masked (zero) params: 29,360,128

  - Sparsity: 50.0%
  - Sparsity: 50.0%
[Rank 0] Attached mask to module.model.layers.6.mlp.down_proj:[Rank 1] Attached mask to module.model.layers.6.mlp.down_proj:

  - Parameters: 58,720,256
  - Parameters: 58,720,256  - Masked (zero) params: 29,360,128

  - Sparsity: 50.0%
  - Masked (zero) params: 29,360,128
  - Sparsity: 50.0%
[Rank 1] Attached mask to module.model.layers.7.self_attn.q_proj:
[Rank 0] Attached mask to module.model.layers.7.self_attn.q_proj:  - Parameters: 16,777,216

  - Masked (zero) params: 8,388,608
  - Parameters: 16,777,216  - Sparsity: 50.0%

  - Masked (zero) params: 8,388,608
  - Sparsity: 50.0%
[Rank 1] Attached mask to module.model.layers.7.self_attn.k_proj:
[Rank 0] Attached mask to module.model.layers.7.self_attn.k_proj:  - Parameters: 4,194,304

  - Masked (zero) params: 2,097,152
  - Parameters: 4,194,304  - Sparsity: 50.0%

  - Masked (zero) params: 2,097,152
  - Sparsity: 50.0%
[Rank 0] Attached mask to module.model.layers.7.self_attn.v_proj:[Rank 1] Attached mask to module.model.layers.7.self_attn.v_proj:

  - Parameters: 4,194,304
  - Parameters: 4,194,304  - Masked (zero) params: 2,097,152

  - Sparsity: 50.0%
  - Masked (zero) params: 2,097,152
  - Sparsity: 50.0%
[Rank 1] Attached mask to module.model.layers.7.self_attn.o_proj:
[Rank 0] Attached mask to module.model.layers.7.self_attn.o_proj:  - Parameters: 16,777,216

  - Masked (zero) params: 8,388,608
  - Parameters: 16,777,216  - Sparsity: 50.0%

  - Masked (zero) params: 8,388,608
  - Sparsity: 50.0%
[Rank 0] Attached mask to module.model.layers.7.mlp.gate_proj:[Rank 1] Attached mask to module.model.layers.7.mlp.gate_proj:

  - Parameters: 58,720,256
  - Parameters: 58,720,256  - Masked (zero) params: 29,360,128

  - Sparsity: 50.0%
  - Masked (zero) params: 29,360,128
  - Sparsity: 50.0%
[Rank 1] Attached mask to module.model.layers.7.mlp.up_proj:
[Rank 0] Attached mask to module.model.layers.7.mlp.up_proj:  - Parameters: 58,720,256

  - Masked (zero) params: 29,360,128
  - Parameters: 58,720,256  - Sparsity: 50.0%

  - Masked (zero) params: 29,360,128
  - Sparsity: 50.0%
[Rank 0] Attached mask to module.model.layers.7.mlp.down_proj:[Rank 1] Attached mask to module.model.layers.7.mlp.down_proj:

  - Parameters: 58,720,256
  - Parameters: 58,720,256  - Masked (zero) params: 29,360,128

  - Sparsity: 50.0%
  - Masked (zero) params: 29,360,128
  - Sparsity: 50.0%
[Rank 1] Attached mask to module.model.layers.8.self_attn.q_proj:
  - Parameters: 16,777,216[Rank 0] Attached mask to module.model.layers.8.self_attn.q_proj:
  - Masked (zero) params: 8,388,608

  - Sparsity: 50.0%  - Parameters: 16,777,216

  - Masked (zero) params: 8,388,608
  - Sparsity: 50.0%
[Rank 1] Attached mask to module.model.layers.8.self_attn.k_proj:
[Rank 0] Attached mask to module.model.layers.8.self_attn.k_proj:  - Parameters: 4,194,304

  - Masked (zero) params: 2,097,152
  - Parameters: 4,194,304  - Sparsity: 50.0%

  - Masked (zero) params: 2,097,152
  - Sparsity: 50.0%
[Rank 1] Attached mask to module.model.layers.8.self_attn.v_proj:
[Rank 0] Attached mask to module.model.layers.8.self_attn.v_proj:  - Parameters: 4,194,304

  - Masked (zero) params: 2,097,152
  - Parameters: 4,194,304  - Sparsity: 50.0%

  - Masked (zero) params: 2,097,152
  - Sparsity: 50.0%
[Rank 1] Attached mask to module.model.layers.8.self_attn.o_proj:
  - Parameters: 16,777,216
[Rank 0] Attached mask to module.model.layers.8.self_attn.o_proj:  - Masked (zero) params: 8,388,608

  - Sparsity: 50.0%
  - Parameters: 16,777,216
  - Masked (zero) params: 8,388,608
  - Sparsity: 50.0%
[Rank 0] Attached mask to module.model.layers.8.mlp.gate_proj:[Rank 1] Attached mask to module.model.layers.8.mlp.gate_proj:

  - Parameters: 58,720,256
  - Parameters: 58,720,256  - Masked (zero) params: 29,360,128

  - Sparsity: 50.0%
  - Masked (zero) params: 29,360,128
  - Sparsity: 50.0%
[Rank 0] Attached mask to module.model.layers.8.mlp.up_proj:[Rank 1] Attached mask to module.model.layers.8.mlp.up_proj:

  - Parameters: 58,720,256
  - Parameters: 58,720,256  - Masked (zero) params: 29,360,128

  - Sparsity: 50.0%
  - Masked (zero) params: 29,360,128
  - Sparsity: 50.0%
[Rank 0] Attached mask to module.model.layers.8.mlp.down_proj:[Rank 1] Attached mask to module.model.layers.8.mlp.down_proj:

  - Parameters: 58,720,256
  - Parameters: 58,720,256  - Masked (zero) params: 29,360,128

  - Sparsity: 50.0%
  - Masked (zero) params: 29,360,128
  - Sparsity: 50.0%
[Rank 1] Attached mask to module.model.layers.9.self_attn.q_proj:
[Rank 0] Attached mask to module.model.layers.9.self_attn.q_proj:  - Parameters: 16,777,216

  - Masked (zero) params: 8,388,608
  - Parameters: 16,777,216  - Sparsity: 50.0%

  - Masked (zero) params: 8,388,608
  - Sparsity: 50.0%
[Rank 1] Attached mask to module.model.layers.9.self_attn.k_proj:
[Rank 0] Attached mask to module.model.layers.9.self_attn.k_proj:  - Parameters: 4,194,304

  - Masked (zero) params: 2,097,152
  - Parameters: 4,194,304  - Sparsity: 50.0%

  - Masked (zero) params: 2,097,152
  - Sparsity: 50.0%
[Rank 1] Attached mask to module.model.layers.9.self_attn.v_proj:
[Rank 0] Attached mask to module.model.layers.9.self_attn.v_proj:  - Parameters: 4,194,304

  - Masked (zero) params: 2,097,152
  - Parameters: 4,194,304  - Sparsity: 50.0%

  - Masked (zero) params: 2,097,152
  - Sparsity: 50.0%
[Rank 1] Attached mask to module.model.layers.9.self_attn.o_proj:
[Rank 0] Attached mask to module.model.layers.9.self_attn.o_proj:  - Parameters: 16,777,216

  - Masked (zero) params: 8,388,608
  - Parameters: 16,777,216  - Sparsity: 50.0%

  - Masked (zero) params: 8,388,608
  - Sparsity: 50.0%
[Rank 0] Attached mask to module.model.layers.9.mlp.gate_proj:[Rank 1] Attached mask to module.model.layers.9.mlp.gate_proj:

  - Parameters: 58,720,256
  - Parameters: 58,720,256  - Masked (zero) params: 29,360,128

  - Sparsity: 50.0%
  - Masked (zero) params: 29,360,128
  - Sparsity: 50.0%
[Rank 0] Attached mask to module.model.layers.9.mlp.up_proj:[Rank 1] Attached mask to module.model.layers.9.mlp.up_proj:

  - Parameters: 58,720,256
  - Parameters: 58,720,256  - Masked (zero) params: 29,360,128

  - Sparsity: 50.0%
  - Masked (zero) params: 29,360,128
  - Sparsity: 50.0%
[Rank 0] Attached mask to module.model.layers.9.mlp.down_proj:[Rank 1] Attached mask to module.model.layers.9.mlp.down_proj:

  - Parameters: 58,720,256
  - Parameters: 58,720,256  - Masked (zero) params: 29,360,128

  - Sparsity: 50.0%
  - Masked (zero) params: 29,360,128
  - Sparsity: 50.0%
[Rank 1] Attached mask to module.model.layers.10.self_attn.q_proj:
  - Parameters: 16,777,216[Rank 0] Attached mask to module.model.layers.10.self_attn.q_proj:
  - Masked (zero) params: 8,388,608

  - Sparsity: 50.0%  - Parameters: 16,777,216

  - Masked (zero) params: 8,388,608
  - Sparsity: 50.0%
[Rank 1] Attached mask to module.model.layers.10.self_attn.k_proj:
[Rank 0] Attached mask to module.model.layers.10.self_attn.k_proj:  - Parameters: 4,194,304

  - Masked (zero) params: 2,097,152
  - Parameters: 4,194,304  - Sparsity: 50.0%

  - Masked (zero) params: 2,097,152
  - Sparsity: 50.0%
[Rank 1] Attached mask to module.model.layers.10.self_attn.v_proj:
[Rank 0] Attached mask to module.model.layers.10.self_attn.v_proj:  - Parameters: 4,194,304

  - Masked (zero) params: 2,097,152
  - Parameters: 4,194,304  - Sparsity: 50.0%

  - Masked (zero) params: 2,097,152
  - Sparsity: 50.0%
[Rank 1] Attached mask to module.model.layers.10.self_attn.o_proj:
[Rank 0] Attached mask to module.model.layers.10.self_attn.o_proj:  - Parameters: 16,777,216

  - Masked (zero) params: 8,388,608
  - Parameters: 16,777,216  - Sparsity: 50.0%

  - Masked (zero) params: 8,388,608
  - Sparsity: 50.0%
[Rank 0] Attached mask to module.model.layers.10.mlp.gate_proj:[Rank 1] Attached mask to module.model.layers.10.mlp.gate_proj:

  - Parameters: 58,720,256
  - Parameters: 58,720,256  - Masked (zero) params: 29,360,128

  - Sparsity: 50.0%
  - Masked (zero) params: 29,360,128
  - Sparsity: 50.0%
[Rank 0] Attached mask to module.model.layers.10.mlp.up_proj:[Rank 1] Attached mask to module.model.layers.10.mlp.up_proj:

  - Parameters: 58,720,256
  - Parameters: 58,720,256  - Masked (zero) params: 29,360,128

  - Sparsity: 50.0%
  - Masked (zero) params: 29,360,128
  - Sparsity: 50.0%
[Rank 0] Attached mask to module.model.layers.10.mlp.down_proj:[Rank 1] Attached mask to module.model.layers.10.mlp.down_proj:

  - Parameters: 58,720,256
  - Parameters: 58,720,256  - Masked (zero) params: 29,360,128

  - Sparsity: 50.0%
  - Masked (zero) params: 29,360,128
  - Sparsity: 50.0%
[Rank 0] Attached mask to module.model.layers.11.self_attn.q_proj:[Rank 1] Attached mask to module.model.layers.11.self_attn.q_proj:

  - Parameters: 16,777,216
  - Parameters: 16,777,216  - Masked (zero) params: 8,388,608

  - Sparsity: 50.0%
  - Masked (zero) params: 8,388,608
  - Sparsity: 50.0%
[Rank 1] Attached mask to module.model.layers.11.self_attn.k_proj:
[Rank 0] Attached mask to module.model.layers.11.self_attn.k_proj:  - Parameters: 4,194,304

  - Masked (zero) params: 2,097,152
  - Parameters: 4,194,304  - Sparsity: 50.0%

  - Masked (zero) params: 2,097,152
  - Sparsity: 50.0%
[Rank 1] Attached mask to module.model.layers.11.self_attn.v_proj:
[Rank 0] Attached mask to module.model.layers.11.self_attn.v_proj:  - Parameters: 4,194,304

  - Masked (zero) params: 2,097,152
  - Parameters: 4,194,304  - Sparsity: 50.0%

  - Masked (zero) params: 2,097,152
  - Sparsity: 50.0%
[Rank 1] Attached mask to module.model.layers.11.self_attn.o_proj:
[Rank 0] Attached mask to module.model.layers.11.self_attn.o_proj:  - Parameters: 16,777,216

  - Masked (zero) params: 8,388,608
  - Parameters: 16,777,216  - Sparsity: 50.0%

  - Masked (zero) params: 8,388,608
  - Sparsity: 50.0%
[Rank 0] Attached mask to module.model.layers.11.mlp.gate_proj:[Rank 1] Attached mask to module.model.layers.11.mlp.gate_proj:

  - Parameters: 58,720,256
  - Parameters: 58,720,256  - Masked (zero) params: 29,360,128

  - Sparsity: 50.0%
  - Masked (zero) params: 29,360,128
  - Sparsity: 50.0%
[Rank 0] Attached mask to module.model.layers.11.mlp.up_proj:[Rank 1] Attached mask to module.model.layers.11.mlp.up_proj:

  - Parameters: 58,720,256
  - Parameters: 58,720,256  - Masked (zero) params: 29,360,128

  - Sparsity: 50.0%
  - Masked (zero) params: 29,360,128
  - Sparsity: 50.0%
[Rank 0] Attached mask to module.model.layers.11.mlp.down_proj:[Rank 1] Attached mask to module.model.layers.11.mlp.down_proj:

  - Parameters: 58,720,256
  - Parameters: 58,720,256  - Masked (zero) params: 29,360,128

  - Sparsity: 50.0%
  - Masked (zero) params: 29,360,128
  - Sparsity: 50.0%
[Rank 1] Attached mask to module.model.layers.12.self_attn.q_proj:
  - Parameters: 16,777,216[Rank 0] Attached mask to module.model.layers.12.self_attn.q_proj:
  - Masked (zero) params: 8,388,608

  - Sparsity: 50.0%  - Parameters: 16,777,216

  - Masked (zero) params: 8,388,608
  - Sparsity: 50.0%
[Rank 1] Attached mask to module.model.layers.12.self_attn.k_proj:
[Rank 0] Attached mask to module.model.layers.12.self_attn.k_proj:  - Parameters: 4,194,304

  - Masked (zero) params: 2,097,152
  - Parameters: 4,194,304  - Sparsity: 50.0%

  - Masked (zero) params: 2,097,152
  - Sparsity: 50.0%
[Rank 1] Attached mask to module.model.layers.12.self_attn.v_proj:
[Rank 0] Attached mask to module.model.layers.12.self_attn.v_proj:  - Parameters: 4,194,304

  - Masked (zero) params: 2,097,152
  - Parameters: 4,194,304  - Sparsity: 50.0%

  - Masked (zero) params: 2,097,152
  - Sparsity: 50.0%
[Rank 1] Attached mask to module.model.layers.12.self_attn.o_proj:
  - Parameters: 16,777,216[Rank 0] Attached mask to module.model.layers.12.self_attn.o_proj:
  - Masked (zero) params: 8,388,608

  - Sparsity: 50.0%
  - Parameters: 16,777,216
  - Masked (zero) params: 8,388,608
  - Sparsity: 50.0%
[Rank 0] Attached mask to module.model.layers.12.mlp.gate_proj:[Rank 1] Attached mask to module.model.layers.12.mlp.gate_proj:

  - Parameters: 58,720,256
  - Parameters: 58,720,256  - Masked (zero) params: 29,360,128

  - Sparsity: 50.0%
  - Masked (zero) params: 29,360,128
  - Sparsity: 50.0%
[Rank 0] Attached mask to module.model.layers.12.mlp.up_proj:[Rank 1] Attached mask to module.model.layers.12.mlp.up_proj:

  - Parameters: 58,720,256
  - Parameters: 58,720,256  - Masked (zero) params: 29,360,128

  - Sparsity: 50.0%
  - Masked (zero) params: 29,360,128
  - Sparsity: 50.0%
[Rank 0] Attached mask to module.model.layers.12.mlp.down_proj:[Rank 1] Attached mask to module.model.layers.12.mlp.down_proj:

  - Parameters: 58,720,256
  - Parameters: 58,720,256  - Masked (zero) params: 29,360,128

  - Sparsity: 50.0%
  - Masked (zero) params: 29,360,128
  - Sparsity: 50.0%
[Rank 1] Attached mask to module.model.layers.13.self_attn.q_proj:
[Rank 0] Attached mask to module.model.layers.13.self_attn.q_proj:  - Parameters: 16,777,216

  - Masked (zero) params: 8,388,608
  - Parameters: 16,777,216  - Sparsity: 50.0%

  - Masked (zero) params: 8,388,608
  - Sparsity: 50.0%
[Rank 1] Attached mask to module.model.layers.13.self_attn.k_proj:
[Rank 0] Attached mask to module.model.layers.13.self_attn.k_proj:  - Parameters: 4,194,304

  - Masked (zero) params: 2,097,152
  - Parameters: 4,194,304  - Sparsity: 50.0%

  - Masked (zero) params: 2,097,152
  - Sparsity: 50.0%
[Rank 1] Attached mask to module.model.layers.13.self_attn.v_proj:
[Rank 0] Attached mask to module.model.layers.13.self_attn.v_proj:  - Parameters: 4,194,304

  - Masked (zero) params: 2,097,152
  - Parameters: 4,194,304  - Sparsity: 50.0%

  - Masked (zero) params: 2,097,152
  - Sparsity: 50.0%
[Rank 1] Attached mask to module.model.layers.13.self_attn.o_proj:
[Rank 0] Attached mask to module.model.layers.13.self_attn.o_proj:  - Parameters: 16,777,216

  - Masked (zero) params: 8,388,608
  - Parameters: 16,777,216  - Sparsity: 50.0%

  - Masked (zero) params: 8,388,608
  - Sparsity: 50.0%
[Rank 0] Attached mask to module.model.layers.13.mlp.gate_proj:[Rank 1] Attached mask to module.model.layers.13.mlp.gate_proj:

  - Parameters: 58,720,256
  - Parameters: 58,720,256  - Masked (zero) params: 29,360,128

  - Sparsity: 50.0%
  - Masked (zero) params: 29,360,128
  - Sparsity: 50.0%
[Rank 0] Attached mask to module.model.layers.13.mlp.up_proj:[Rank 1] Attached mask to module.model.layers.13.mlp.up_proj:

  - Parameters: 58,720,256
  - Parameters: 58,720,256  - Masked (zero) params: 29,360,128

  - Sparsity: 50.0%
  - Masked (zero) params: 29,360,128
  - Sparsity: 50.0%
[Rank 0] Attached mask to module.model.layers.13.mlp.down_proj:[Rank 1] Attached mask to module.model.layers.13.mlp.down_proj:

  - Parameters: 58,720,256
  - Parameters: 58,720,256  - Masked (zero) params: 29,360,128

  - Sparsity: 50.0%
  - Masked (zero) params: 29,360,128
  - Sparsity: 50.0%
[Rank 1] Attached mask to module.model.layers.14.self_attn.q_proj:
  - Parameters: 16,777,216[Rank 0] Attached mask to module.model.layers.14.self_attn.q_proj:
  - Masked (zero) params: 8,388,608

  - Sparsity: 50.0%  - Parameters: 16,777,216

  - Masked (zero) params: 8,388,608
  - Sparsity: 50.0%
[Rank 1] Attached mask to module.model.layers.14.self_attn.k_proj:
[Rank 0] Attached mask to module.model.layers.14.self_attn.k_proj:  - Parameters: 4,194,304

  - Masked (zero) params: 2,097,152
  - Parameters: 4,194,304  - Sparsity: 50.0%

  - Masked (zero) params: 2,097,152
  - Sparsity: 50.0%
[Rank 1] Attached mask to module.model.layers.14.self_attn.v_proj:
[Rank 0] Attached mask to module.model.layers.14.self_attn.v_proj:  - Parameters: 4,194,304

  - Masked (zero) params: 2,097,152
  - Parameters: 4,194,304  - Sparsity: 50.0%

  - Masked (zero) params: 2,097,152
  - Sparsity: 50.0%
[Rank 1] Attached mask to module.model.layers.14.self_attn.o_proj:
  - Parameters: 16,777,216[Rank 0] Attached mask to module.model.layers.14.self_attn.o_proj:
  - Masked (zero) params: 8,388,608

  - Sparsity: 50.0%  - Parameters: 16,777,216

  - Masked (zero) params: 8,388,608
  - Sparsity: 50.0%
[Rank 0] Attached mask to module.model.layers.14.mlp.gate_proj:[Rank 1] Attached mask to module.model.layers.14.mlp.gate_proj:

  - Parameters: 58,720,256
  - Parameters: 58,720,256  - Masked (zero) params: 29,360,128

  - Sparsity: 50.0%
  - Masked (zero) params: 29,360,128
  - Sparsity: 50.0%
[Rank 0] Attached mask to module.model.layers.14.mlp.up_proj:
[Rank 1] Attached mask to module.model.layers.14.mlp.up_proj:
  - Parameters: 58,720,256  - Parameters: 58,720,256

  - Masked (zero) params: 29,360,128
  - Masked (zero) params: 29,360,128  - Sparsity: 50.0%

  - Sparsity: 50.0%
[Rank 0] Attached mask to module.model.layers.14.mlp.down_proj:[Rank 1] Attached mask to module.model.layers.14.mlp.down_proj:

  - Parameters: 58,720,256
  - Parameters: 58,720,256  - Masked (zero) params: 29,360,128

  - Sparsity: 50.0%
  - Masked (zero) params: 29,360,128
  - Sparsity: 50.0%
[Rank 1] Attached mask to module.model.layers.15.self_attn.q_proj:
[Rank 0] Attached mask to module.model.layers.15.self_attn.q_proj:  - Parameters: 16,777,216

  - Masked (zero) params: 8,388,608
  - Parameters: 16,777,216  - Sparsity: 50.0%

  - Masked (zero) params: 8,388,608
  - Sparsity: 50.0%
[Rank 1] Attached mask to module.model.layers.15.self_attn.k_proj:
[Rank 0] Attached mask to module.model.layers.15.self_attn.k_proj:  - Parameters: 4,194,304

  - Masked (zero) params: 2,097,152
  - Parameters: 4,194,304  - Sparsity: 50.0%

  - Masked (zero) params: 2,097,152
  - Sparsity: 50.0%
[Rank 1] Attached mask to module.model.layers.15.self_attn.v_proj:
[Rank 0] Attached mask to module.model.layers.15.self_attn.v_proj:  - Parameters: 4,194,304

  - Masked (zero) params: 2,097,152
  - Parameters: 4,194,304  - Sparsity: 50.0%

  - Masked (zero) params: 2,097,152
  - Sparsity: 50.0%
[Rank 1] Attached mask to module.model.layers.15.self_attn.o_proj:
[Rank 0] Attached mask to module.model.layers.15.self_attn.o_proj:  - Parameters: 16,777,216

  - Masked (zero) params: 8,388,608
  - Parameters: 16,777,216  - Sparsity: 50.0%

  - Masked (zero) params: 8,388,608
  - Sparsity: 50.0%
[Rank 0] Attached mask to module.model.layers.15.mlp.gate_proj:[Rank 1] Attached mask to module.model.layers.15.mlp.gate_proj:

  - Parameters: 58,720,256
  - Parameters: 58,720,256  - Masked (zero) params: 29,360,128

  - Sparsity: 50.0%
  - Masked (zero) params: 29,360,128
  - Sparsity: 50.0%
[Rank 0] Attached mask to module.model.layers.15.mlp.up_proj:[Rank 1] Attached mask to module.model.layers.15.mlp.up_proj:

  - Parameters: 58,720,256
  - Parameters: 58,720,256  - Masked (zero) params: 29,360,128

  - Sparsity: 50.0%
  - Masked (zero) params: 29,360,128
  - Sparsity: 50.0%
[Rank 1] Attached mask to module.model.layers.15.mlp.down_proj:
[Rank 0] Attached mask to module.model.layers.15.mlp.down_proj:  - Parameters: 58,720,256

  - Masked (zero) params: 29,360,128
  - Parameters: 58,720,256  - Sparsity: 50.0%

  - Masked (zero) params: 29,360,128
  - Sparsity: 50.0%
[Rank 1] Attached mask to module.model.layers.16.self_attn.q_proj:
  - Parameters: 16,777,216[Rank 0] Attached mask to module.model.layers.16.self_attn.q_proj:
  - Masked (zero) params: 8,388,608

  - Sparsity: 50.0%  - Parameters: 16,777,216

  - Masked (zero) params: 8,388,608
  - Sparsity: 50.0%
[Rank 1] Attached mask to module.model.layers.16.self_attn.k_proj:
[Rank 0] Attached mask to module.model.layers.16.self_attn.k_proj:  - Parameters: 4,194,304

  - Masked (zero) params: 2,097,152
  - Parameters: 4,194,304  - Sparsity: 50.0%

  - Masked (zero) params: 2,097,152
  - Sparsity: 50.0%
[Rank 1] Attached mask to module.model.layers.16.self_attn.v_proj:
[Rank 0] Attached mask to module.model.layers.16.self_attn.v_proj:  - Parameters: 4,194,304

  - Masked (zero) params: 2,097,152
  - Parameters: 4,194,304  - Sparsity: 50.0%

  - Masked (zero) params: 2,097,152
  - Sparsity: 50.0%
[Rank 1] Attached mask to module.model.layers.16.self_attn.o_proj:
  - Parameters: 16,777,216[Rank 0] Attached mask to module.model.layers.16.self_attn.o_proj:
  - Masked (zero) params: 8,388,608

  - Sparsity: 50.0%  - Parameters: 16,777,216

  - Masked (zero) params: 8,388,608
  - Sparsity: 50.0%
[Rank 0] Attached mask to module.model.layers.16.mlp.gate_proj:[Rank 1] Attached mask to module.model.layers.16.mlp.gate_proj:

  - Parameters: 58,720,256
  - Parameters: 58,720,256  - Masked (zero) params: 29,360,128

  - Sparsity: 50.0%
  - Masked (zero) params: 29,360,128
  - Sparsity: 50.0%
[Rank 0] Attached mask to module.model.layers.16.mlp.up_proj:[Rank 1] Attached mask to module.model.layers.16.mlp.up_proj:

  - Parameters: 58,720,256
  - Parameters: 58,720,256  - Masked (zero) params: 29,360,128

  - Sparsity: 50.0%
  - Masked (zero) params: 29,360,128
  - Sparsity: 50.0%
[Rank 0] Attached mask to module.model.layers.16.mlp.down_proj:[Rank 1] Attached mask to module.model.layers.16.mlp.down_proj:

  - Parameters: 58,720,256
  - Parameters: 58,720,256  - Masked (zero) params: 29,360,128

  - Sparsity: 50.0%
  - Masked (zero) params: 29,360,128
  - Sparsity: 50.0%
[Rank 0] Attached mask to module.model.layers.17.self_attn.q_proj:[Rank 1] Attached mask to module.model.layers.17.self_attn.q_proj:

  - Parameters: 16,777,216
  - Parameters: 16,777,216  - Masked (zero) params: 8,388,608

  - Sparsity: 50.0%
  - Masked (zero) params: 8,388,608
  - Sparsity: 50.0%
[Rank 1] Attached mask to module.model.layers.17.self_attn.k_proj:
[Rank 0] Attached mask to module.model.layers.17.self_attn.k_proj:  - Parameters: 4,194,304

  - Masked (zero) params: 2,097,152
  - Parameters: 4,194,304  - Sparsity: 50.0%

  - Masked (zero) params: 2,097,152
  - Sparsity: 50.0%
[Rank 1] Attached mask to module.model.layers.17.self_attn.v_proj:
[Rank 0] Attached mask to module.model.layers.17.self_attn.v_proj:  - Parameters: 4,194,304

  - Masked (zero) params: 2,097,152
  - Parameters: 4,194,304  - Sparsity: 50.0%

  - Masked (zero) params: 2,097,152
  - Sparsity: 50.0%
[Rank 1] Attached mask to module.model.layers.17.self_attn.o_proj:
[Rank 0] Attached mask to module.model.layers.17.self_attn.o_proj:  - Parameters: 16,777,216

  - Masked (zero) params: 8,388,608
  - Parameters: 16,777,216  - Sparsity: 50.0%

  - Masked (zero) params: 8,388,608
  - Sparsity: 50.0%
[Rank 0] Attached mask to module.model.layers.17.mlp.gate_proj:[Rank 1] Attached mask to module.model.layers.17.mlp.gate_proj:

  - Parameters: 58,720,256
  - Parameters: 58,720,256  - Masked (zero) params: 29,360,128

  - Sparsity: 50.0%
  - Masked (zero) params: 29,360,128
  - Sparsity: 50.0%
[Rank 0] Attached mask to module.model.layers.17.mlp.up_proj:[Rank 1] Attached mask to module.model.layers.17.mlp.up_proj:

  - Parameters: 58,720,256
  - Parameters: 58,720,256  - Masked (zero) params: 29,360,128

  - Sparsity: 50.0%
  - Masked (zero) params: 29,360,128
  - Sparsity: 50.0%
[Rank 0] Attached mask to module.model.layers.17.mlp.down_proj:[Rank 1] Attached mask to module.model.layers.17.mlp.down_proj:

  - Parameters: 58,720,256
  - Parameters: 58,720,256  - Masked (zero) params: 29,360,128

  - Sparsity: 50.0%
  - Masked (zero) params: 29,360,128
  - Sparsity: 50.0%
[Rank 1] Attached mask to module.model.layers.18.self_attn.q_proj:
  - Parameters: 16,777,216[Rank 0] Attached mask to module.model.layers.18.self_attn.q_proj:
  - Masked (zero) params: 8,388,608

  - Sparsity: 50.0%  - Parameters: 16,777,216

  - Masked (zero) params: 8,388,608
  - Sparsity: 50.0%
[Rank 1] Attached mask to module.model.layers.18.self_attn.k_proj:
[Rank 0] Attached mask to module.model.layers.18.self_attn.k_proj:  - Parameters: 4,194,304

  - Masked (zero) params: 2,097,152
  - Parameters: 4,194,304  - Sparsity: 50.0%

  - Masked (zero) params: 2,097,152
  - Sparsity: 50.0%
[Rank 1] Attached mask to module.model.layers.18.self_attn.v_proj:
[Rank 0] Attached mask to module.model.layers.18.self_attn.v_proj:  - Parameters: 4,194,304

  - Masked (zero) params: 2,097,152
  - Parameters: 4,194,304  - Sparsity: 50.0%

  - Masked (zero) params: 2,097,152
  - Sparsity: 50.0%
[Rank 1] Attached mask to module.model.layers.18.self_attn.o_proj:
  - Parameters: 16,777,216[Rank 0] Attached mask to module.model.layers.18.self_attn.o_proj:
  - Masked (zero) params: 8,388,608

  - Sparsity: 50.0%  - Parameters: 16,777,216

  - Masked (zero) params: 8,388,608
  - Sparsity: 50.0%
[Rank 0] Attached mask to module.model.layers.18.mlp.gate_proj:
[Rank 1] Attached mask to module.model.layers.18.mlp.gate_proj:
  - Parameters: 58,720,256  - Parameters: 58,720,256

  - Masked (zero) params: 29,360,128
  - Masked (zero) params: 29,360,128  - Sparsity: 50.0%

  - Sparsity: 50.0%
[Rank 0] Attached mask to module.model.layers.18.mlp.up_proj:[Rank 1] Attached mask to module.model.layers.18.mlp.up_proj:

  - Parameters: 58,720,256
  - Parameters: 58,720,256  - Masked (zero) params: 29,360,128

  - Sparsity: 50.0%
  - Masked (zero) params: 29,360,128
  - Sparsity: 50.0%
[Rank 0] Attached mask to module.model.layers.18.mlp.down_proj:[Rank 1] Attached mask to module.model.layers.18.mlp.down_proj:

  - Parameters: 58,720,256
  - Parameters: 58,720,256  - Masked (zero) params: 29,360,128

  - Sparsity: 50.0%
  - Masked (zero) params: 29,360,128
  - Sparsity: 50.0%
[Rank 1] Attached mask to module.model.layers.19.self_attn.q_proj:
[Rank 0] Attached mask to module.model.layers.19.self_attn.q_proj:  - Parameters: 16,777,216

  - Masked (zero) params: 8,388,608
  - Parameters: 16,777,216  - Sparsity: 50.0%

  - Masked (zero) params: 8,388,608
  - Sparsity: 50.0%
[Rank 1] Attached mask to module.model.layers.19.self_attn.k_proj:
[Rank 0] Attached mask to module.model.layers.19.self_attn.k_proj:  - Parameters: 4,194,304

  - Masked (zero) params: 2,097,152
  - Parameters: 4,194,304  - Sparsity: 50.0%

  - Masked (zero) params: 2,097,152
  - Sparsity: 50.0%
[Rank 1] Attached mask to module.model.layers.19.self_attn.v_proj:
[Rank 0] Attached mask to module.model.layers.19.self_attn.v_proj:  - Parameters: 4,194,304

  - Masked (zero) params: 2,097,152
  - Parameters: 4,194,304  - Sparsity: 50.0%

  - Masked (zero) params: 2,097,152
  - Sparsity: 50.0%
[Rank 0] Attached mask to module.model.layers.19.self_attn.o_proj:[Rank 1] Attached mask to module.model.layers.19.self_attn.o_proj:

  - Parameters: 16,777,216
  - Parameters: 16,777,216  - Masked (zero) params: 8,388,608

  - Sparsity: 50.0%
  - Masked (zero) params: 8,388,608
  - Sparsity: 50.0%
[Rank 0] Attached mask to module.model.layers.19.mlp.gate_proj:[Rank 1] Attached mask to module.model.layers.19.mlp.gate_proj:

  - Parameters: 58,720,256
  - Parameters: 58,720,256  - Masked (zero) params: 29,360,128

  - Sparsity: 50.0%
  - Masked (zero) params: 29,360,128
  - Sparsity: 50.0%
[Rank 0] Attached mask to module.model.layers.19.mlp.up_proj:[Rank 1] Attached mask to module.model.layers.19.mlp.up_proj:

  - Parameters: 58,720,256
  - Parameters: 58,720,256  - Masked (zero) params: 29,360,128

  - Sparsity: 50.0%
  - Masked (zero) params: 29,360,128
  - Sparsity: 50.0%
[Rank 0] Attached mask to module.model.layers.19.mlp.down_proj:[Rank 1] Attached mask to module.model.layers.19.mlp.down_proj:

  - Parameters: 58,720,256
  - Parameters: 58,720,256  - Masked (zero) params: 29,360,128

  - Sparsity: 50.0%
  - Masked (zero) params: 29,360,128
  - Sparsity: 50.0%
[Rank 1] Attached mask to module.model.layers.20.self_attn.q_proj:
  - Parameters: 16,777,216[Rank 0] Attached mask to module.model.layers.20.self_attn.q_proj:
  - Masked (zero) params: 8,388,608

  - Sparsity: 50.0%  - Parameters: 16,777,216

  - Masked (zero) params: 8,388,608
  - Sparsity: 50.0%
[Rank 1] Attached mask to module.model.layers.20.self_attn.k_proj:
[Rank 0] Attached mask to module.model.layers.20.self_attn.k_proj:  - Parameters: 4,194,304

  - Masked (zero) params: 2,097,152
  - Parameters: 4,194,304  - Sparsity: 50.0%

  - Masked (zero) params: 2,097,152
  - Sparsity: 50.0%
[Rank 1] Attached mask to module.model.layers.20.self_attn.v_proj:
[Rank 0] Attached mask to module.model.layers.20.self_attn.v_proj:  - Parameters: 4,194,304

  - Masked (zero) params: 2,097,152
  - Parameters: 4,194,304  - Sparsity: 50.0%

  - Masked (zero) params: 2,097,152
  - Sparsity: 50.0%
[Rank 1] Attached mask to module.model.layers.20.self_attn.o_proj:
[Rank 0] Attached mask to module.model.layers.20.self_attn.o_proj:  - Parameters: 16,777,216

  - Masked (zero) params: 8,388,608
  - Parameters: 16,777,216  - Sparsity: 50.0%

  - Masked (zero) params: 8,388,608
  - Sparsity: 50.0%
[Rank 0] Attached mask to module.model.layers.20.mlp.gate_proj:[Rank 1] Attached mask to module.model.layers.20.mlp.gate_proj:

  - Parameters: 58,720,256
  - Parameters: 58,720,256  - Masked (zero) params: 29,360,128

  - Sparsity: 50.0%
  - Masked (zero) params: 29,360,128
  - Sparsity: 50.0%
[Rank 0] Attached mask to module.model.layers.20.mlp.up_proj:[Rank 1] Attached mask to module.model.layers.20.mlp.up_proj:

  - Parameters: 58,720,256
  - Parameters: 58,720,256  - Masked (zero) params: 29,360,128

  - Sparsity: 50.0%
  - Masked (zero) params: 29,360,128
  - Sparsity: 50.0%
[Rank 0] Attached mask to module.model.layers.20.mlp.down_proj:[Rank 1] Attached mask to module.model.layers.20.mlp.down_proj:

  - Parameters: 58,720,256
  - Parameters: 58,720,256  - Masked (zero) params: 29,360,128

  - Sparsity: 50.0%
  - Masked (zero) params: 29,360,128
  - Sparsity: 50.0%
[Rank 0] Attached mask to module.model.layers.21.self_attn.q_proj:[Rank 1] Attached mask to module.model.layers.21.self_attn.q_proj:

  - Parameters: 16,777,216
  - Parameters: 16,777,216  - Masked (zero) params: 8,388,608

  - Sparsity: 50.0%
  - Masked (zero) params: 8,388,608
  - Sparsity: 50.0%
[Rank 1] Attached mask to module.model.layers.21.self_attn.k_proj:
[Rank 0] Attached mask to module.model.layers.21.self_attn.k_proj:  - Parameters: 4,194,304

  - Masked (zero) params: 2,097,152
  - Parameters: 4,194,304  - Sparsity: 50.0%

  - Masked (zero) params: 2,097,152
  - Sparsity: 50.0%
[Rank 1] Attached mask to module.model.layers.21.self_attn.v_proj:
[Rank 0] Attached mask to module.model.layers.21.self_attn.v_proj:  - Parameters: 4,194,304

  - Masked (zero) params: 2,097,152
  - Parameters: 4,194,304  - Sparsity: 50.0%

  - Masked (zero) params: 2,097,152
  - Sparsity: 50.0%
[Rank 1] Attached mask to module.model.layers.21.self_attn.o_proj:
[Rank 0] Attached mask to module.model.layers.21.self_attn.o_proj:  - Parameters: 16,777,216

  - Masked (zero) params: 8,388,608
  - Parameters: 16,777,216  - Sparsity: 50.0%

  - Masked (zero) params: 8,388,608
  - Sparsity: 50.0%
[Rank 0] Attached mask to module.model.layers.21.mlp.gate_proj:[Rank 1] Attached mask to module.model.layers.21.mlp.gate_proj:

  - Parameters: 58,720,256
  - Parameters: 58,720,256  - Masked (zero) params: 29,360,128

  - Sparsity: 50.0%
  - Masked (zero) params: 29,360,128
  - Sparsity: 50.0%
[Rank 0] Attached mask to module.model.layers.21.mlp.up_proj:[Rank 1] Attached mask to module.model.layers.21.mlp.up_proj:

  - Parameters: 58,720,256
  - Parameters: 58,720,256  - Masked (zero) params: 29,360,128

  - Sparsity: 50.0%
  - Masked (zero) params: 29,360,128
  - Sparsity: 50.0%
[Rank 0] Attached mask to module.model.layers.21.mlp.down_proj:[Rank 1] Attached mask to module.model.layers.21.mlp.down_proj:

  - Parameters: 58,720,256
  - Parameters: 58,720,256  - Masked (zero) params: 29,360,128

  - Sparsity: 50.0%
  - Masked (zero) params: 29,360,128
  - Sparsity: 50.0%
[Rank 1] Attached mask to module.model.layers.22.self_attn.q_proj:
  - Parameters: 16,777,216[Rank 0] Attached mask to module.model.layers.22.self_attn.q_proj:
  - Masked (zero) params: 8,388,608

  - Sparsity: 50.0%  - Parameters: 16,777,216

  - Masked (zero) params: 8,388,608
  - Sparsity: 50.0%
[Rank 1] Attached mask to module.model.layers.22.self_attn.k_proj:
[Rank 0] Attached mask to module.model.layers.22.self_attn.k_proj:  - Parameters: 4,194,304

  - Masked (zero) params: 2,097,152
  - Parameters: 4,194,304  - Sparsity: 50.0%

  - Masked (zero) params: 2,097,152
  - Sparsity: 50.0%
[Rank 1] Attached mask to module.model.layers.22.self_attn.v_proj:
  - Parameters: 4,194,304[Rank 0] Attached mask to module.model.layers.22.self_attn.v_proj:
  - Masked (zero) params: 2,097,152

  - Sparsity: 50.0%  - Parameters: 4,194,304

  - Masked (zero) params: 2,097,152
  - Sparsity: 50.0%
[Rank 1] Attached mask to module.model.layers.22.self_attn.o_proj:
  - Parameters: 16,777,216[Rank 0] Attached mask to module.model.layers.22.self_attn.o_proj:
  - Masked (zero) params: 8,388,608

  - Sparsity: 50.0%  - Parameters: 16,777,216

  - Masked (zero) params: 8,388,608
  - Sparsity: 50.0%
[Rank 0] Attached mask to module.model.layers.22.mlp.gate_proj:[Rank 1] Attached mask to module.model.layers.22.mlp.gate_proj:

  - Parameters: 58,720,256
  - Parameters: 58,720,256  - Masked (zero) params: 29,360,128

  - Sparsity: 50.0%
  - Masked (zero) params: 29,360,128
  - Sparsity: 50.0%
[Rank 0] Attached mask to module.model.layers.22.mlp.up_proj:[Rank 1] Attached mask to module.model.layers.22.mlp.up_proj:

  - Parameters: 58,720,256
  - Parameters: 58,720,256  - Masked (zero) params: 29,360,128

  - Sparsity: 50.0%
  - Masked (zero) params: 29,360,128
  - Sparsity: 50.0%
[Rank 0] Attached mask to module.model.layers.22.mlp.down_proj:[Rank 1] Attached mask to module.model.layers.22.mlp.down_proj:

  - Parameters: 58,720,256
  - Parameters: 58,720,256  - Masked (zero) params: 29,360,128

  - Sparsity: 50.0%
  - Masked (zero) params: 29,360,128
  - Sparsity: 50.0%
[Rank 1] Attached mask to module.model.layers.23.self_attn.q_proj:
  - Parameters: 16,777,216[Rank 0] Attached mask to module.model.layers.23.self_attn.q_proj:
  - Masked (zero) params: 8,388,608

  - Sparsity: 50.0%  - Parameters: 16,777,216

  - Masked (zero) params: 8,388,608
  - Sparsity: 50.0%
[Rank 1] Attached mask to module.model.layers.23.self_attn.k_proj:
[Rank 0] Attached mask to module.model.layers.23.self_attn.k_proj:  - Parameters: 4,194,304

  - Masked (zero) params: 2,097,152
  - Parameters: 4,194,304  - Sparsity: 50.0%

  - Masked (zero) params: 2,097,152
  - Sparsity: 50.0%
[Rank 1] Attached mask to module.model.layers.23.self_attn.v_proj:
[Rank 0] Attached mask to module.model.layers.23.self_attn.v_proj:  - Parameters: 4,194,304

  - Masked (zero) params: 2,097,152
  - Parameters: 4,194,304  - Sparsity: 50.0%

  - Masked (zero) params: 2,097,152
  - Sparsity: 50.0%
[Rank 1] Attached mask to module.model.layers.23.self_attn.o_proj:
[Rank 0] Attached mask to module.model.layers.23.self_attn.o_proj:  - Parameters: 16,777,216

  - Masked (zero) params: 8,388,608
  - Parameters: 16,777,216  - Sparsity: 50.0%

  - Masked (zero) params: 8,388,608
  - Sparsity: 50.0%
[Rank 0] Attached mask to module.model.layers.23.mlp.gate_proj:[Rank 1] Attached mask to module.model.layers.23.mlp.gate_proj:

  - Parameters: 58,720,256
  - Parameters: 58,720,256  - Masked (zero) params: 29,360,128

  - Sparsity: 50.0%
  - Masked (zero) params: 29,360,128
  - Sparsity: 50.0%
[Rank 0] Attached mask to module.model.layers.23.mlp.up_proj:[Rank 1] Attached mask to module.model.layers.23.mlp.up_proj:

  - Parameters: 58,720,256
  - Parameters: 58,720,256  - Masked (zero) params: 29,360,128

  - Sparsity: 50.0%
  - Masked (zero) params: 29,360,128
  - Sparsity: 50.0%
[Rank 0] Attached mask to module.model.layers.23.mlp.down_proj:[Rank 1] Attached mask to module.model.layers.23.mlp.down_proj:

  - Parameters: 58,720,256
  - Parameters: 58,720,256  - Masked (zero) params: 29,360,128

  - Sparsity: 50.0%
  - Masked (zero) params: 29,360,128
  - Sparsity: 50.0%
[Rank 1] Attached mask to module.model.layers.24.self_attn.q_proj:
[Rank 0] Attached mask to module.model.layers.24.self_attn.q_proj:  - Parameters: 16,777,216

  - Masked (zero) params: 8,388,608
  - Parameters: 16,777,216  - Sparsity: 50.0%

  - Masked (zero) params: 8,388,608
  - Sparsity: 50.0%
[Rank 1] Attached mask to module.model.layers.24.self_attn.k_proj:
[Rank 0] Attached mask to module.model.layers.24.self_attn.k_proj:  - Parameters: 4,194,304

  - Masked (zero) params: 2,097,152
  - Parameters: 4,194,304  - Sparsity: 50.0%

  - Masked (zero) params: 2,097,152
  - Sparsity: 50.0%
[Rank 1] Attached mask to module.model.layers.24.self_attn.v_proj:
[Rank 0] Attached mask to module.model.layers.24.self_attn.v_proj:  - Parameters: 4,194,304

  - Masked (zero) params: 2,097,152
  - Parameters: 4,194,304  - Sparsity: 50.0%

  - Masked (zero) params: 2,097,152
  - Sparsity: 50.0%
[Rank 1] Attached mask to module.model.layers.24.self_attn.o_proj:
  - Parameters: 16,777,216[Rank 0] Attached mask to module.model.layers.24.self_attn.o_proj:
  - Masked (zero) params: 8,388,608

  - Sparsity: 50.0%  - Parameters: 16,777,216

  - Masked (zero) params: 8,388,608
  - Sparsity: 50.0%
[Rank 0] Attached mask to module.model.layers.24.mlp.gate_proj:[Rank 1] Attached mask to module.model.layers.24.mlp.gate_proj:

  - Parameters: 58,720,256
  - Parameters: 58,720,256  - Masked (zero) params: 29,360,128

  - Sparsity: 50.0%
  - Masked (zero) params: 29,360,128
  - Sparsity: 50.0%
[Rank 0] Attached mask to module.model.layers.24.mlp.up_proj:[Rank 1] Attached mask to module.model.layers.24.mlp.up_proj:

  - Parameters: 58,720,256
  - Parameters: 58,720,256  - Masked (zero) params: 29,360,128

  - Sparsity: 50.0%
  - Masked (zero) params: 29,360,128
  - Sparsity: 50.0%
[Rank 0] Attached mask to module.model.layers.24.mlp.down_proj:[Rank 1] Attached mask to module.model.layers.24.mlp.down_proj:

  - Parameters: 58,720,256
  - Parameters: 58,720,256  - Masked (zero) params: 29,360,128

  - Sparsity: 50.0%
  - Masked (zero) params: 29,360,128
  - Sparsity: 50.0%
[Rank 0] Attached mask to module.model.layers.25.self_attn.q_proj:[Rank 1] Attached mask to module.model.layers.25.self_attn.q_proj:

  - Parameters: 16,777,216
  - Parameters: 16,777,216  - Masked (zero) params: 8,388,608

  - Sparsity: 50.0%
  - Masked (zero) params: 8,388,608
  - Sparsity: 50.0%
[Rank 1] Attached mask to module.model.layers.25.self_attn.k_proj:
[Rank 0] Attached mask to module.model.layers.25.self_attn.k_proj:  - Parameters: 4,194,304

  - Masked (zero) params: 2,097,152
  - Parameters: 4,194,304  - Sparsity: 50.0%

  - Masked (zero) params: 2,097,152
  - Sparsity: 50.0%
[Rank 1] Attached mask to module.model.layers.25.self_attn.v_proj:
[Rank 0] Attached mask to module.model.layers.25.self_attn.v_proj:  - Parameters: 4,194,304

  - Masked (zero) params: 2,097,152
  - Parameters: 4,194,304  - Sparsity: 50.0%

  - Masked (zero) params: 2,097,152
  - Sparsity: 50.0%
[Rank 1] Attached mask to module.model.layers.25.self_attn.o_proj:
[Rank 0] Attached mask to module.model.layers.25.self_attn.o_proj:  - Parameters: 16,777,216

  - Masked (zero) params: 8,388,608
  - Parameters: 16,777,216  - Sparsity: 50.0%

  - Masked (zero) params: 8,388,608
  - Sparsity: 50.0%
[Rank 0] Attached mask to module.model.layers.25.mlp.gate_proj:[Rank 1] Attached mask to module.model.layers.25.mlp.gate_proj:

  - Parameters: 58,720,256
  - Parameters: 58,720,256  - Masked (zero) params: 29,360,128

  - Sparsity: 50.0%
  - Masked (zero) params: 29,360,128
  - Sparsity: 50.0%
[Rank 0] Attached mask to module.model.layers.25.mlp.up_proj:[Rank 1] Attached mask to module.model.layers.25.mlp.up_proj:

  - Parameters: 58,720,256
  - Parameters: 58,720,256  - Masked (zero) params: 29,360,128

  - Sparsity: 50.0%
  - Masked (zero) params: 29,360,128
  - Sparsity: 50.0%
[Rank 0] Attached mask to module.model.layers.25.mlp.down_proj:[Rank 1] Attached mask to module.model.layers.25.mlp.down_proj:

  - Parameters: 58,720,256
  - Parameters: 58,720,256  - Masked (zero) params: 29,360,128

  - Sparsity: 50.0%
  - Masked (zero) params: 29,360,128
  - Sparsity: 50.0%
[Rank 1] Attached mask to module.model.layers.26.self_attn.q_proj:
[Rank 0] Attached mask to module.model.layers.26.self_attn.q_proj:  - Parameters: 16,777,216

  - Masked (zero) params: 8,388,608
  - Parameters: 16,777,216  - Sparsity: 50.0%

  - Masked (zero) params: 8,388,608
  - Sparsity: 50.0%
[Rank 1] Attached mask to module.model.layers.26.self_attn.k_proj:
[Rank 0] Attached mask to module.model.layers.26.self_attn.k_proj:  - Parameters: 4,194,304

  - Masked (zero) params: 2,097,152
  - Parameters: 4,194,304  - Sparsity: 50.0%

  - Masked (zero) params: 2,097,152
  - Sparsity: 50.0%
[Rank 1] Attached mask to module.model.layers.26.self_attn.v_proj:
[Rank 0] Attached mask to module.model.layers.26.self_attn.v_proj:  - Parameters: 4,194,304

  - Masked (zero) params: 2,097,152
  - Parameters: 4,194,304  - Sparsity: 50.0%

  - Masked (zero) params: 2,097,152
  - Sparsity: 50.0%
[Rank 1] Attached mask to module.model.layers.26.self_attn.o_proj:
  - Parameters: 16,777,216[Rank 0] Attached mask to module.model.layers.26.self_attn.o_proj:
  - Masked (zero) params: 8,388,608

  - Sparsity: 50.0%  - Parameters: 16,777,216

  - Masked (zero) params: 8,388,608
  - Sparsity: 50.0%
[Rank 0] Attached mask to module.model.layers.26.mlp.gate_proj:[Rank 1] Attached mask to module.model.layers.26.mlp.gate_proj:

  - Parameters: 58,720,256
  - Parameters: 58,720,256  - Masked (zero) params: 29,360,128

  - Sparsity: 50.0%
  - Masked (zero) params: 29,360,128
  - Sparsity: 50.0%
[Rank 0] Attached mask to module.model.layers.26.mlp.up_proj:[Rank 1] Attached mask to module.model.layers.26.mlp.up_proj:

  - Parameters: 58,720,256
  - Parameters: 58,720,256  - Masked (zero) params: 29,360,128

  - Sparsity: 50.0%
  - Masked (zero) params: 29,360,128
  - Sparsity: 50.0%
[Rank 0] Attached mask to module.model.layers.26.mlp.down_proj:[Rank 1] Attached mask to module.model.layers.26.mlp.down_proj:

  - Parameters: 58,720,256
  - Parameters: 58,720,256  - Masked (zero) params: 29,360,128

  - Sparsity: 50.0%
  - Masked (zero) params: 29,360,128
  - Sparsity: 50.0%
[Rank 0] Attached mask to module.model.layers.27.self_attn.q_proj:[Rank 1] Attached mask to module.model.layers.27.self_attn.q_proj:

  - Parameters: 16,777,216
  - Parameters: 16,777,216  - Masked (zero) params: 8,388,608

  - Sparsity: 50.0%
  - Masked (zero) params: 8,388,608
  - Sparsity: 50.0%
[Rank 1] Attached mask to module.model.layers.27.self_attn.k_proj:
[Rank 0] Attached mask to module.model.layers.27.self_attn.k_proj:  - Parameters: 4,194,304

  - Masked (zero) params: 2,097,152
  - Parameters: 4,194,304  - Sparsity: 50.0%

  - Masked (zero) params: 2,097,152
  - Sparsity: 50.0%
[Rank 1] Attached mask to module.model.layers.27.self_attn.v_proj:
[Rank 0] Attached mask to module.model.layers.27.self_attn.v_proj:  - Parameters: 4,194,304

  - Masked (zero) params: 2,097,152
  - Parameters: 4,194,304  - Sparsity: 50.0%

  - Masked (zero) params: 2,097,152
  - Sparsity: 50.0%
[Rank 0] Attached mask to module.model.layers.27.self_attn.o_proj:[Rank 1] Attached mask to module.model.layers.27.self_attn.o_proj:

  - Parameters: 16,777,216
  - Parameters: 16,777,216  - Masked (zero) params: 8,388,608

  - Sparsity: 50.0%
  - Masked (zero) params: 8,388,608
  - Sparsity: 50.0%
[Rank 0] Attached mask to module.model.layers.27.mlp.gate_proj:[Rank 1] Attached mask to module.model.layers.27.mlp.gate_proj:

  - Parameters: 58,720,256
  - Parameters: 58,720,256  - Masked (zero) params: 29,360,128

  - Sparsity: 50.0%
  - Masked (zero) params: 29,360,128
  - Sparsity: 50.0%
[Rank 1] Attached mask to module.model.layers.27.mlp.up_proj:
[Rank 0] Attached mask to module.model.layers.27.mlp.up_proj:  - Parameters: 58,720,256

  - Masked (zero) params: 29,360,128
  - Parameters: 58,720,256  - Sparsity: 50.0%

  - Masked (zero) params: 29,360,128
  - Sparsity: 50.0%
[Rank 0] Attached mask to module.model.layers.27.mlp.down_proj:[Rank 1] Attached mask to module.model.layers.27.mlp.down_proj:

  - Parameters: 58,720,256
  - Parameters: 58,720,256  - Masked (zero) params: 29,360,128

  - Sparsity: 50.0%
  - Masked (zero) params: 29,360,128
  - Sparsity: 50.0%
[Rank 1] Attached mask to module.model.layers.28.self_attn.q_proj:
  - Parameters: 16,777,216[Rank 0] Attached mask to module.model.layers.28.self_attn.q_proj:
  - Masked (zero) params: 8,388,608

  - Sparsity: 50.0%  - Parameters: 16,777,216

  - Masked (zero) params: 8,388,608
  - Sparsity: 50.0%
[Rank 1] Attached mask to module.model.layers.28.self_attn.k_proj:
[Rank 0] Attached mask to module.model.layers.28.self_attn.k_proj:  - Parameters: 4,194,304

  - Masked (zero) params: 2,097,152
  - Parameters: 4,194,304  - Sparsity: 50.0%

  - Masked (zero) params: 2,097,152
  - Sparsity: 50.0%
[Rank 1] Attached mask to module.model.layers.28.self_attn.v_proj:
[Rank 0] Attached mask to module.model.layers.28.self_attn.v_proj:  - Parameters: 4,194,304

  - Masked (zero) params: 2,097,152
  - Parameters: 4,194,304  - Sparsity: 50.0%

  - Masked (zero) params: 2,097,152
  - Sparsity: 50.0%
[Rank 1] Attached mask to module.model.layers.28.self_attn.o_proj:
[Rank 0] Attached mask to module.model.layers.28.self_attn.o_proj:  - Parameters: 16,777,216

  - Masked (zero) params: 8,388,608
  - Parameters: 16,777,216  - Sparsity: 50.0%

  - Masked (zero) params: 8,388,608
  - Sparsity: 50.0%
[Rank 0] Attached mask to module.model.layers.28.mlp.gate_proj:[Rank 1] Attached mask to module.model.layers.28.mlp.gate_proj:

  - Parameters: 58,720,256
  - Parameters: 58,720,256  - Masked (zero) params: 29,360,128

  - Sparsity: 50.0%
  - Masked (zero) params: 29,360,128
  - Sparsity: 50.0%
[Rank 1] Attached mask to module.model.layers.28.mlp.up_proj:
[Rank 0] Attached mask to module.model.layers.28.mlp.up_proj:  - Parameters: 58,720,256

  - Masked (zero) params: 29,360,128
  - Parameters: 58,720,256  - Sparsity: 50.0%

  - Masked (zero) params: 29,360,128
  - Sparsity: 50.0%
[Rank 1] Attached mask to module.model.layers.28.mlp.down_proj:
[Rank 0] Attached mask to module.model.layers.28.mlp.down_proj:  - Parameters: 58,720,256

  - Masked (zero) params: 29,360,128
  - Parameters: 58,720,256  - Sparsity: 50.0%

  - Masked (zero) params: 29,360,128
  - Sparsity: 50.0%
[Rank 0] Attached mask to module.model.layers.29.self_attn.q_proj:[Rank 1] Attached mask to module.model.layers.29.self_attn.q_proj:

  - Parameters: 16,777,216
  - Parameters: 16,777,216  - Masked (zero) params: 8,388,608

  - Sparsity: 50.0%
  - Masked (zero) params: 8,388,608
  - Sparsity: 50.0%
[Rank 1] Attached mask to module.model.layers.29.self_attn.k_proj:
[Rank 0] Attached mask to module.model.layers.29.self_attn.k_proj:  - Parameters: 4,194,304

  - Masked (zero) params: 2,097,152
  - Parameters: 4,194,304  - Sparsity: 50.0%

  - Masked (zero) params: 2,097,152
  - Sparsity: 50.0%
[Rank 1] Attached mask to module.model.layers.29.self_attn.v_proj:
[Rank 0] Attached mask to module.model.layers.29.self_attn.v_proj:  - Parameters: 4,194,304

  - Masked (zero) params: 2,097,152
  - Parameters: 4,194,304  - Sparsity: 50.0%

  - Masked (zero) params: 2,097,152
  - Sparsity: 50.0%
[Rank 1] Attached mask to module.model.layers.29.self_attn.o_proj:
[Rank 0] Attached mask to module.model.layers.29.self_attn.o_proj:  - Parameters: 16,777,216

  - Masked (zero) params: 8,388,608
  - Parameters: 16,777,216  - Sparsity: 50.0%

  - Masked (zero) params: 8,388,608
  - Sparsity: 50.0%
[Rank 0] Attached mask to module.model.layers.29.mlp.gate_proj:[Rank 1] Attached mask to module.model.layers.29.mlp.gate_proj:

  - Parameters: 58,720,256
  - Parameters: 58,720,256  - Masked (zero) params: 29,360,128

  - Sparsity: 50.0%
  - Masked (zero) params: 29,360,128
  - Sparsity: 50.0%
[Rank 0] Attached mask to module.model.layers.29.mlp.up_proj:[Rank 1] Attached mask to module.model.layers.29.mlp.up_proj:

  - Parameters: 58,720,256
  - Parameters: 58,720,256  - Masked (zero) params: 29,360,128

  - Sparsity: 50.0%
  - Masked (zero) params: 29,360,128
  - Sparsity: 50.0%
[Rank 0] Attached mask to module.model.layers.29.mlp.down_proj:[Rank 1] Attached mask to module.model.layers.29.mlp.down_proj:

  - Parameters: 58,720,256
  - Parameters: 58,720,256  - Masked (zero) params: 29,360,128

  - Sparsity: 50.0%
  - Masked (zero) params: 29,360,128
  - Sparsity: 50.0%
[Rank 1] Attached mask to module.model.layers.30.self_attn.q_proj:
[Rank 0] Attached mask to module.model.layers.30.self_attn.q_proj:  - Parameters: 16,777,216

  - Masked (zero) params: 8,388,608
  - Parameters: 16,777,216  - Sparsity: 50.0%

  - Masked (zero) params: 8,388,608
  - Sparsity: 50.0%
[Rank 1] Attached mask to module.model.layers.30.self_attn.k_proj:
[Rank 0] Attached mask to module.model.layers.30.self_attn.k_proj:  - Parameters: 4,194,304

  - Masked (zero) params: 2,097,152
  - Parameters: 4,194,304  - Sparsity: 50.0%

  - Masked (zero) params: 2,097,152
  - Sparsity: 50.0%
[Rank 1] Attached mask to module.model.layers.30.self_attn.v_proj:
[Rank 0] Attached mask to module.model.layers.30.self_attn.v_proj:  - Parameters: 4,194,304

  - Masked (zero) params: 2,097,152
  - Parameters: 4,194,304  - Sparsity: 50.0%

  - Masked (zero) params: 2,097,152
  - Sparsity: 50.0%
[Rank 1] Attached mask to module.model.layers.30.self_attn.o_proj:
[Rank 0] Attached mask to module.model.layers.30.self_attn.o_proj:  - Parameters: 16,777,216

  - Masked (zero) params: 8,388,608
  - Parameters: 16,777,216  - Sparsity: 50.0%

  - Masked (zero) params: 8,388,608
  - Sparsity: 50.0%
[Rank 0] Attached mask to module.model.layers.30.mlp.gate_proj:[Rank 1] Attached mask to module.model.layers.30.mlp.gate_proj:

  - Parameters: 58,720,256
  - Parameters: 58,720,256  - Masked (zero) params: 29,360,128

  - Sparsity: 50.0%
  - Masked (zero) params: 29,360,128
  - Sparsity: 50.0%
[Rank 0] Attached mask to module.model.layers.30.mlp.up_proj:[Rank 1] Attached mask to module.model.layers.30.mlp.up_proj:

  - Parameters: 58,720,256
  - Parameters: 58,720,256  - Masked (zero) params: 29,360,128

  - Sparsity: 50.0%
  - Masked (zero) params: 29,360,128
  - Sparsity: 50.0%
[Rank 0] Attached mask to module.model.layers.30.mlp.down_proj:[Rank 1] Attached mask to module.model.layers.30.mlp.down_proj:

  - Parameters: 58,720,256
  - Parameters: 58,720,256  - Masked (zero) params: 29,360,128

  - Sparsity: 50.0%
  - Masked (zero) params: 29,360,128
  - Sparsity: 50.0%
[Rank 1] Attached mask to module.model.layers.31.self_attn.q_proj:
[Rank 0] Attached mask to module.model.layers.31.self_attn.q_proj:  - Parameters: 16,777,216

  - Masked (zero) params: 8,388,608
  - Parameters: 16,777,216  - Sparsity: 50.0%

  - Masked (zero) params: 8,388,608
  - Sparsity: 50.0%
[Rank 1] Attached mask to module.model.layers.31.self_attn.k_proj:
[Rank 0] Attached mask to module.model.layers.31.self_attn.k_proj:  - Parameters: 4,194,304

  - Masked (zero) params: 2,097,152
  - Parameters: 4,194,304  - Sparsity: 50.0%

  - Masked (zero) params: 2,097,152
  - Sparsity: 50.0%
[Rank 1] Attached mask to module.model.layers.31.self_attn.v_proj:
[Rank 0] Attached mask to module.model.layers.31.self_attn.v_proj:  - Parameters: 4,194,304

  - Masked (zero) params: 2,097,152
  - Parameters: 4,194,304  - Sparsity: 50.0%

  - Masked (zero) params: 2,097,152
  - Sparsity: 50.0%
[Rank 1] Attached mask to module.model.layers.31.self_attn.o_proj:
[Rank 0] Attached mask to module.model.layers.31.self_attn.o_proj:  - Parameters: 16,777,216

  - Masked (zero) params: 8,388,608
  - Parameters: 16,777,216  - Sparsity: 50.0%

  - Masked (zero) params: 8,388,608
  - Sparsity: 50.0%
[Rank 0] Attached mask to module.model.layers.31.mlp.gate_proj:[Rank 1] Attached mask to module.model.layers.31.mlp.gate_proj:

  - Parameters: 58,720,256
  - Parameters: 58,720,256  - Masked (zero) params: 29,360,128

  - Sparsity: 50.0%
  - Masked (zero) params: 29,360,128
  - Sparsity: 50.0%
[Rank 0] Attached mask to module.model.layers.31.mlp.up_proj:[Rank 1] Attached mask to module.model.layers.31.mlp.up_proj:

  - Parameters: 58,720,256
  - Parameters: 58,720,256  - Masked (zero) params: 29,360,128

  - Sparsity: 50.0%
  - Masked (zero) params: 29,360,128
  - Sparsity: 50.0%
[Rank 0] Attached mask to module.model.layers.31.mlp.down_proj:[Rank 1] Attached mask to module.model.layers.31.mlp.down_proj:

  - Parameters: 58,720,256
  - Parameters: 58,720,256  - Masked (zero) params: 29,360,128

  - Sparsity: 50.0%
  - Masked (zero) params: 29,360,128
  - Sparsity: 50.0%

[Rank 1] Local Stats:

[Rank 0] Local Stats:Total parameters: 6,979,321,856

Masked parameters: 3,489,660,928
Total parameters: 6,979,321,856Sparsity: 50.0%

Non-empty layers: 224
Masked parameters: 3,489,660,928Empty layers: 0

Sparsity: 50.0%
Non-empty layers: 224
Empty layers: 0

Global Stats (All Ranks):
Total parameters across model: 13,958,643,712
Total masked parameters: 6,979,321,856Starting from epoch 0 and step 0.

Global sparsity: 50.0%
Total non-empty layers: 448
Total empty/skipped layers: 0
Layers with correct sparsity: 448
INFO:__main__:Masks attached successfully!
INFO:__main__:***** Running training *****
INFO:__main__:  Num examples = 100
INFO:__main__:  Num Epochs = 1
INFO:__main__:  Instantaneous batch size per device = 1
INFO:__main__:  Total train batch size (w. parallel, distributed & accumulation) = 16
INFO:__main__:  Gradient Accumulation steps = 8
INFO:__main__:  Total optimization steps = 7

  0%|                                                                                                     | 0/7 [00:00<?, ?it/s]WARNING:open_instruct.utils:Output directory exists but no checkpoint found. Starting from scratch.
Starting from epoch 0 and step 0.
/workspace/tulu_env/lib/python3.10/site-packages/transformers/data/data_collator.py:656: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)
  batch["labels"] = torch.tensor(batch["labels"], dtype=torch.int64)
/workspace/tulu_env/lib/python3.10/site-packages/transformers/data/data_collator.py:656: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)
  batch["labels"] = torch.tensor(batch["labels"], dtype=torch.int64)
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)
/workspace/tulu_env/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
/workspace/tulu_env/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]

 14%|█████████████▎                                                                               | 1/7 [00:29<02:57, 29.66s/it]INFO:__main__:  Step: 1, LR: 4.5833333333333327e-07, Loss: 0.08664339780807495

 29%|██████████████████████████▌                                                                  | 2/7 [00:49<01:58, 23.66s/it]INFO:__main__:  Step: 2, LR: 3.75e-07, Loss: 0.08664339780807495

 43%|███████████████████████████████████████▊                                                     | 3/7 [01:08<01:27, 21.87s/it]INFO:__main__:  Step: 3, LR: 2.916666666666667e-07, Loss: 0.08664339780807495

 57%|█████████████████████████████████████████████████████▏                                       | 4/7 [01:29<01:03, 21.31s/it]INFO:__main__:  Step: 4, LR: 2.0833333333333333e-07, Loss: 0.08664339780807495

 71%|██████████████████████████████████████████████████████████████████▍                          | 5/7 [01:49<00:41, 20.91s/it]INFO:__main__:  Step: 5, LR: 1.25e-07, Loss: 0.08664339780807495

 86%|███████████████████████████████████████████████████████████████████████████████▋             | 6/7 [02:09<00:20, 20.62s/it]INFO:__main__:  Step: 6, LR: 4.166666666666666e-08, Loss: 0.08664339780807495

100%|█████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [02:28<00:00, 19.92s/it]INFO:__main__:  Step: 7, LR: 0.0, Loss: 0.08664339780807495
Configuration saved in output/dpo_pythia_14m/config.json
Configuration saved in output/dpo_pythia_14m/generation_config.json
The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 4 checkpoint shards. You can find where each parameters has been saved in the index located at output/dpo_pythia_14m/pytorch_model.bin.index.json.
tokenizer config file saved in output/dpo_pythia_14m/tokenizer_config.json
Special tokens file saved in output/dpo_pythia_14m/special_tokens_map.json
INFO:open_instruct.utils:Remaining files:['tokenizer.json', 'special_tokens_map.json', 'tokenizer_config.json', 'pytorch_model.bin.index.json', 'pytorch_model-00004-of-00004.bin', 'pytorch_model-00003-of-00004.bin', 'pytorch_model-00002-of-00004.bin', 'pytorch_model-00001-of-00004.bin', 'generation_config.json', 'config.json']


Upload 4 LFS files:   0%|                                                                                 | 0/4 [00:00<?, ?it/s][A


pytorch_model-00002-of-00004.bin:   0%|                                                             | 0.00/5.00G [00:00<?, ?B/s][A[A



pytorch_model-00004-of-00004.bin:   0%|                                                             | 0.00/1.17G [00:00<?, ?B/s][A[A[A





pytorch_model-00001-of-00004.bin:   0%|                                                             | 0.00/4.98G [00:00<?, ?B/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:   0%|                                                             | 0.00/4.92G [00:00<?, ?B/s][A[A[A[A


pytorch_model-00002-of-00004.bin:   0%|                                                    | 3.24M/5.00G [00:00<02:34, 32.4MB/s][A[A



pytorch_model-00004-of-00004.bin:   0%|▏                                                   | 4.49M/1.17G [00:00<00:25, 44.9MB/s][A[A[A





pytorch_model-00001-of-00004.bin:   0%|                                                    | 5.11M/4.98G [00:00<01:37, 51.0MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:   0%|                                                    | 4.21M/4.92G [00:00<01:56, 42.1MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:   0%|▏                                                   | 12.3M/5.00G [00:00<01:14, 66.8MB/s][A[A



pytorch_model-00004-of-00004.bin:   1%|▌                                                   | 13.1M/1.17G [00:00<00:16, 68.9MB/s][A[A[A





pytorch_model-00001-of-00004.bin:   0%|▏                                                   | 14.4M/4.98G [00:00<01:05, 75.7MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:   0%|▏                                                   | 12.8M/4.92G [00:00<01:12, 67.8MB/s][A[A[A[A





pytorch_model-00001-of-00004.bin:   0%|▏                                                   | 22.0M/4.98G [00:00<01:21, 60.9MB/s][A[A[A[A[A


pytorch_model-00002-of-00004.bin:   0%|▏                                                   | 19.0M/5.00G [00:00<01:36, 51.5MB/s][A[A




pytorch_model-00003-of-00004.bin:   0%|▏                                                   | 19.6M/4.92G [00:00<01:42, 47.6MB/s][A[A[A[A



pytorch_model-00004-of-00004.bin:   2%|▉                                                   | 20.0M/1.17G [00:00<00:25, 45.2MB/s][A[A[A


pytorch_model-00002-of-00004.bin:   1%|▎                                                   | 31.5M/5.00G [00:00<01:05, 75.9MB/s][A[A




pytorch_model-00003-of-00004.bin:   1%|▎                                                   | 32.0M/4.92G [00:00<01:35, 51.3MB/s][A[A[A[A



pytorch_model-00004-of-00004.bin:   3%|█▍                                                  | 32.0M/1.17G [00:00<00:23, 49.1MB/s][A[A[A




pytorch_model-00003-of-00004.bin:   1%|▌                                                   | 47.9M/4.92G [00:00<01:02, 77.6MB/s][A[A[A[A



pytorch_model-00004-of-00004.bin:   4%|██                                                  | 47.4M/1.17G [00:00<00:15, 73.6MB/s][A[A[A





pytorch_model-00001-of-00004.bin:   1%|▎                                                   | 32.0M/4.98G [00:00<02:41, 30.7MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:   1%|▌                                                   | 57.1M/4.92G [00:00<01:20, 60.1MB/s][A[A[A[A





pytorch_model-00001-of-00004.bin:   1%|▍                                                   | 43.5M/4.98G [00:00<01:50, 44.8MB/s][A[A[A[A[A



pytorch_model-00004-of-00004.bin:   5%|██▌                                                 | 56.2M/1.17G [00:01<00:20, 54.4MB/s][A[A[A


pytorch_model-00002-of-00004.bin:   1%|▍                                                   | 39.7M/5.00G [00:01<02:38, 31.2MB/s][A[A




pytorch_model-00003-of-00004.bin:   1%|▋                                                   | 64.5M/4.92G [00:01<01:32, 52.2MB/s][A[A[A[A



pytorch_model-00004-of-00004.bin:   5%|██▊                                                 | 64.0M/1.17G [00:01<00:22, 48.9MB/s][A[A[A


pytorch_model-00002-of-00004.bin:   1%|▍                                                   | 48.0M/5.00G [00:01<02:27, 33.5MB/s][A[A





pytorch_model-00001-of-00004.bin:   1%|▌                                                   | 50.6M/4.98G [00:01<02:26, 33.7MB/s][A[A[A[A[A


pytorch_model-00002-of-00004.bin:   1%|▋                                                   | 62.8M/5.00G [00:01<01:35, 51.7MB/s][A[A




pytorch_model-00003-of-00004.bin:   2%|▊                                                   | 80.0M/4.92G [00:01<01:29, 54.1MB/s][A[A[A[A



pytorch_model-00004-of-00004.bin:   7%|███▌                                                | 80.0M/1.17G [00:01<00:19, 55.5MB/s][A[A[A





pytorch_model-00001-of-00004.bin:   1%|▋                                                   | 64.0M/4.98G [00:01<01:53, 43.4MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:   2%|▉                                                   | 89.8M/4.92G [00:01<01:18, 61.8MB/s][A[A[A[A



pytorch_model-00004-of-00004.bin:   8%|███▉                                                | 89.1M/1.17G [00:01<00:17, 61.8MB/s][A[A[A


pytorch_model-00002-of-00004.bin:   1%|▋                                                   | 71.1M/5.00G [00:01<01:51, 44.0MB/s][A[A





pytorch_model-00001-of-00004.bin:   1%|▊                                                   | 74.4M/4.98G [00:01<01:31, 53.3MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:   2%|█                                                   | 97.0M/4.92G [00:01<01:28, 54.3MB/s][A[A[A[A



pytorch_model-00004-of-00004.bin:   8%|████▎                                               | 96.2M/1.17G [00:01<00:19, 54.2MB/s][A[A[A


pytorch_model-00002-of-00004.bin:   2%|▊                                                   | 80.0M/5.00G [00:01<01:49, 45.1MB/s][A[A





pytorch_model-00001-of-00004.bin:   2%|▊                                                   | 81.8M/4.98G [00:01<01:42, 47.7MB/s][A[A[A[A[A



pytorch_model-00004-of-00004.bin:  10%|█████                                                | 112M/1.17G [00:01<00:17, 60.6MB/s][A[A[A




pytorch_model-00003-of-00004.bin:   2%|█▏                                                   | 112M/4.92G [00:01<01:27, 55.2MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:   2%|▉                                                   | 96.0M/5.00G [00:02<01:31, 53.5MB/s][A[A



pytorch_model-00004-of-00004.bin:  11%|█████▌                                               | 123M/1.17G [00:02<00:14, 70.3MB/s][A[A[A




pytorch_model-00003-of-00004.bin:   2%|█▎                                                   | 123M/4.92G [00:02<01:14, 64.2MB/s][A[A[A[A





pytorch_model-00001-of-00004.bin:   2%|█                                                   | 96.0M/4.98G [00:02<01:57, 41.5MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:   3%|█▍                                                   | 130M/4.92G [00:02<01:22, 58.2MB/s][A[A[A[A



pytorch_model-00004-of-00004.bin:  11%|█████▉                                               | 131M/1.17G [00:02<00:17, 59.8MB/s][A[A[A


pytorch_model-00002-of-00004.bin:   2%|█▏                                                   | 112M/5.00G [00:02<01:32, 53.1MB/s][A[A





pytorch_model-00001-of-00004.bin:   2%|█▏                                                   | 108M/4.98G [00:02<01:32, 52.4MB/s][A[A[A[A[A



pytorch_model-00004-of-00004.bin:  12%|██████▌                                              | 143M/1.17G [00:02<00:14, 72.4MB/s][A[A[A


pytorch_model-00002-of-00004.bin:   3%|█▎                                                   | 128M/5.00G [00:02<01:10, 68.7MB/s][A[A




pytorch_model-00003-of-00004.bin:   3%|█▌                                                   | 144M/4.92G [00:02<01:18, 61.1MB/s][A[A[A[A



pytorch_model-00004-of-00004.bin:  13%|██████▉                                              | 152M/1.17G [00:02<00:16, 62.1MB/s][A[A[A


pytorch_model-00002-of-00004.bin:   3%|█▍                                                   | 137M/5.00G [00:02<01:20, 60.2MB/s][A[A




pytorch_model-00003-of-00004.bin:   3%|█▋                                                   | 160M/4.92G [00:02<01:11, 66.6MB/s][A[A[A[A





pytorch_model-00001-of-00004.bin:   2%|█▏                                                   | 115M/4.98G [00:02<02:06, 38.3MB/s][A[A[A[A[A



pytorch_model-00004-of-00004.bin:  14%|███████▎                                             | 160M/1.17G [00:02<00:18, 53.9MB/s][A[A[A


pytorch_model-00002-of-00004.bin:   3%|█▌                                                   | 144M/5.00G [00:02<01:29, 54.5MB/s][A[A




pytorch_model-00003-of-00004.bin:   4%|█▉                                                   | 176M/4.92G [00:02<01:10, 66.9MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:   3%|█▌                                                   | 150M/5.00G [00:02<01:28, 54.7MB/s][A[A



pytorch_model-00004-of-00004.bin:  15%|███████▉                                             | 176M/1.17G [00:02<00:15, 62.7MB/s][A[A[A





pytorch_model-00001-of-00004.bin:   3%|█▎                                                   | 128M/4.98G [00:03<02:14, 36.1MB/s][A[A[A[A[A



pytorch_model-00004-of-00004.bin:  16%|████████▋                                            | 192M/1.17G [00:03<00:15, 63.8MB/s][A[A[A




pytorch_model-00003-of-00004.bin:   4%|██                                                   | 192M/4.92G [00:03<01:16, 61.8MB/s][A[A[A[A



pytorch_model-00004-of-00004.bin:  18%|█████████▎                                           | 206M/1.17G [00:03<00:12, 77.1MB/s][A[A[A





pytorch_model-00001-of-00004.bin:   3%|█▌                                                   | 144M/4.98G [00:03<01:53, 42.4MB/s][A[A[A[A[A



pytorch_model-00004-of-00004.bin:  18%|█████████▋                                           | 215M/1.17G [00:03<00:13, 68.2MB/s][A[A[A




pytorch_model-00003-of-00004.bin:   4%|██▏                                                  | 208M/4.92G [00:03<01:27, 53.8MB/s][A[A[A[A





pytorch_model-00001-of-00004.bin:   3%|█▋                                                   | 160M/4.98G [00:03<01:37, 49.6MB/s][A[A[A[A[A



pytorch_model-00004-of-00004.bin:  19%|██████████▏                                          | 224M/1.17G [00:03<00:16, 57.8MB/s][A[A[A




pytorch_model-00003-of-00004.bin:   5%|██▍                                                  | 224M/4.92G [00:03<01:16, 61.1MB/s][A[A[A[A



pytorch_model-00004-of-00004.bin:  20%|██████████▊                                          | 239M/1.17G [00:03<00:12, 75.4MB/s][A[A[A





pytorch_model-00001-of-00004.bin:   4%|█▊                                                   | 176M/4.98G [00:03<01:31, 52.7MB/s][A[A[A[A[A


pytorch_model-00002-of-00004.bin:   3%|█▋                                                   | 160M/5.00G [00:03<03:36, 22.4MB/s][A[A




pytorch_model-00003-of-00004.bin:   5%|██▌                                                  | 240M/4.92G [00:03<01:09, 67.2MB/s][A[A[A[A



pytorch_model-00004-of-00004.bin:  21%|███████████▎                                         | 249M/1.17G [00:04<00:16, 55.9MB/s][A[A[A





pytorch_model-00001-of-00004.bin:   4%|██                                                   | 192M/4.98G [00:04<01:28, 54.0MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:   5%|██▊                                                  | 256M/4.92G [00:04<01:08, 68.4MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:   4%|█▊                                                   | 176M/5.00G [00:04<02:39, 30.2MB/s][A[A





pytorch_model-00001-of-00004.bin:   4%|██▏                                                  | 207M/4.98G [00:04<01:10, 67.3MB/s][A[A[A[A[A



pytorch_model-00004-of-00004.bin:  22%|███████████▌                                         | 256M/1.17G [00:04<00:16, 53.8MB/s][A[A[A




pytorch_model-00003-of-00004.bin:   6%|██▉                                                  | 272M/4.92G [00:04<01:09, 67.2MB/s][A[A[A[A





pytorch_model-00001-of-00004.bin:   4%|██▎                                                  | 216M/4.98G [00:04<01:19, 59.8MB/s][A[A[A[A[A


pytorch_model-00002-of-00004.bin:   4%|██                                                   | 192M/5.00G [00:04<02:07, 37.7MB/s][A[A



pytorch_model-00004-of-00004.bin:  23%|████████████▎                                        | 272M/1.17G [00:04<00:14, 59.9MB/s][A[A[A





pytorch_model-00001-of-00004.bin:   5%|██▍                                                  | 224M/4.98G [00:04<01:26, 54.6MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:   6%|███                                                  | 288M/4.92G [00:04<01:08, 68.0MB/s][A[A[A[A



pytorch_model-00004-of-00004.bin:  25%|█████████████                                        | 288M/1.17G [00:04<00:14, 60.5MB/s][A[A[A


pytorch_model-00002-of-00004.bin:   4%|██▏                                                  | 208M/5.00G [00:04<01:52, 42.5MB/s][A[A




pytorch_model-00003-of-00004.bin:   6%|███▎                                                 | 303M/4.92G [00:04<00:56, 81.0MB/s][A[A[A[A





pytorch_model-00001-of-00004.bin:   5%|██▌                                                  | 240M/4.98G [00:04<01:24, 56.3MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:   6%|███▎                                                 | 313M/4.92G [00:04<01:04, 71.3MB/s][A[A[A[A



pytorch_model-00004-of-00004.bin:  26%|█████████████▊                                       | 304M/1.17G [00:05<00:14, 60.4MB/s][A[A[A


pytorch_model-00002-of-00004.bin:   4%|██▎                                                  | 224M/5.00G [00:05<01:41, 46.9MB/s][A[A





pytorch_model-00001-of-00004.bin:   5%|██▋                                                  | 256M/4.98G [00:05<01:23, 56.9MB/s][A[A[A[A[A


pytorch_model-00002-of-00004.bin:   5%|██▌                                                  | 240M/5.00G [00:05<01:27, 54.5MB/s][A[A



pytorch_model-00004-of-00004.bin:  27%|██████████████▌                                      | 320M/1.17G [00:05<00:13, 64.9MB/s][A[A[A





pytorch_model-00001-of-00004.bin:   5%|██▉                                                  | 272M/4.98G [00:05<01:05, 72.1MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:   7%|███▍                                                 | 321M/4.92G [00:05<01:49, 42.0MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:   5%|██▋                                                  | 256M/5.00G [00:05<01:23, 57.0MB/s][A[A



pytorch_model-00004-of-00004.bin:  29%|███████████████▏                                     | 336M/1.17G [00:05<00:13, 62.2MB/s][A[A[A





pytorch_model-00001-of-00004.bin:   6%|██▉                                                  | 281M/4.98G [00:05<01:18, 59.6MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:   7%|███▌                                                 | 336M/4.92G [00:05<01:34, 48.6MB/s][A[A[A[A



pytorch_model-00004-of-00004.bin:  30%|███████████████▉                                     | 352M/1.17G [00:05<00:12, 67.1MB/s][A[A[A


pytorch_model-00002-of-00004.bin:   5%|██▉                                                  | 272M/5.00G [00:05<01:18, 60.1MB/s][A[A





pytorch_model-00001-of-00004.bin:   6%|███                                                  | 288M/4.98G [00:05<01:29, 52.5MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:   7%|███▋                                                 | 346M/4.92G [00:05<01:21, 56.1MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:   6%|███                                                  | 284M/5.00G [00:05<01:08, 68.8MB/s][A[A



pytorch_model-00004-of-00004.bin:  32%|████████████████▋                                    | 368M/1.17G [00:05<00:11, 70.2MB/s][A[A[A




pytorch_model-00003-of-00004.bin:   7%|███▊                                                 | 354M/4.92G [00:05<01:27, 52.4MB/s][A[A[A[A





pytorch_model-00001-of-00004.bin:   6%|███▏                                                 | 304M/4.98G [00:05<01:21, 57.4MB/s][A[A[A[A[A


pytorch_model-00002-of-00004.bin:   6%|███                                                  | 292M/5.00G [00:06<01:18, 59.7MB/s][A[A




pytorch_model-00003-of-00004.bin:   7%|███▉                                                 | 367M/4.92G [00:06<01:09, 65.3MB/s][A[A[A[A





pytorch_model-00001-of-00004.bin:   6%|███▍                                                 | 319M/4.98G [00:06<01:04, 71.9MB/s][A[A[A[A[A


pytorch_model-00002-of-00004.bin:   6%|███▏                                                 | 304M/5.00G [00:06<01:17, 61.0MB/s][A[A



pytorch_model-00004-of-00004.bin:  33%|█████████████████▍                                   | 384M/1.17G [00:06<00:12, 61.2MB/s][A[A[A




pytorch_model-00003-of-00004.bin:   8%|████                                                 | 375M/4.92G [00:06<01:22, 55.1MB/s][A[A[A[A





pytorch_model-00001-of-00004.bin:   7%|███▍                                                 | 327M/4.98G [00:06<01:20, 57.6MB/s][A[A[A[A[A


pytorch_model-00002-of-00004.bin:   6%|███▍                                                 | 320M/5.00G [00:06<01:15, 62.2MB/s][A[A




pytorch_model-00003-of-00004.bin:   8%|████▏                                                | 384M/4.92G [00:06<01:22, 55.2MB/s][A[A[A[A



pytorch_model-00004-of-00004.bin:  34%|██████████████████▏                                  | 400M/1.17G [00:06<00:12, 60.5MB/s][A[A[A



pytorch_model-00004-of-00004.bin:  36%|██████████████████▊                                  | 416M/1.17G [00:06<00:10, 74.4MB/s][A[A[A





pytorch_model-00001-of-00004.bin:   7%|███▌                                                 | 336M/4.98G [00:06<01:38, 47.2MB/s][A[A[A[A[A


pytorch_model-00002-of-00004.bin:   7%|███▌                                                 | 336M/5.00G [00:06<01:15, 61.7MB/s][A[A




pytorch_model-00003-of-00004.bin:   8%|████▎                                                | 400M/4.92G [00:06<01:18, 57.4MB/s][A[A[A[A




pytorch_model-00003-of-00004.bin:   8%|████▍                                                | 414M/4.92G [00:06<01:02, 72.3MB/s][A[A[A[A



pytorch_model-00004-of-00004.bin:  36%|███████████████████▎                                 | 425M/1.17G [00:06<00:11, 64.3MB/s][A[A[A





pytorch_model-00001-of-00004.bin:   7%|███▋                                                 | 352M/4.98G [00:06<01:41, 45.6MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:   9%|████▌                                                | 423M/4.92G [00:07<01:12, 61.7MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:   7%|███▋                                                 | 352M/5.00G [00:07<01:25, 54.2MB/s][A[A





pytorch_model-00001-of-00004.bin:   7%|███▉                                                 | 368M/4.98G [00:07<01:15, 60.8MB/s][A[A[A[A[A



pytorch_model-00004-of-00004.bin:  37%|███████████████████▋                                 | 433M/1.17G [00:07<00:14, 51.9MB/s][A[A[A




pytorch_model-00003-of-00004.bin:   9%|████▋                                                | 432M/4.92G [00:07<01:16, 58.4MB/s][A[A[A[A



pytorch_model-00004-of-00004.bin:  38%|████████████████████▎                                | 448M/1.17G [00:07<00:12, 59.0MB/s][A[A[A





pytorch_model-00001-of-00004.bin:   8%|████                                                 | 376M/4.98G [00:07<01:28, 51.9MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:   9%|████▊                                                | 448M/4.92G [00:07<01:15, 59.1MB/s][A[A[A[A





pytorch_model-00001-of-00004.bin:   8%|████                                                 | 384M/4.98G [00:07<01:33, 49.3MB/s][A[A[A[A[A


pytorch_model-00002-of-00004.bin:   7%|███▉                                                 | 368M/5.00G [00:07<01:46, 43.6MB/s][A[A



pytorch_model-00004-of-00004.bin:  40%|█████████████████████                                | 464M/1.17G [00:07<00:12, 54.9MB/s][A[A[A




pytorch_model-00003-of-00004.bin:   9%|█████                                                | 464M/4.92G [00:07<01:09, 64.4MB/s][A[A[A[A





pytorch_model-00001-of-00004.bin:   8%|████▎                                                | 400M/4.98G [00:07<01:19, 57.6MB/s][A[A[A[A[A


pytorch_model-00002-of-00004.bin:   8%|████                                                 | 384M/5.00G [00:07<01:30, 51.1MB/s][A[A



pytorch_model-00004-of-00004.bin:  41%|█████████████████████▊                               | 480M/1.17G [00:07<00:11, 60.2MB/s][A[A[A




pytorch_model-00003-of-00004.bin:  10%|█████▏                                               | 480M/4.92G [00:07<01:03, 69.4MB/s][A[A[A[A





pytorch_model-00001-of-00004.bin:   8%|████▍                                                | 416M/4.98G [00:07<01:11, 63.4MB/s][A[A[A[A[A


pytorch_model-00002-of-00004.bin:   8%|████▏                                                | 400M/5.00G [00:08<01:26, 53.5MB/s][A[A



pytorch_model-00004-of-00004.bin:  42%|██████████████████████▌                              | 496M/1.17G [00:08<00:11, 59.7MB/s][A[A[A


pytorch_model-00002-of-00004.bin:   8%|████▍                                                | 416M/5.00G [00:08<01:16, 60.0MB/s][A[A





pytorch_model-00001-of-00004.bin:   9%|████▌                                                | 432M/4.98G [00:08<01:19, 57.5MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:  10%|█████▎                                               | 496M/4.92G [00:08<01:32, 47.9MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:   9%|████▌                                                | 432M/5.00G [00:08<01:09, 65.4MB/s][A[A





pytorch_model-00001-of-00004.bin:   9%|████▊                                                | 448M/4.98G [00:08<01:12, 62.6MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:  10%|█████▌                                               | 510M/4.92G [00:08<01:14, 59.5MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:   9%|████▋                                                | 445M/5.00G [00:08<01:01, 74.6MB/s][A[A


pytorch_model-00002-of-00004.bin:   9%|████▊                                                | 453M/5.00G [00:08<01:09, 65.2MB/s][A[A





pytorch_model-00001-of-00004.bin:   9%|████▉                                                | 464M/4.98G [00:08<01:13, 61.2MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:  11%|█████▌                                               | 519M/4.92G [00:08<01:28, 49.7MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:   9%|████▉                                                | 461M/5.00G [00:08<01:08, 66.5MB/s][A[A





pytorch_model-00001-of-00004.bin:  10%|█████                                                | 476M/4.98G [00:08<01:04, 70.2MB/s][A[A[A[A[A



pytorch_model-00004-of-00004.bin:  44%|███████████████████████▏                             | 512M/1.17G [00:08<00:18, 35.9MB/s][A[A[A




pytorch_model-00003-of-00004.bin:  11%|█████▋                                               | 528M/4.92G [00:08<01:27, 50.2MB/s][A[A[A[A





pytorch_model-00001-of-00004.bin:  10%|█████▏                                               | 484M/4.98G [00:09<01:11, 62.6MB/s][A[A[A[A[A


pytorch_model-00002-of-00004.bin:   9%|████▉                                                | 468M/5.00G [00:09<01:23, 54.0MB/s][A[A



pytorch_model-00004-of-00004.bin:  45%|███████████████████████▉                             | 528M/1.17G [00:09<00:16, 39.8MB/s][A[A[A





pytorch_model-00001-of-00004.bin:  10%|█████▎                                               | 496M/4.98G [00:09<01:15, 59.1MB/s][A[A[A[A[A


pytorch_model-00002-of-00004.bin:  10%|█████                                                | 480M/5.00G [00:09<01:30, 50.0MB/s][A[A





pytorch_model-00001-of-00004.bin:  10%|█████▍                                               | 512M/4.98G [00:09<01:08, 65.2MB/s][A[A[A[A[A


pytorch_model-00002-of-00004.bin:  10%|█████▎                                               | 496M/5.00G [00:09<01:17, 58.2MB/s][A[A





pytorch_model-00001-of-00004.bin:  11%|█████▌                                               | 528M/4.98G [00:09<00:55, 80.9MB/s][A[A[A[A[A



pytorch_model-00004-of-00004.bin:  47%|████████████████████████▋                            | 544M/1.17G [00:09<00:14, 42.4MB/s][A[A[A



pytorch_model-00004-of-00004.bin:  48%|█████████████████████████▍                           | 560M/1.17G [00:09<00:12, 49.1MB/s][A[A[A


pytorch_model-00002-of-00004.bin:  10%|█████▍                                               | 512M/5.00G [00:09<01:17, 57.5MB/s][A[A





pytorch_model-00001-of-00004.bin:  11%|█████▋                                               | 537M/4.98G [00:09<01:13, 60.7MB/s][A[A[A[A[A



pytorch_model-00004-of-00004.bin:  49%|██████████████████████████▏                          | 576M/1.17G [00:09<00:10, 54.5MB/s][A[A[A




pytorch_model-00003-of-00004.bin:  11%|█████▊                                               | 544M/4.92G [00:10<02:42, 26.9MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:  11%|█████▌                                               | 528M/5.00G [00:10<01:11, 62.3MB/s][A[A





pytorch_model-00001-of-00004.bin:  11%|█████▊                                               | 545M/4.98G [00:10<01:24, 52.7MB/s][A[A[A[A[A



pytorch_model-00004-of-00004.bin:  51%|██████████████████████████▊                          | 592M/1.17G [00:10<00:10, 56.1MB/s][A[A[A


pytorch_model-00002-of-00004.bin:  11%|█████▊                                               | 544M/5.00G [00:10<01:09, 64.6MB/s][A[A




pytorch_model-00003-of-00004.bin:  11%|██████                                               | 560M/4.92G [00:10<02:09, 33.6MB/s][A[A[A[A





pytorch_model-00001-of-00004.bin:  11%|█████▉                                               | 560M/4.98G [00:10<01:21, 54.2MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:  12%|██████▏                                              | 570M/4.92G [00:10<01:48, 39.9MB/s][A[A[A[A



pytorch_model-00004-of-00004.bin:  52%|███████████████████████████▌                         | 608M/1.17G [00:10<00:09, 59.4MB/s][A[A[A





pytorch_model-00001-of-00004.bin:  12%|██████▏                                              | 576M/4.98G [00:10<01:12, 60.4MB/s][A[A[A[A[A


pytorch_model-00002-of-00004.bin:  11%|█████▉                                               | 560M/5.00G [00:10<01:14, 59.8MB/s][A[A



pytorch_model-00004-of-00004.bin:  53%|███████████████████████████▉                         | 616M/1.17G [00:10<00:08, 61.9MB/s][A[A[A



pytorch_model-00004-of-00004.bin:  53%|████████████████████████████▎                        | 624M/1.17G [00:10<00:09, 56.1MB/s][A[A[A





pytorch_model-00001-of-00004.bin:  12%|██████▏                                              | 583M/4.98G [00:10<01:28, 49.7MB/s][A[A[A[A[A


pytorch_model-00002-of-00004.bin:  12%|██████                                               | 576M/5.00G [00:10<01:15, 58.9MB/s][A[A




pytorch_model-00003-of-00004.bin:  12%|██████▏                                              | 577M/4.92G [00:10<02:36, 27.7MB/s][A[A[A[A



pytorch_model-00004-of-00004.bin:  55%|█████████████████████████████                        | 640M/1.17G [00:10<00:08, 63.1MB/s][A[A[A


pytorch_model-00002-of-00004.bin:  12%|██████▎                                              | 592M/5.00G [00:11<01:12, 60.6MB/s][A[A



pytorch_model-00004-of-00004.bin:  56%|█████████████████████████████▊                       | 656M/1.17G [00:11<00:08, 61.3MB/s][A[A[A





pytorch_model-00001-of-00004.bin:  12%|██████▎                                              | 592M/4.98G [00:11<02:05, 35.0MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:  12%|██████▍                                              | 592M/4.92G [00:11<02:13, 32.3MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:  12%|██████▍                                              | 608M/5.00G [00:11<01:12, 60.6MB/s][A[A





pytorch_model-00001-of-00004.bin:  12%|██████▍                                              | 608M/4.98G [00:11<01:35, 45.9MB/s][A[A[A[A[A


pytorch_model-00002-of-00004.bin:  12%|██████▌                                              | 624M/5.00G [00:11<01:08, 63.7MB/s][A[A




pytorch_model-00003-of-00004.bin:  12%|██████▌                                              | 608M/4.92G [00:11<01:55, 37.3MB/s][A[A[A[A





pytorch_model-00001-of-00004.bin:  13%|██████▋                                              | 624M/4.98G [00:11<01:19, 54.5MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:  13%|██████▋                                              | 624M/4.92G [00:11<01:40, 42.6MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:  13%|██████▊                                              | 640M/5.00G [00:11<01:12, 60.4MB/s][A[A





pytorch_model-00001-of-00004.bin:  13%|██████▊                                              | 640M/4.98G [00:11<01:16, 56.9MB/s][A[A[A[A[A


pytorch_model-00002-of-00004.bin:  13%|██████▉                                              | 655M/5.00G [00:12<00:59, 73.5MB/s][A[A


pytorch_model-00002-of-00004.bin:  13%|███████                                              | 664M/5.00G [00:12<01:02, 69.9MB/s][A[A



pytorch_model-00004-of-00004.bin:  58%|██████████████████████████████▍                      | 672M/1.17G [00:12<00:14, 33.7MB/s][A[A[A





pytorch_model-00001-of-00004.bin:  13%|██████▉                                              | 656M/4.98G [00:12<01:15, 57.2MB/s][A[A[A[A[A


pytorch_model-00002-of-00004.bin:  13%|███████▏                                             | 672M/5.00G [00:12<01:12, 59.3MB/s][A[A



pytorch_model-00004-of-00004.bin:  59%|███████████████████████████████▏                     | 688M/1.17G [00:12<00:12, 39.4MB/s][A[A[A





pytorch_model-00001-of-00004.bin:  14%|███████▏                                             | 672M/4.98G [00:12<01:13, 58.6MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:  13%|██████▉                                              | 640M/4.92G [00:12<02:02, 34.8MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:  14%|███████▎                                             | 688M/5.00G [00:12<01:07, 64.3MB/s][A[A




pytorch_model-00003-of-00004.bin:  13%|███████                                              | 655M/4.92G [00:12<01:33, 45.5MB/s][A[A[A[A



pytorch_model-00004-of-00004.bin:  60%|███████████████████████████████▉                     | 704M/1.17G [00:12<00:09, 47.4MB/s][A[A[A





pytorch_model-00001-of-00004.bin:  14%|███████▎                                             | 688M/4.98G [00:12<01:08, 62.3MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:  13%|███████▏                                             | 663M/4.92G [00:12<01:35, 44.6MB/s][A[A[A[A



pytorch_model-00004-of-00004.bin:  62%|████████████████████████████████▋                    | 720M/1.17G [00:12<00:08, 53.5MB/s][A[A[A


pytorch_model-00002-of-00004.bin:  14%|███████▍                                             | 704M/5.00G [00:12<01:21, 52.9MB/s][A[A




pytorch_model-00003-of-00004.bin:  14%|███████▏                                             | 672M/4.92G [00:13<01:39, 42.7MB/s][A[A[A[A



pytorch_model-00004-of-00004.bin:  63%|█████████████████████████████████▍                   | 736M/1.17G [00:13<00:08, 51.7MB/s][A[A[A


pytorch_model-00002-of-00004.bin:  14%|███████▋                                             | 720M/5.00G [00:13<01:11, 60.1MB/s][A[A




pytorch_model-00003-of-00004.bin:  14%|███████▍                                             | 688M/4.92G [00:13<01:23, 50.7MB/s][A[A[A[A



pytorch_model-00004-of-00004.bin:  64%|██████████████████████████████████                   | 752M/1.17G [00:13<00:07, 57.0MB/s][A[A[A


pytorch_model-00002-of-00004.bin:  15%|███████▊                                             | 736M/5.00G [00:13<01:09, 61.4MB/s][A[A




pytorch_model-00003-of-00004.bin:  14%|███████▌                                             | 704M/4.92G [00:13<01:15, 55.6MB/s][A[A[A[A



pytorch_model-00004-of-00004.bin:  66%|██████████████████████████████████▊                  | 768M/1.17G [00:13<00:06, 59.1MB/s][A[A[A


pytorch_model-00002-of-00004.bin:  15%|███████▉                                             | 752M/5.00G [00:13<01:05, 65.3MB/s][A[A




pytorch_model-00003-of-00004.bin:  15%|███████▊                                             | 720M/4.92G [00:13<01:13, 57.1MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:  15%|████████▏                                            | 768M/5.00G [00:13<01:01, 68.9MB/s][A[A



pytorch_model-00004-of-00004.bin:  67%|███████████████████████████████████▌                 | 784M/1.17G [00:13<00:06, 60.4MB/s][A[A[A




pytorch_model-00003-of-00004.bin:  15%|███████▉                                             | 736M/4.92G [00:14<01:11, 58.3MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:  16%|████████▎                                            | 784M/5.00G [00:14<00:58, 71.9MB/s][A[A



pytorch_model-00004-of-00004.bin:  68%|████████████████████████████████████▎                | 800M/1.17G [00:14<00:05, 62.2MB/s][A[A[A




pytorch_model-00003-of-00004.bin:  15%|████████                                             | 752M/4.92G [00:14<01:06, 62.3MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:  16%|████████▍                                            | 800M/5.00G [00:14<00:59, 70.8MB/s][A[A



pytorch_model-00004-of-00004.bin:  70%|█████████████████████████████████████                | 816M/1.17G [00:14<00:06, 56.3MB/s][A[A[A


pytorch_model-00002-of-00004.bin:  16%|████████▋                                            | 816M/5.00G [00:14<01:02, 67.4MB/s][A[A




pytorch_model-00003-of-00004.bin:  16%|████████▎                                            | 768M/4.92G [00:14<01:16, 54.2MB/s][A[A[A[A



pytorch_model-00004-of-00004.bin:  71%|█████████████████████████████████████▋               | 832M/1.17G [00:14<00:06, 54.9MB/s][A[A[A


pytorch_model-00002-of-00004.bin:  17%|████████▊                                            | 832M/5.00G [00:14<01:04, 64.9MB/s][A[A




pytorch_model-00003-of-00004.bin:  16%|████████▍                                            | 784M/4.92G [00:14<01:07, 61.5MB/s][A[A[A[A




pytorch_model-00003-of-00004.bin:  16%|████████▋                                            | 800M/4.92G [00:14<01:01, 67.3MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:  17%|████████▉                                            | 848M/5.00G [00:15<01:01, 67.2MB/s][A[A



pytorch_model-00004-of-00004.bin:  73%|██████████████████████████████████████▍              | 848M/1.17G [00:15<00:05, 54.4MB/s][A[A[A


pytorch_model-00002-of-00004.bin:  17%|█████████▏                                           | 864M/5.00G [00:15<00:58, 70.2MB/s][A[A




pytorch_model-00003-of-00004.bin:  17%|████████▊                                            | 816M/4.92G [00:15<01:02, 65.9MB/s][A[A[A[A



pytorch_model-00004-of-00004.bin:  74%|███████████████████████████████████████▏             | 864M/1.17G [00:15<00:05, 59.3MB/s][A[A[A


pytorch_model-00002-of-00004.bin:  18%|█████████▎                                           | 879M/5.00G [00:15<00:49, 82.5MB/s][A[A




pytorch_model-00003-of-00004.bin:  17%|████████▉                                            | 832M/4.92G [00:15<00:58, 70.1MB/s][A[A[A[A



pytorch_model-00004-of-00004.bin:  75%|███████████████████████████████████████▉             | 880M/1.17G [00:15<00:04, 62.2MB/s][A[A[A


pytorch_model-00002-of-00004.bin:  18%|█████████▍                                           | 888M/5.00G [00:15<01:05, 62.6MB/s][A[A



pytorch_model-00004-of-00004.bin:  77%|████████████████████████████████████████▋            | 896M/1.17G [00:15<00:04, 67.4MB/s][A[A[A




pytorch_model-00003-of-00004.bin:  17%|█████████▏                                           | 848M/4.92G [00:15<01:03, 63.7MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:  18%|█████████▍                                           | 896M/5.00G [00:15<01:11, 57.4MB/s][A[A





pytorch_model-00001-of-00004.bin:  14%|███████▍                                             | 704M/4.98G [00:15<05:14, 13.6MB/s][A[A[A[A[A



pytorch_model-00004-of-00004.bin:  78%|█████████████████████████████████████████▍           | 912M/1.17G [00:15<00:03, 68.6MB/s][A[A[A




pytorch_model-00003-of-00004.bin:  18%|█████████▎                                           | 864M/4.92G [00:15<01:02, 65.2MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:  18%|█████████▋                                           | 912M/5.00G [00:16<01:05, 62.6MB/s][A[A





pytorch_model-00001-of-00004.bin:  14%|███████▋                                             | 720M/4.98G [00:16<03:55, 18.1MB/s][A[A[A[A[A



pytorch_model-00004-of-00004.bin:  79%|██████████████████████████████████████████           | 928M/1.17G [00:16<00:03, 68.3MB/s][A[A[A




pytorch_model-00003-of-00004.bin:  18%|█████████▍                                           | 880M/4.92G [00:16<01:00, 67.0MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:  19%|█████████▊                                           | 928M/5.00G [00:16<01:00, 66.8MB/s][A[A





pytorch_model-00001-of-00004.bin:  15%|███████▊                                             | 736M/4.98G [00:16<02:59, 23.6MB/s][A[A[A[A[A



pytorch_model-00004-of-00004.bin:  81%|██████████████████████████████████████████▊          | 944M/1.17G [00:16<00:03, 68.8MB/s][A[A[A




pytorch_model-00003-of-00004.bin:  18%|█████████▋                                           | 896M/4.92G [00:16<01:00, 66.7MB/s][A[A[A[A





pytorch_model-00001-of-00004.bin:  15%|████████                                             | 752M/4.98G [00:16<02:21, 29.8MB/s][A[A[A[A[A



pytorch_model-00004-of-00004.bin:  82%|███████████████████████████████████████████▌         | 960M/1.17G [00:16<00:02, 71.2MB/s][A[A[A




pytorch_model-00003-of-00004.bin:  19%|█████████▊                                           | 912M/4.92G [00:16<00:56, 70.9MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:  19%|██████████                                           | 944M/5.00G [00:16<01:23, 48.8MB/s][A[A



pytorch_model-00004-of-00004.bin:  84%|████████████████████████████████████████████▎        | 976M/1.17G [00:16<00:02, 71.6MB/s][A[A[A





pytorch_model-00001-of-00004.bin:  15%|████████▏                                            | 768M/4.98G [00:16<02:06, 33.3MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:  19%|██████████                                           | 928M/4.92G [00:16<00:59, 66.9MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:  19%|██████████▏                                          | 960M/5.00G [00:16<01:15, 53.8MB/s][A[A



pytorch_model-00004-of-00004.bin:  85%|█████████████████████████████████████████████        | 992M/1.17G [00:17<00:02, 75.1MB/s][A[A[A





pytorch_model-00001-of-00004.bin:  16%|████████▎                                            | 784M/4.98G [00:17<01:44, 40.3MB/s][A[A[A[A[A


pytorch_model-00002-of-00004.bin:  20%|██████████▎                                          | 976M/5.00G [00:17<01:05, 61.4MB/s][A[A



pytorch_model-00004-of-00004.bin:  86%|████████████████████████████████████████████▊       | 1.01G/1.17G [00:17<00:02, 68.7MB/s][A[A[A


pytorch_model-00002-of-00004.bin:  20%|██████████▌                                          | 992M/5.00G [00:17<01:00, 66.8MB/s][A[A





pytorch_model-00001-of-00004.bin:  16%|████████▌                                            | 800M/4.98G [00:17<01:32, 45.0MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:  19%|██████████▏                                          | 944M/4.92G [00:17<01:16, 51.7MB/s][A[A[A[A





pytorch_model-00001-of-00004.bin:  16%|████████▋                                            | 816M/4.98G [00:17<01:13, 56.9MB/s][A[A[A[A[A



pytorch_model-00004-of-00004.bin:  88%|█████████████████████████████████████████████▌      | 1.02G/1.17G [00:17<00:02, 70.9MB/s][A[A[A


pytorch_model-00002-of-00004.bin:  20%|██████████▍                                         | 1.01G/5.00G [00:17<00:57, 69.5MB/s][A[A




pytorch_model-00003-of-00004.bin:  20%|██████████▎                                          | 960M/4.92G [00:17<01:09, 57.2MB/s][A[A[A[A





pytorch_model-00001-of-00004.bin:  17%|████████▊                                            | 825M/4.98G [00:17<01:17, 53.7MB/s][A[A[A[A[A



pytorch_model-00004-of-00004.bin:  89%|██████████████████████████████████████████████▎     | 1.04G/1.17G [00:17<00:01, 71.9MB/s][A[A[A


pytorch_model-00002-of-00004.bin:  20%|██████████▋                                         | 1.02G/5.00G [00:17<00:56, 69.9MB/s][A[A




pytorch_model-00003-of-00004.bin:  20%|██████████▌                                          | 976M/4.92G [00:17<01:02, 62.8MB/s][A[A[A[A





pytorch_model-00001-of-00004.bin:  17%|████████▊                                            | 833M/4.98G [00:17<01:26, 48.0MB/s][A[A[A[A[A



pytorch_model-00004-of-00004.bin:  90%|███████████████████████████████████████████████     | 1.06G/1.17G [00:17<00:01, 70.3MB/s][A[A[A


pytorch_model-00002-of-00004.bin:  21%|██████████▊                                         | 1.04G/5.00G [00:18<00:57, 68.6MB/s][A[A




pytorch_model-00003-of-00004.bin:  20%|██████████▋                                          | 992M/4.92G [00:18<01:01, 63.6MB/s][A[A[A[A



pytorch_model-00004-of-00004.bin:  92%|███████████████████████████████████████████████▋    | 1.07G/1.17G [00:18<00:01, 72.3MB/s][A[A[A





pytorch_model-00001-of-00004.bin:  17%|█████████                                            | 848M/4.98G [00:18<01:21, 50.6MB/s][A[A[A[A[A


pytorch_model-00002-of-00004.bin:  21%|██████████▉                                         | 1.06G/5.00G [00:18<00:56, 70.0MB/s][A[A




pytorch_model-00003-of-00004.bin:  21%|██████████▋                                         | 1.01G/4.92G [00:18<01:02, 62.8MB/s][A[A[A[A





pytorch_model-00001-of-00004.bin:  17%|█████████▏                                           | 864M/4.98G [00:18<01:12, 57.0MB/s][A[A[A[A[A


pytorch_model-00002-of-00004.bin:  21%|███████████▏                                        | 1.07G/5.00G [00:18<00:55, 71.4MB/s][A[A




pytorch_model-00003-of-00004.bin:  21%|██████████▊                                         | 1.02G/4.92G [00:18<01:00, 64.8MB/s][A[A[A[A



pytorch_model-00004-of-00004.bin:  93%|████████████████████████████████████████████████▍   | 1.09G/1.17G [00:18<00:01, 56.4MB/s][A[A[A





pytorch_model-00001-of-00004.bin:  18%|█████████▎                                           | 880M/4.98G [00:18<01:05, 62.5MB/s][A[A[A[A[A


pytorch_model-00002-of-00004.bin:  22%|███████████▎                                        | 1.09G/5.00G [00:18<00:54, 72.4MB/s][A[A




pytorch_model-00003-of-00004.bin:  21%|███████████                                         | 1.04G/4.92G [00:18<00:58, 66.0MB/s][A[A[A[A



pytorch_model-00004-of-00004.bin:  95%|█████████████████████████████████████████████████▏  | 1.10G/1.17G [00:18<00:01, 63.3MB/s][A[A[A





pytorch_model-00001-of-00004.bin:  18%|█████████▌                                           | 896M/4.98G [00:18<00:59, 68.5MB/s][A[A[A[A[A


pytorch_model-00002-of-00004.bin:  22%|███████████▍                                        | 1.10G/5.00G [00:18<00:57, 67.6MB/s][A[A




pytorch_model-00003-of-00004.bin:  21%|███████████▏                                        | 1.06G/4.92G [00:18<00:55, 69.1MB/s][A[A[A[A





pytorch_model-00001-of-00004.bin:  18%|█████████▋                                           | 912M/4.98G [00:19<00:58, 69.3MB/s][A[A[A[A[A



pytorch_model-00004-of-00004.bin:  96%|█████████████████████████████████████████████████▊  | 1.12G/1.17G [00:19<00:00, 63.7MB/s][A[A[A




pytorch_model-00003-of-00004.bin:  22%|███████████▎                                        | 1.07G/4.92G [00:19<00:53, 71.6MB/s][A[A[A[A



pytorch_model-00004-of-00004.bin:  97%|██████████████████████████████████████████████████▌ | 1.14G/1.17G [00:19<00:00, 66.4MB/s][A[A[A





pytorch_model-00001-of-00004.bin:  19%|█████████▉                                           | 928M/4.98G [00:19<01:04, 62.9MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:  22%|███████████▌                                        | 1.09G/4.92G [00:19<00:52, 72.4MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:  22%|███████████▋                                        | 1.12G/5.00G [00:19<01:14, 52.3MB/s][A[A



pytorch_model-00004-of-00004.bin:  99%|███████████████████████████████████████████████████▎| 1.15G/1.17G [00:19<00:00, 68.5MB/s][A[A[A





pytorch_model-00001-of-00004.bin:  19%|██████████                                           | 944M/4.98G [00:19<01:04, 62.9MB/s][A[A[A[A[A



pytorch_model-00004-of-00004.bin: 100%|███████████████████████████████████████████████████▉| 1.17G/1.17G [00:19<00:00, 69.9MB/s][A[A[A




pytorch_model-00003-of-00004.bin:  22%|███████████▋                                        | 1.10G/4.92G [00:19<00:58, 64.7MB/s][A[A[A[A
pytorch_model-00004-of-00004.bin: 100%|████████████████████████████████████████████████████| 1.17G/1.17G [00:19<00:00, 59.1MB/s]






pytorch_model-00001-of-00004.bin:  19%|██████████▏                                          | 960M/4.98G [00:19<01:01, 64.9MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:  23%|███████████▊                                        | 1.12G/4.92G [00:19<00:54, 70.3MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:  23%|███████████▊                                        | 1.14G/5.00G [00:20<01:36, 40.2MB/s][A[A





pytorch_model-00001-of-00004.bin:  20%|██████████▍                                          | 976M/4.98G [00:20<00:58, 67.9MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:  23%|████████████                                        | 1.14G/4.92G [00:20<00:54, 69.6MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:  23%|███████████▉                                        | 1.15G/5.00G [00:20<01:21, 47.0MB/s][A[A





pytorch_model-00001-of-00004.bin:  20%|██████████▌                                          | 992M/4.98G [00:20<00:59, 67.5MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:  23%|████████████▏                                       | 1.15G/4.92G [00:20<00:52, 71.7MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:  23%|████████████▏                                       | 1.17G/5.00G [00:20<01:11, 53.8MB/s][A[A





pytorch_model-00001-of-00004.bin:  20%|██████████▌                                         | 1.01G/4.98G [00:20<00:58, 67.5MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:  24%|████████████▎                                       | 1.17G/4.92G [00:20<00:51, 72.5MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:  24%|████████████▎                                       | 1.18G/5.00G [00:20<01:03, 59.8MB/s][A[A





pytorch_model-00001-of-00004.bin:  21%|██████████▋                                         | 1.02G/4.98G [00:20<00:56, 70.3MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:  24%|████████████▌                                       | 1.18G/4.92G [00:20<00:53, 70.1MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:  24%|████████████▍                                       | 1.20G/5.00G [00:20<01:01, 61.3MB/s][A[A





pytorch_model-00001-of-00004.bin:  21%|██████████▊                                         | 1.04G/4.98G [00:20<00:56, 70.0MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:  24%|████████████▋                                       | 1.20G/4.92G [00:21<00:55, 67.2MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:  24%|████████████▋                                       | 1.22G/5.00G [00:21<01:02, 60.8MB/s][A[A





pytorch_model-00001-of-00004.bin:  21%|███████████                                         | 1.06G/4.98G [00:21<01:00, 64.3MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:  25%|████████████▊                                       | 1.22G/4.92G [00:21<00:58, 62.7MB/s][A[A[A[A





pytorch_model-00001-of-00004.bin:  22%|███████████▏                                        | 1.07G/4.98G [00:21<01:02, 62.6MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:  25%|█████████████                                       | 1.23G/4.92G [00:21<00:56, 64.9MB/s][A[A[A[A




pytorch_model-00003-of-00004.bin:  25%|█████████████▏                                      | 1.25G/4.92G [00:21<00:52, 69.7MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:  25%|████████████▊                                       | 1.23G/5.00G [00:21<01:27, 42.9MB/s][A[A





pytorch_model-00001-of-00004.bin:  22%|███████████▎                                        | 1.09G/4.98G [00:21<01:08, 56.6MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:  26%|█████████████▎                                      | 1.26G/4.92G [00:21<00:51, 70.8MB/s][A[A[A[A





pytorch_model-00001-of-00004.bin:  22%|███████████▌                                        | 1.10G/4.98G [00:22<01:03, 60.8MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:  26%|█████████████▌                                      | 1.28G/4.92G [00:22<00:43, 83.8MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:  25%|████████████▉                                       | 1.25G/5.00G [00:22<01:28, 42.5MB/s][A[A





pytorch_model-00001-of-00004.bin:  23%|███████████▋                                        | 1.12G/4.98G [00:22<00:59, 64.5MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:  26%|█████████████▋                                      | 1.29G/4.92G [00:22<00:54, 66.2MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:  25%|█████████████▏                                      | 1.26G/5.00G [00:22<01:15, 49.8MB/s][A[A





pytorch_model-00001-of-00004.bin:  23%|███████████▊                                        | 1.14G/4.98G [00:22<00:57, 66.4MB/s][A[A[A[A[A


pytorch_model-00002-of-00004.bin:  26%|█████████████▎                                      | 1.28G/5.00G [00:22<01:08, 54.4MB/s][A[A




pytorch_model-00003-of-00004.bin:  26%|█████████████▋                                      | 1.30G/4.92G [00:22<01:09, 52.0MB/s][A[A[A[A





pytorch_model-00001-of-00004.bin:  23%|████████████                                        | 1.15G/4.98G [00:22<00:57, 66.5MB/s][A[A[A[A[A


pytorch_model-00002-of-00004.bin:  26%|█████████████▍                                      | 1.30G/5.00G [00:22<01:09, 53.3MB/s][A[A


pytorch_model-00002-of-00004.bin:  26%|█████████████▋                                      | 1.31G/5.00G [00:23<01:03, 58.3MB/s][A[A





pytorch_model-00001-of-00004.bin:  23%|████████████▏                                       | 1.17G/4.98G [00:23<01:06, 57.4MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:  27%|█████████████▉                                      | 1.31G/4.92G [00:23<01:26, 41.8MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:  27%|█████████████▊                                      | 1.33G/5.00G [00:23<00:53, 68.1MB/s][A[A





pytorch_model-00001-of-00004.bin:  24%|████████████▎                                       | 1.18G/4.98G [00:23<00:54, 69.0MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:  27%|█████████████▉                                      | 1.32G/4.92G [00:23<01:12, 49.5MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:  27%|█████████████▊                                      | 1.33G/5.00G [00:23<00:58, 63.1MB/s][A[A





pytorch_model-00001-of-00004.bin:  24%|████████████▍                                       | 1.19G/4.98G [00:23<00:59, 63.3MB/s][A[A[A[A[A





pytorch_model-00001-of-00004.bin:  24%|████████████▌                                       | 1.20G/4.98G [00:23<01:05, 57.4MB/s][A[A[A[A[A


pytorch_model-00002-of-00004.bin:  27%|█████████████▉                                      | 1.34G/5.00G [00:23<01:05, 56.1MB/s][A[A




pytorch_model-00003-of-00004.bin:  27%|██████████████                                      | 1.33G/4.92G [00:23<01:41, 35.2MB/s][A[A[A[A





pytorch_model-00001-of-00004.bin:  24%|████████████▋                                       | 1.22G/4.98G [00:23<00:58, 64.0MB/s][A[A[A[A[A


pytorch_model-00002-of-00004.bin:  27%|██████████████▏                                     | 1.36G/5.00G [00:23<00:59, 61.6MB/s][A[A


pytorch_model-00002-of-00004.bin:  28%|██████████████▎                                     | 1.38G/5.00G [00:24<00:51, 70.4MB/s][A[A


pytorch_model-00002-of-00004.bin:  28%|██████████████▍                                     | 1.39G/5.00G [00:24<00:48, 73.9MB/s][A[A





pytorch_model-00001-of-00004.bin:  25%|████████████▊                                       | 1.23G/4.98G [00:24<01:15, 49.6MB/s][A[A[A[A[A


pytorch_model-00002-of-00004.bin:  28%|██████████████▋                                     | 1.41G/5.00G [00:24<00:48, 73.9MB/s][A[A





pytorch_model-00001-of-00004.bin:  25%|█████████████                                       | 1.25G/4.98G [00:24<01:08, 54.6MB/s][A[A[A[A[A


pytorch_model-00002-of-00004.bin:  28%|██████████████▊                                     | 1.42G/5.00G [00:24<00:49, 72.0MB/s][A[A





pytorch_model-00001-of-00004.bin:  25%|█████████████▏                                      | 1.26G/4.98G [00:24<01:00, 61.2MB/s][A[A[A[A[A





pytorch_model-00001-of-00004.bin:  26%|█████████████▎                                      | 1.28G/4.98G [00:24<00:57, 64.4MB/s][A[A[A[A[A


pytorch_model-00002-of-00004.bin:  29%|██████████████▉                                     | 1.44G/5.00G [00:24<00:52, 67.5MB/s][A[A


pytorch_model-00002-of-00004.bin:  29%|███████████████▏                                    | 1.46G/5.00G [00:25<00:52, 68.1MB/s][A[A




pytorch_model-00003-of-00004.bin:  27%|██████████████▏                                     | 1.34G/4.92G [00:25<03:27, 17.2MB/s][A[A[A[A





pytorch_model-00001-of-00004.bin:  26%|█████████████▌                                      | 1.30G/4.98G [00:25<01:03, 58.3MB/s][A[A[A[A[A


pytorch_model-00002-of-00004.bin:  29%|███████████████▎                                    | 1.47G/5.00G [00:25<00:49, 71.2MB/s][A[A





pytorch_model-00001-of-00004.bin:  26%|█████████████▋                                      | 1.31G/4.98G [00:25<00:57, 64.2MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:  28%|██████████████▍                                     | 1.36G/4.92G [00:25<02:34, 23.1MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:  30%|███████████████▍                                    | 1.49G/5.00G [00:25<00:46, 74.8MB/s][A[A





pytorch_model-00001-of-00004.bin:  27%|█████████████▉                                      | 1.33G/4.98G [00:25<00:55, 66.0MB/s][A[A[A[A[A


pytorch_model-00002-of-00004.bin:  30%|███████████████▋                                    | 1.50G/5.00G [00:25<00:48, 71.9MB/s][A[A




pytorch_model-00003-of-00004.bin:  28%|██████████████▌                                     | 1.38G/4.92G [00:25<02:03, 28.7MB/s][A[A[A[A





pytorch_model-00001-of-00004.bin:  27%|██████████████                                      | 1.34G/4.98G [00:25<00:54, 67.0MB/s][A[A[A[A[A


pytorch_model-00002-of-00004.bin:  30%|███████████████▊                                    | 1.52G/5.00G [00:25<00:46, 75.4MB/s][A[A




pytorch_model-00003-of-00004.bin:  28%|██████████████▋                                     | 1.39G/4.92G [00:25<01:34, 37.4MB/s][A[A[A[A





pytorch_model-00001-of-00004.bin:  27%|██████████████▏                                     | 1.36G/4.98G [00:26<00:55, 65.6MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:  29%|██████████████▉                                     | 1.41G/4.92G [00:26<01:17, 45.1MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:  31%|███████████████▉                                    | 1.54G/5.00G [00:26<00:47, 72.9MB/s][A[A





pytorch_model-00001-of-00004.bin:  28%|██████████████▍                                     | 1.38G/4.98G [00:26<00:54, 66.0MB/s][A[A[A[A[A


pytorch_model-00002-of-00004.bin:  31%|████████████████▏                                   | 1.55G/5.00G [00:26<00:48, 71.1MB/s][A[A


pytorch_model-00002-of-00004.bin:  31%|████████████████▎                                   | 1.57G/5.00G [00:26<00:47, 72.9MB/s][A[A





pytorch_model-00001-of-00004.bin:  28%|██████████████▌                                     | 1.39G/4.98G [00:26<00:59, 60.1MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:  29%|███████████████                                     | 1.42G/4.92G [00:26<01:33, 37.3MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:  32%|████████████████▍                                   | 1.58G/5.00G [00:26<00:48, 71.0MB/s][A[A





pytorch_model-00001-of-00004.bin:  28%|██████████████▋                                     | 1.41G/4.98G [00:26<00:56, 63.7MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:  29%|███████████████▏                                    | 1.44G/4.92G [00:27<01:22, 42.2MB/s][A[A[A[A




pytorch_model-00003-of-00004.bin:  30%|███████████████▍                                    | 1.46G/4.92G [00:27<01:12, 47.9MB/s][A[A[A[A





pytorch_model-00001-of-00004.bin:  29%|██████████████▉                                     | 1.42G/4.98G [00:27<01:15, 47.3MB/s][A[A[A[A[A


pytorch_model-00002-of-00004.bin:  32%|████████████████▋                                   | 1.60G/5.00G [00:27<01:10, 48.2MB/s][A[A




pytorch_model-00003-of-00004.bin:  30%|███████████████▌                                    | 1.47G/4.92G [00:27<01:03, 54.3MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:  32%|████████████████▋                                   | 1.61G/5.00G [00:27<01:09, 48.9MB/s][A[A





pytorch_model-00001-of-00004.bin:  29%|███████████████                                     | 1.44G/4.98G [00:27<01:08, 51.7MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:  30%|███████████████▋                                    | 1.49G/4.92G [00:27<00:59, 57.2MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:  32%|████████████████▊                                   | 1.62G/5.00G [00:27<01:10, 48.0MB/s][A[A





pytorch_model-00001-of-00004.bin:  29%|███████████████▏                                    | 1.46G/4.98G [00:27<01:00, 58.7MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:  31%|███████████████▉                                    | 1.50G/4.92G [00:27<00:56, 60.0MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:  33%|████████████████▉                                   | 1.63G/5.00G [00:28<01:05, 51.4MB/s][A[A





pytorch_model-00001-of-00004.bin:  30%|███████████████▍                                    | 1.47G/4.98G [00:28<00:56, 62.2MB/s][A[A[A[A[A





pytorch_model-00001-of-00004.bin:  30%|███████████████▌                                    | 1.49G/4.98G [00:28<00:52, 66.9MB/s][A[A[A[A[A


pytorch_model-00002-of-00004.bin:  33%|█████████████████▏                                  | 1.65G/5.00G [00:28<01:04, 52.2MB/s][A[A




pytorch_model-00003-of-00004.bin:  31%|████████████████                                    | 1.52G/4.92G [00:28<01:11, 47.8MB/s][A[A[A[A





pytorch_model-00001-of-00004.bin:  30%|███████████████▋                                    | 1.50G/4.98G [00:28<00:48, 71.9MB/s][A[A[A[A[A


pytorch_model-00002-of-00004.bin:  33%|█████████████████▎                                  | 1.66G/5.00G [00:28<00:56, 58.8MB/s][A[A




pytorch_model-00003-of-00004.bin:  31%|████████████████▏                                   | 1.54G/4.92G [00:28<01:02, 54.1MB/s][A[A[A[A





pytorch_model-00001-of-00004.bin:  31%|███████████████▉                                    | 1.52G/4.98G [00:28<00:46, 73.8MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:  32%|████████████████▍                                   | 1.55G/4.92G [00:28<00:57, 58.8MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:  34%|█████████████████▍                                  | 1.68G/5.00G [00:28<01:04, 51.1MB/s][A[A




pytorch_model-00003-of-00004.bin:  32%|████████████████▌                                   | 1.57G/4.92G [00:29<00:54, 61.5MB/s][A[A[A[A





pytorch_model-00001-of-00004.bin:  31%|████████████████                                    | 1.54G/4.98G [00:29<01:01, 56.1MB/s][A[A[A[A[A


pytorch_model-00002-of-00004.bin:  34%|█████████████████▋                                  | 1.70G/5.00G [00:29<00:59, 55.9MB/s][A[A




pytorch_model-00003-of-00004.bin:  32%|████████████████▊                                   | 1.58G/4.92G [00:29<00:51, 64.8MB/s][A[A[A[A





pytorch_model-00001-of-00004.bin:  31%|████████████████▏                                   | 1.55G/4.98G [00:29<00:55, 61.5MB/s][A[A[A[A[A


pytorch_model-00002-of-00004.bin:  34%|█████████████████▊                                  | 1.71G/5.00G [00:29<01:00, 54.6MB/s][A[A





pytorch_model-00001-of-00004.bin:  32%|████████████████▍                                   | 1.57G/4.98G [00:29<00:52, 65.3MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:  33%|████████████████▉                                   | 1.60G/4.92G [00:29<00:53, 62.6MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:  35%|█████████████████▉                                  | 1.73G/5.00G [00:29<00:53, 60.9MB/s][A[A





pytorch_model-00001-of-00004.bin:  32%|████████████████▌                                   | 1.58G/4.98G [00:29<00:53, 63.2MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:  33%|█████████████████                                   | 1.62G/4.92G [00:29<00:51, 63.7MB/s][A[A[A[A




pytorch_model-00003-of-00004.bin:  33%|█████████████████▎                                  | 1.63G/4.92G [00:30<00:48, 68.2MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:  35%|██████████████████▏                                 | 1.74G/5.00G [00:30<00:59, 54.7MB/s][A[A





pytorch_model-00001-of-00004.bin:  32%|████████████████▋                                   | 1.60G/4.98G [00:30<00:56, 59.8MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:  34%|█████████████████▍                                  | 1.65G/4.92G [00:30<00:46, 70.3MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:  35%|██████████████████▎                                 | 1.76G/5.00G [00:30<00:52, 61.3MB/s][A[A




pytorch_model-00003-of-00004.bin:  34%|█████████████████▌                                  | 1.66G/4.92G [00:30<00:47, 68.7MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:  36%|██████████████████▍                                 | 1.78G/5.00G [00:30<00:51, 62.6MB/s][A[A





pytorch_model-00001-of-00004.bin:  32%|████████████████▉                                   | 1.62G/4.98G [00:30<01:04, 51.8MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:  34%|█████████████████▊                                  | 1.68G/4.92G [00:30<00:45, 70.7MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:  36%|██████████████████▋                                 | 1.79G/5.00G [00:30<00:51, 61.9MB/s][A[A





pytorch_model-00001-of-00004.bin:  33%|█████████████████                                   | 1.63G/4.98G [00:30<00:59, 56.2MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:  35%|█████████████████▉                                  | 1.70G/4.92G [00:30<00:45, 71.3MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:  36%|██████████████████▊                                 | 1.81G/5.00G [00:30<00:46, 68.4MB/s][A[A





pytorch_model-00001-of-00004.bin:  33%|█████████████████▏                                  | 1.65G/4.98G [00:30<00:54, 61.1MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:  35%|██████████████████                                  | 1.71G/4.92G [00:31<00:46, 68.9MB/s][A[A[A[A





pytorch_model-00001-of-00004.bin:  33%|█████████████████▍                                  | 1.66G/4.98G [00:31<00:50, 65.2MB/s][A[A[A[A[A


pytorch_model-00002-of-00004.bin:  36%|██████████████████▉                                 | 1.82G/5.00G [00:31<00:47, 67.1MB/s][A[A





pytorch_model-00001-of-00004.bin:  34%|█████████████████▌                                  | 1.68G/4.98G [00:31<00:51, 63.8MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:  35%|██████████████████▎                                 | 1.73G/4.92G [00:31<00:49, 64.7MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:  37%|███████████████████▏                                | 1.84G/5.00G [00:31<00:59, 52.9MB/s][A[A




pytorch_model-00003-of-00004.bin:  35%|██████████████████▍                                 | 1.74G/4.92G [00:31<00:46, 68.3MB/s][A[A[A[A




pytorch_model-00003-of-00004.bin:  36%|██████████████████▌                                 | 1.76G/4.92G [00:31<00:46, 67.4MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:  37%|███████████████████▎                                | 1.86G/5.00G [00:31<00:59, 53.2MB/s][A[A


pytorch_model-00002-of-00004.bin:  37%|███████████████████▍                                | 1.87G/5.00G [00:32<00:53, 58.0MB/s][A[A




pytorch_model-00003-of-00004.bin:  36%|██████████████████▊                                 | 1.78G/4.92G [00:32<00:51, 60.7MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:  38%|███████████████████▋                                | 1.89G/5.00G [00:32<00:53, 58.3MB/s][A[A




pytorch_model-00003-of-00004.bin:  36%|██████████████████▉                                 | 1.79G/4.92G [00:32<00:48, 64.7MB/s][A[A[A[A





pytorch_model-00001-of-00004.bin:  34%|█████████████████▋                                  | 1.70G/4.98G [00:32<01:45, 31.1MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:  37%|███████████████████                                 | 1.81G/4.92G [00:32<00:47, 65.8MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:  38%|███████████████████▊                                | 1.90G/5.00G [00:32<00:57, 54.0MB/s][A[A





pytorch_model-00001-of-00004.bin:  34%|█████████████████▉                                  | 1.71G/4.98G [00:32<01:32, 35.4MB/s][A[A[A[A[A


pytorch_model-00002-of-00004.bin:  38%|███████████████████▉                                | 1.92G/5.00G [00:33<00:53, 57.1MB/s][A[A




pytorch_model-00003-of-00004.bin:  37%|███████████████████▎                                | 1.82G/4.92G [00:33<00:55, 55.5MB/s][A[A[A[A





pytorch_model-00001-of-00004.bin:  35%|██████████████████                                  | 1.73G/4.98G [00:33<01:24, 38.6MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:  37%|███████████████████▍                                | 1.84G/4.92G [00:33<00:50, 61.1MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:  39%|████████████████████▏                               | 1.94G/5.00G [00:33<00:52, 58.7MB/s][A[A





pytorch_model-00001-of-00004.bin:  35%|██████████████████▏                                 | 1.74G/4.98G [00:33<01:09, 46.4MB/s][A[A[A[A[A


pytorch_model-00002-of-00004.bin:  39%|████████████████████▎                               | 1.95G/5.00G [00:33<00:47, 64.0MB/s][A[A




pytorch_model-00003-of-00004.bin:  38%|███████████████████▋                                | 1.86G/4.92G [00:33<00:47, 64.1MB/s][A[A[A[A





pytorch_model-00001-of-00004.bin:  35%|██████████████████▍                                 | 1.76G/4.98G [00:33<00:59, 53.8MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:  38%|███████████████████▊                                | 1.87G/4.92G [00:33<00:46, 65.7MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:  39%|████████████████████▍                               | 1.97G/5.00G [00:33<00:47, 63.8MB/s][A[A





pytorch_model-00001-of-00004.bin:  36%|██████████████████▌                                 | 1.78G/4.98G [00:33<00:58, 54.3MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:  38%|███████████████████▉                                | 1.89G/4.92G [00:33<00:45, 66.1MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:  40%|████████████████████▋                               | 1.98G/5.00G [00:33<00:47, 64.2MB/s][A[A





pytorch_model-00001-of-00004.bin:  36%|██████████████████▋                                 | 1.79G/4.98G [00:34<00:50, 62.8MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:  39%|████████████████████▏                               | 1.90G/4.92G [00:34<00:44, 67.5MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:  40%|████████████████████▊                               | 2.00G/5.00G [00:34<00:45, 66.0MB/s][A[A





pytorch_model-00001-of-00004.bin:  36%|██████████████████▉                                 | 1.81G/4.98G [00:34<00:55, 57.2MB/s][A[A[A[A[A


pytorch_model-00002-of-00004.bin:  40%|████████████████████▉                               | 2.02G/5.00G [00:34<00:44, 66.8MB/s][A[A




pytorch_model-00003-of-00004.bin:  39%|████████████████████▎                               | 1.92G/4.92G [00:34<00:46, 65.1MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:  41%|█████████████████████▏                              | 2.03G/5.00G [00:34<00:43, 67.6MB/s][A[A




pytorch_model-00003-of-00004.bin:  39%|████████████████████▍                               | 1.94G/4.92G [00:34<00:44, 66.5MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:  41%|█████████████████████▎                              | 2.05G/5.00G [00:34<00:41, 70.6MB/s][A[A


pytorch_model-00002-of-00004.bin:  41%|█████████████████████▍                              | 2.06G/5.00G [00:35<00:41, 71.6MB/s][A[A





pytorch_model-00001-of-00004.bin:  37%|███████████████████                                 | 1.82G/4.98G [00:35<01:23, 37.9MB/s][A[A[A[A[A


pytorch_model-00002-of-00004.bin:  42%|█████████████████████▋                              | 2.08G/5.00G [00:35<00:40, 72.6MB/s][A[A





pytorch_model-00001-of-00004.bin:  37%|███████████████████▏                                | 1.84G/4.98G [00:35<01:12, 43.0MB/s][A[A[A[A[A


pytorch_model-00002-of-00004.bin:  42%|█████████████████████▊                              | 2.10G/5.00G [00:35<00:39, 74.2MB/s][A[A





pytorch_model-00001-of-00004.bin:  37%|███████████████████▍                                | 1.86G/4.98G [00:35<01:08, 45.7MB/s][A[A[A[A[A


pytorch_model-00002-of-00004.bin:  42%|█████████████████████▉                              | 2.11G/5.00G [00:35<00:38, 74.7MB/s][A[A





pytorch_model-00001-of-00004.bin:  38%|███████████████████▌                                | 1.87G/4.98G [00:35<00:58, 53.0MB/s][A[A[A[A[A


pytorch_model-00002-of-00004.bin:  43%|██████████████████████▏                             | 2.13G/5.00G [00:35<00:36, 79.0MB/s][A[A




pytorch_model-00003-of-00004.bin:  40%|████████████████████▋                               | 1.95G/4.92G [00:36<01:47, 27.5MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:  43%|██████████████████████▎                             | 2.14G/5.00G [00:36<00:36, 78.4MB/s][A[A





pytorch_model-00001-of-00004.bin:  38%|███████████████████▋                                | 1.89G/4.98G [00:36<00:57, 53.3MB/s][A[A[A[A[A


pytorch_model-00002-of-00004.bin:  43%|██████████████████████▍                             | 2.16G/5.00G [00:36<00:36, 78.1MB/s][A[A




pytorch_model-00003-of-00004.bin:  40%|████████████████████▊                               | 1.97G/4.92G [00:36<01:29, 32.8MB/s][A[A[A[A





pytorch_model-00001-of-00004.bin:  38%|███████████████████▉                                | 1.90G/4.98G [00:36<00:51, 59.4MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:  40%|████████████████████▉                               | 1.98G/4.92G [00:36<01:12, 40.3MB/s][A[A[A[A





pytorch_model-00001-of-00004.bin:  39%|████████████████████                                | 1.92G/4.98G [00:36<00:47, 65.0MB/s][A[A[A[A[A


pytorch_model-00002-of-00004.bin:  44%|██████████████████████▋                             | 2.18G/5.00G [00:36<00:44, 63.1MB/s][A[A




pytorch_model-00003-of-00004.bin:  41%|█████████████████████▏                              | 2.00G/4.92G [00:36<01:03, 46.2MB/s][A[A[A[A





pytorch_model-00001-of-00004.bin:  39%|████████████████████▏                               | 1.94G/4.98G [00:36<00:44, 68.5MB/s][A[A[A[A[A


pytorch_model-00002-of-00004.bin:  44%|██████████████████████▊                             | 2.19G/5.00G [00:37<00:50, 55.1MB/s][A[A





pytorch_model-00001-of-00004.bin:  39%|████████████████████▍                               | 1.95G/4.98G [00:37<00:47, 63.2MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:  41%|█████████████████████▎                              | 2.02G/4.92G [00:37<01:08, 42.2MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:  44%|██████████████████████▉                             | 2.21G/5.00G [00:37<00:46, 60.7MB/s][A[A





pytorch_model-00001-of-00004.bin:  40%|████████████████████▌                               | 1.97G/4.98G [00:37<00:48, 61.8MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:  41%|█████████████████████▍                              | 2.03G/4.92G [00:37<00:58, 49.6MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:  44%|███████████████████████▏                            | 2.22G/5.00G [00:37<00:43, 63.8MB/s][A[A




pytorch_model-00003-of-00004.bin:  42%|█████████████████████▋                              | 2.05G/4.92G [00:37<00:51, 55.8MB/s][A[A[A[A





pytorch_model-00001-of-00004.bin:  40%|████████████████████▋                               | 1.98G/4.98G [00:37<00:55, 53.6MB/s][A[A[A[A[A


pytorch_model-00002-of-00004.bin:  45%|███████████████████████▎                            | 2.24G/5.00G [00:37<00:44, 61.5MB/s][A[A




pytorch_model-00003-of-00004.bin:  42%|█████████████████████▊                              | 2.06G/4.92G [00:37<00:45, 62.0MB/s][A[A[A[A




pytorch_model-00003-of-00004.bin:  42%|██████████████████████                              | 2.08G/4.92G [00:37<00:44, 64.1MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:  45%|███████████████████████▍                            | 2.26G/5.00G [00:38<00:45, 60.7MB/s][A[A





pytorch_model-00001-of-00004.bin:  40%|████████████████████▉                               | 2.00G/4.98G [00:38<01:00, 49.2MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:  43%|██████████████████████▏                             | 2.10G/4.92G [00:38<00:40, 69.0MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:  45%|███████████████████████▋                            | 2.27G/5.00G [00:38<00:41, 66.1MB/s][A[A





pytorch_model-00001-of-00004.bin:  41%|█████████████████████                               | 2.02G/4.98G [00:38<00:53, 55.0MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:  43%|██████████████████████▎                             | 2.11G/4.92G [00:38<00:39, 71.1MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:  46%|███████████████████████▊                            | 2.29G/5.00G [00:38<00:43, 62.7MB/s][A[A





pytorch_model-00001-of-00004.bin:  41%|█████████████████████▏                              | 2.03G/4.98G [00:38<00:54, 54.2MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:  43%|██████████████████████▌                             | 2.13G/4.92G [00:38<00:39, 70.0MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:  46%|███████████████████████▉                            | 2.30G/5.00G [00:38<00:40, 66.2MB/s][A[A





pytorch_model-00001-of-00004.bin:  41%|█████████████████████▎                              | 2.04G/4.98G [00:38<00:46, 63.1MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:  44%|██████████████████████▋                             | 2.14G/4.92G [00:38<00:41, 66.5MB/s][A[A[A[A





pytorch_model-00001-of-00004.bin:  41%|█████████████████████▍                              | 2.05G/4.98G [00:38<00:51, 57.3MB/s][A[A[A[A[A


pytorch_model-00002-of-00004.bin:  46%|████████████████████████▏                           | 2.32G/5.00G [00:38<00:41, 65.2MB/s][A[A




pytorch_model-00003-of-00004.bin:  44%|██████████████████████▊                             | 2.16G/4.92G [00:38<00:35, 77.6MB/s][A[A[A[A





pytorch_model-00001-of-00004.bin:  41%|█████████████████████▌                              | 2.06G/4.98G [00:39<00:49, 58.7MB/s][A[A[A[A[A


pytorch_model-00002-of-00004.bin:  47%|████████████████████████▎                           | 2.34G/5.00G [00:39<00:38, 68.7MB/s][A[A




pytorch_model-00003-of-00004.bin:  44%|██████████████████████▉                             | 2.17G/4.92G [00:39<00:38, 70.6MB/s][A[A[A[A





pytorch_model-00001-of-00004.bin:  42%|█████████████████████▋                              | 2.08G/4.98G [00:39<00:45, 63.1MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:  44%|███████████████████████                             | 2.18G/4.92G [00:39<00:42, 64.7MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:  47%|████████████████████████▍                           | 2.35G/5.00G [00:39<00:40, 65.6MB/s][A[A





pytorch_model-00001-of-00004.bin:  42%|█████████████████████▉                              | 2.10G/4.98G [00:39<00:45, 62.8MB/s][A[A[A[A[A


pytorch_model-00002-of-00004.bin:  47%|████████████████████████▋                           | 2.37G/5.00G [00:39<00:37, 69.3MB/s][A[A





pytorch_model-00001-of-00004.bin:  42%|██████████████████████                              | 2.11G/4.98G [00:39<00:38, 74.8MB/s][A[A[A[A[A


pytorch_model-00002-of-00004.bin:  48%|████████████████████████▊                           | 2.38G/5.00G [00:39<00:39, 66.2MB/s][A[A




pytorch_model-00003-of-00004.bin:  45%|███████████████████████▏                            | 2.19G/4.92G [00:39<01:03, 42.8MB/s][A[A[A[A





pytorch_model-00001-of-00004.bin:  43%|██████████████████████▏                             | 2.12G/4.98G [00:39<00:51, 55.6MB/s][A[A[A[A[A


pytorch_model-00002-of-00004.bin:  48%|████████████████████████▉                           | 2.40G/5.00G [00:40<00:38, 67.8MB/s][A[A




pytorch_model-00003-of-00004.bin:  45%|███████████████████████▎                            | 2.21G/4.92G [00:40<00:57, 47.3MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:  48%|█████████████████████████▏                          | 2.42G/5.00G [00:40<00:36, 70.8MB/s][A[A





pytorch_model-00001-of-00004.bin:  43%|██████████████████████▏                             | 2.13G/4.98G [00:40<01:04, 44.1MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:  45%|███████████████████████▌                            | 2.22G/4.92G [00:40<00:48, 55.5MB/s][A[A[A[A





pytorch_model-00001-of-00004.bin:  43%|██████████████████████▍                             | 2.14G/4.98G [00:40<00:47, 59.6MB/s][A[A[A[A[A


pytorch_model-00002-of-00004.bin:  49%|█████████████████████████▎                          | 2.43G/5.00G [00:40<00:36, 71.3MB/s][A[A




pytorch_model-00003-of-00004.bin:  46%|███████████████████████▋                            | 2.24G/4.92G [00:40<00:43, 61.4MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:  49%|█████████████████████████▍                          | 2.45G/5.00G [00:40<00:33, 76.4MB/s][A[A




pytorch_model-00003-of-00004.bin:  46%|███████████████████████▊                            | 2.26G/4.92G [00:40<00:41, 64.5MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:  49%|█████████████████████████▋                          | 2.46G/5.00G [00:40<00:33, 75.7MB/s][A[A





pytorch_model-00001-of-00004.bin:  43%|██████████████████████▍                             | 2.15G/4.98G [00:41<01:21, 34.6MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:  46%|████████████████████████                            | 2.27G/4.92G [00:41<00:39, 67.6MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:  50%|█████████████████████████▊                          | 2.48G/5.00G [00:41<00:33, 75.1MB/s][A[A





pytorch_model-00001-of-00004.bin:  43%|██████████████████████▌                             | 2.16G/4.98G [00:41<01:17, 36.6MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:  47%|████████████████████████▏                           | 2.29G/4.92G [00:41<00:38, 67.5MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:  50%|█████████████████████████▉                          | 2.50G/5.00G [00:41<00:32, 76.7MB/s][A[A




pytorch_model-00003-of-00004.bin:  47%|████████████████████████▎                           | 2.30G/4.92G [00:41<00:37, 70.3MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:  50%|██████████████████████████▏                         | 2.51G/5.00G [00:41<00:33, 75.3MB/s][A[A





pytorch_model-00001-of-00004.bin:  44%|██████████████████████▋                             | 2.18G/4.98G [00:41<01:11, 39.4MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:  47%|████████████████████████▌                           | 2.32G/4.92G [00:41<00:36, 70.4MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:  51%|██████████████████████████▎                         | 2.53G/5.00G [00:41<00:32, 76.2MB/s][A[A





pytorch_model-00001-of-00004.bin:  44%|██████████████████████▉                             | 2.19G/4.98G [00:41<00:59, 46.7MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:  48%|████████████████████████▋                           | 2.34G/4.92G [00:41<00:38, 66.6MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:  51%|██████████████████████████▍                         | 2.54G/5.00G [00:42<00:36, 67.2MB/s][A[A





pytorch_model-00001-of-00004.bin:  44%|███████████████████████                             | 2.21G/4.98G [00:42<00:56, 48.6MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:  48%|████████████████████████▉                           | 2.35G/4.92G [00:42<00:38, 66.1MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:  51%|██████████████████████████▋                         | 2.56G/5.00G [00:42<00:38, 63.8MB/s][A[A





pytorch_model-00001-of-00004.bin:  45%|███████████████████████▏                            | 2.22G/4.98G [00:42<00:54, 50.2MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:  48%|█████████████████████████                           | 2.37G/4.92G [00:42<00:39, 64.8MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:  52%|██████████████████████████▊                         | 2.58G/5.00G [00:42<00:35, 69.1MB/s][A[A




pytorch_model-00003-of-00004.bin:  48%|█████████████████████████▏                          | 2.38G/4.92G [00:42<00:32, 78.5MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:  52%|██████████████████████████▉                         | 2.59G/5.00G [00:42<00:34, 70.0MB/s][A[A




pytorch_model-00003-of-00004.bin:  49%|█████████████████████████▎                          | 2.39G/4.92G [00:42<00:39, 63.8MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:  52%|███████████████████████████                         | 2.61G/5.00G [00:42<00:33, 71.9MB/s][A[A




pytorch_model-00003-of-00004.bin:  49%|█████████████████████████▍                          | 2.40G/4.92G [00:43<00:44, 56.7MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:  52%|███████████████████████████▎                        | 2.62G/5.00G [00:43<00:32, 73.6MB/s][A[A




pytorch_model-00003-of-00004.bin:  49%|█████████████████████████▌                          | 2.42G/4.92G [00:43<00:40, 62.3MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:  53%|███████████████████████████▍                        | 2.64G/5.00G [00:43<00:34, 67.7MB/s][A[A





pytorch_model-00001-of-00004.bin:  45%|███████████████████████▍                            | 2.24G/4.98G [00:43<01:39, 27.6MB/s][A[A[A[A[A


pytorch_model-00002-of-00004.bin:  53%|███████████████████████████▌                        | 2.66G/5.00G [00:43<00:32, 71.2MB/s][A[A




pytorch_model-00003-of-00004.bin:  49%|█████████████████████████▋                          | 2.43G/4.92G [00:43<00:49, 50.4MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:  53%|███████████████████████████▊                        | 2.67G/5.00G [00:43<00:29, 78.0MB/s][A[A





pytorch_model-00001-of-00004.bin:  45%|███████████████████████▌                            | 2.26G/4.98G [00:43<01:23, 32.6MB/s][A[A[A[A[A


pytorch_model-00002-of-00004.bin:  54%|███████████████████████████▉                        | 2.69G/5.00G [00:44<00:30, 75.5MB/s][A[A





pytorch_model-00001-of-00004.bin:  46%|███████████████████████▋                            | 2.27G/4.98G [00:44<01:11, 37.7MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:  50%|█████████████████████████▉                          | 2.45G/4.92G [00:44<00:56, 43.4MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:  54%|████████████████████████████                        | 2.70G/5.00G [00:44<00:30, 75.6MB/s][A[A




pytorch_model-00003-of-00004.bin:  50%|██████████████████████████                          | 2.46G/4.92G [00:44<00:50, 48.7MB/s][A[A[A[A





pytorch_model-00001-of-00004.bin:  46%|███████████████████████▉                            | 2.29G/4.98G [00:44<01:04, 41.8MB/s][A[A[A[A[A


pytorch_model-00002-of-00004.bin:  54%|████████████████████████████▎                       | 2.72G/5.00G [00:44<00:34, 65.3MB/s][A[A




pytorch_model-00003-of-00004.bin:  50%|██████████████████████████▏                         | 2.48G/4.92G [00:44<00:43, 56.0MB/s][A[A[A[A





pytorch_model-00001-of-00004.bin:  46%|████████████████████████                            | 2.30G/4.98G [00:44<00:57, 46.1MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:  51%|██████████████████████████▍                         | 2.50G/4.92G [00:44<00:39, 61.5MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:  55%|████████████████████████████▍                       | 2.74G/5.00G [00:44<00:35, 63.2MB/s][A[A





pytorch_model-00001-of-00004.bin:  47%|████████████████████████▏                           | 2.32G/4.98G [00:44<00:52, 50.2MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:  51%|██████████████████████████▌                         | 2.51G/4.92G [00:44<00:36, 65.0MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:  55%|████████████████████████████▌                       | 2.75G/5.00G [00:45<00:37, 60.5MB/s][A[A





pytorch_model-00001-of-00004.bin:  47%|████████████████████████▍                           | 2.34G/4.98G [00:45<00:49, 53.7MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:  51%|██████████████████████████▋                         | 2.53G/4.92G [00:45<00:34, 70.0MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:  55%|████████████████████████████▊                       | 2.77G/5.00G [00:45<00:36, 60.8MB/s][A[A





pytorch_model-00001-of-00004.bin:  47%|████████████████████████▌                           | 2.35G/4.98G [00:45<00:49, 52.5MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:  52%|██████████████████████████▉                         | 2.54G/4.92G [00:45<00:38, 61.3MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:  56%|████████████████████████████▉                       | 2.78G/5.00G [00:45<00:37, 58.6MB/s][A[A




pytorch_model-00003-of-00004.bin:  52%|███████████████████████████                         | 2.56G/4.92G [00:45<00:36, 63.7MB/s][A[A[A[A





pytorch_model-00001-of-00004.bin:  48%|████████████████████████▋                           | 2.37G/4.98G [00:45<00:48, 53.9MB/s][A[A[A[A[A


pytorch_model-00002-of-00004.bin:  56%|█████████████████████████████                       | 2.80G/5.00G [00:45<00:35, 62.4MB/s][A[A




pytorch_model-00003-of-00004.bin:  52%|███████████████████████████▏                        | 2.58G/4.92G [00:45<00:35, 66.0MB/s][A[A[A[A





pytorch_model-00001-of-00004.bin:  48%|████████████████████████▉                           | 2.38G/4.98G [00:45<00:44, 58.9MB/s][A[A[A[A[A





pytorch_model-00001-of-00004.bin:  48%|█████████████████████████                           | 2.40G/4.98G [00:46<00:39, 65.9MB/s][A[A[A[A[A


pytorch_model-00002-of-00004.bin:  56%|█████████████████████████████▎                      | 2.82G/5.00G [00:46<00:35, 62.0MB/s][A[A




pytorch_model-00003-of-00004.bin:  53%|███████████████████████████▍                        | 2.59G/4.92G [00:46<00:37, 62.6MB/s][A[A[A[A





pytorch_model-00001-of-00004.bin:  49%|█████████████████████████▏                          | 2.42G/4.98G [00:46<00:37, 68.3MB/s][A[A[A[A[A


pytorch_model-00002-of-00004.bin:  57%|█████████████████████████████▍                      | 2.83G/5.00G [00:46<00:39, 54.3MB/s][A[A





pytorch_model-00001-of-00004.bin:  49%|█████████████████████████▍                          | 2.43G/4.98G [00:46<00:35, 70.8MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:  53%|███████████████████████████▌                        | 2.61G/4.92G [00:46<00:39, 58.3MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:  57%|█████████████████████████████▌                      | 2.85G/5.00G [00:46<00:35, 60.3MB/s][A[A


pytorch_model-00002-of-00004.bin:  57%|█████████████████████████████▊                      | 2.86G/5.00G [00:47<00:35, 59.3MB/s][A[A




pytorch_model-00003-of-00004.bin:  53%|███████████████████████████▊                        | 2.62G/4.92G [00:47<00:48, 47.3MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:  58%|█████████████████████████████▉                      | 2.88G/5.00G [00:47<00:34, 61.2MB/s][A[A




pytorch_model-00003-of-00004.bin:  54%|███████████████████████████▉                        | 2.64G/4.92G [00:47<00:42, 53.2MB/s][A[A[A[A





pytorch_model-00001-of-00004.bin:  49%|█████████████████████████▌                          | 2.45G/4.98G [00:47<01:08, 36.9MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:  54%|████████████████████████████                        | 2.66G/4.92G [00:47<00:39, 57.8MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:  58%|██████████████████████████████                      | 2.90G/5.00G [00:47<00:36, 57.3MB/s][A[A





pytorch_model-00001-of-00004.bin:  50%|█████████████████████████▋                          | 2.46G/4.98G [00:47<00:57, 43.7MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:  54%|████████████████████████████▎                       | 2.67G/4.92G [00:47<00:40, 54.8MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:  58%|██████████████████████████████▎                     | 2.91G/5.00G [00:47<00:36, 57.9MB/s][A[A





pytorch_model-00001-of-00004.bin:  50%|█████████████████████████▉                          | 2.48G/4.98G [00:47<00:50, 49.1MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:  55%|████████████████████████████▍                       | 2.69G/4.92G [00:48<00:38, 58.4MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:  59%|██████████████████████████████▍                     | 2.93G/5.00G [00:48<00:34, 59.7MB/s][A[A





pytorch_model-00001-of-00004.bin:  50%|██████████████████████████                          | 2.50G/4.98G [00:48<00:45, 54.1MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:  55%|████████████████████████████▌                       | 2.70G/4.92G [00:48<00:36, 61.2MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:  59%|██████████████████████████████▌                     | 2.94G/5.00G [00:48<00:32, 64.1MB/s][A[A





pytorch_model-00001-of-00004.bin:  50%|██████████████████████████▏                         | 2.51G/4.98G [00:48<00:47, 51.8MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:  55%|████████████████████████████▊                       | 2.72G/4.92G [00:48<00:34, 63.3MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:  59%|██████████████████████████████▊                     | 2.96G/5.00G [00:48<00:30, 67.2MB/s][A[A





pytorch_model-00001-of-00004.bin:  51%|██████████████████████████▎                         | 2.52G/4.98G [00:48<00:40, 60.3MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:  56%|████████████████████████████▉                       | 2.73G/4.92G [00:48<00:31, 70.1MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:  59%|██████████████████████████████▉                     | 2.97G/5.00G [00:48<00:27, 72.6MB/s][A[A




pytorch_model-00003-of-00004.bin:  56%|████████████████████████████▉                       | 2.74G/4.92G [00:48<00:33, 65.6MB/s][A[A[A[A





pytorch_model-00001-of-00004.bin:  51%|██████████████████████████▍                         | 2.53G/4.98G [00:48<00:43, 56.0MB/s][A[A[A[A[A


pytorch_model-00002-of-00004.bin:  60%|██████████████████████████████▉                     | 2.98G/5.00G [00:48<00:37, 53.9MB/s][A[A





pytorch_model-00001-of-00004.bin:  51%|██████████████████████████▌                         | 2.54G/4.98G [00:48<00:41, 59.0MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:  56%|█████████████████████████████                       | 2.75G/4.92G [00:48<00:32, 67.0MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:  60%|███████████████████████████████                     | 2.99G/5.00G [00:49<00:37, 54.2MB/s][A[A




pytorch_model-00003-of-00004.bin:  56%|█████████████████████████████▎                      | 2.77G/4.92G [00:49<00:33, 63.9MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:  60%|███████████████████████████████▎                    | 3.01G/5.00G [00:49<00:33, 59.9MB/s][A[A





pytorch_model-00001-of-00004.bin:  51%|██████████████████████████▋                         | 2.56G/4.98G [00:49<00:52, 46.1MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:  57%|█████████████████████████████▍                      | 2.78G/4.92G [00:49<00:33, 63.1MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:  60%|███████████████████████████████▍                    | 3.02G/5.00G [00:49<00:33, 58.6MB/s][A[A





pytorch_model-00001-of-00004.bin:  52%|██████████████████████████▉                         | 2.58G/4.98G [00:49<00:47, 50.7MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:  57%|█████████████████████████████▌                      | 2.80G/4.92G [00:49<00:32, 64.6MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:  61%|███████████████████████████████▌                    | 3.04G/5.00G [00:49<00:29, 65.7MB/s][A[A





pytorch_model-00001-of-00004.bin:  52%|███████████████████████████                         | 2.59G/4.98G [00:49<00:42, 56.4MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:  57%|█████████████████████████████▊                      | 2.82G/4.92G [00:50<00:33, 62.4MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:  61%|███████████████████████████████▊                    | 3.06G/5.00G [00:50<00:27, 70.0MB/s][A[A





pytorch_model-00001-of-00004.bin:  52%|███████████████████████████▏                        | 2.61G/4.98G [00:50<00:39, 60.2MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:  58%|█████████████████████████████▉                      | 2.83G/4.92G [00:50<00:32, 64.9MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:  61%|███████████████████████████████▉                    | 3.07G/5.00G [00:50<00:26, 71.8MB/s][A[A





pytorch_model-00001-of-00004.bin:  53%|███████████████████████████▍                        | 2.62G/4.98G [00:50<00:36, 63.6MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:  58%|██████████████████████████████▏                     | 2.85G/4.92G [00:50<00:29, 69.0MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:  62%|████████████████████████████████                    | 3.09G/5.00G [00:50<00:26, 71.2MB/s][A[A





pytorch_model-00001-of-00004.bin:  53%|███████████████████████████▌                        | 2.64G/4.98G [00:50<00:35, 66.2MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:  58%|██████████████████████████████▎                     | 2.86G/4.92G [00:50<00:29, 69.6MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:  62%|████████████████████████████████▎                   | 3.10G/5.00G [00:50<00:26, 72.0MB/s][A[A





pytorch_model-00001-of-00004.bin:  53%|███████████████████████████▊                        | 2.66G/4.98G [00:50<00:32, 71.8MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:  59%|██████████████████████████████▍                     | 2.88G/4.92G [00:50<00:29, 68.0MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:  62%|████████████████████████████████▍                   | 3.12G/5.00G [00:50<00:25, 73.0MB/s][A[A





pytorch_model-00001-of-00004.bin:  54%|███████████████████████████▉                        | 2.67G/4.98G [00:50<00:32, 70.8MB/s][A[A[A[A[A


pytorch_model-00002-of-00004.bin:  63%|████████████████████████████████▌                   | 3.14G/5.00G [00:51<00:24, 76.4MB/s][A[A





pytorch_model-00001-of-00004.bin:  54%|████████████████████████████                        | 2.69G/4.98G [00:51<00:32, 70.2MB/s][A[A[A[A[A


pytorch_model-00002-of-00004.bin:  63%|████████████████████████████████▊                   | 3.15G/5.00G [00:51<00:24, 76.0MB/s][A[A




pytorch_model-00003-of-00004.bin:  59%|██████████████████████████████▋                     | 2.90G/4.92G [00:51<00:41, 48.8MB/s][A[A[A[A





pytorch_model-00001-of-00004.bin:  54%|████████████████████████████▎                       | 2.70G/4.98G [00:51<00:37, 60.8MB/s][A[A[A[A[A


pytorch_model-00002-of-00004.bin:  63%|████████████████████████████████▉                   | 3.17G/5.00G [00:51<00:25, 71.0MB/s][A[A




pytorch_model-00003-of-00004.bin:  59%|██████████████████████████████▊                     | 2.91G/4.92G [00:51<00:37, 53.7MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:  64%|█████████████████████████████████                   | 3.18G/5.00G [00:51<00:24, 74.2MB/s][A[A





pytorch_model-00001-of-00004.bin:  55%|████████████████████████████▍                       | 2.72G/4.98G [00:51<00:42, 52.7MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:  60%|██████████████████████████████▉                     | 2.93G/4.92G [00:51<00:37, 53.3MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:  64%|█████████████████████████████████▎                  | 3.20G/5.00G [00:52<00:25, 69.5MB/s][A[A





pytorch_model-00001-of-00004.bin:  55%|████████████████████████████▌                       | 2.74G/4.98G [00:52<00:37, 60.2MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:  60%|███████████████████████████████▏                    | 2.94G/4.92G [00:52<00:36, 53.7MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:  64%|█████████████████████████████████▍                  | 3.22G/5.00G [00:52<00:26, 67.9MB/s][A[A


pytorch_model-00002-of-00004.bin:  65%|█████████████████████████████████▌                  | 3.23G/5.00G [00:52<00:26, 66.5MB/s][A[A




pytorch_model-00003-of-00004.bin:  60%|███████████████████████████████▎                    | 2.96G/4.92G [00:52<00:45, 43.1MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:  65%|█████████████████████████████████▊                  | 3.25G/5.00G [00:52<00:31, 56.0MB/s][A[A





pytorch_model-00001-of-00004.bin:  55%|████████████████████████████▊                       | 2.75G/4.98G [00:52<00:59, 37.1MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:  61%|███████████████████████████████▍                    | 2.98G/4.92G [00:53<00:44, 43.3MB/s][A[A[A[A





pytorch_model-00001-of-00004.bin:  56%|████████████████████████████▉                       | 2.77G/4.98G [00:53<00:51, 43.0MB/s][A[A[A[A[A


pytorch_model-00002-of-00004.bin:  65%|█████████████████████████████████▉                  | 3.26G/5.00G [00:53<00:30, 56.7MB/s][A[A





pytorch_model-00001-of-00004.bin:  56%|█████████████████████████████                       | 2.78G/4.98G [00:53<00:39, 55.0MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:  61%|███████████████████████████████▋                    | 2.99G/4.92G [00:53<00:38, 49.6MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:  66%|██████████████████████████████████                  | 3.28G/5.00G [00:53<00:28, 59.6MB/s][A[A





pytorch_model-00001-of-00004.bin:  56%|█████████████████████████████▏                      | 2.79G/4.98G [00:53<00:39, 54.7MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:  61%|███████████████████████████████▊                    | 3.01G/4.92G [00:53<00:34, 55.4MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:  66%|██████████████████████████████████▎                 | 3.30G/5.00G [00:53<00:28, 60.1MB/s][A[A





pytorch_model-00001-of-00004.bin:  56%|█████████████████████████████▎                      | 2.80G/4.98G [00:53<00:46, 47.2MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:  62%|███████████████████████████████▉                    | 3.02G/4.92G [00:53<00:33, 57.1MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:  66%|██████████████████████████████████▍                 | 3.31G/5.00G [00:53<00:26, 64.1MB/s][A[A





pytorch_model-00001-of-00004.bin:  57%|█████████████████████████████▍                      | 2.82G/4.98G [00:53<00:41, 51.6MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:  62%|████████████████████████████████▏                   | 3.04G/4.92G [00:54<00:34, 55.0MB/s][A[A[A[A





pytorch_model-00001-of-00004.bin:  57%|█████████████████████████████▌                      | 2.83G/4.98G [00:54<00:37, 57.2MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:  62%|████████████████████████████████▎                   | 3.06G/4.92G [00:54<00:30, 60.3MB/s][A[A[A[A





pytorch_model-00001-of-00004.bin:  57%|█████████████████████████████▊                      | 2.85G/4.98G [00:54<00:34, 61.6MB/s][A[A[A[A[A


pytorch_model-00002-of-00004.bin:  67%|██████████████████████████████████▌                 | 3.33G/5.00G [00:54<00:35, 46.9MB/s][A[A




pytorch_model-00003-of-00004.bin:  62%|████████████████████████████████▍                   | 3.07G/4.92G [00:54<00:28, 64.9MB/s][A[A[A[A





pytorch_model-00001-of-00004.bin:  58%|█████████████████████████████▉                      | 2.86G/4.98G [00:54<00:32, 64.8MB/s][A[A[A[A[A


pytorch_model-00002-of-00004.bin:  67%|██████████████████████████████████▊                 | 3.34G/5.00G [00:54<00:30, 55.0MB/s][A[A




pytorch_model-00003-of-00004.bin:  63%|████████████████████████████████▋                   | 3.09G/4.92G [00:54<00:27, 65.8MB/s][A[A[A[A





pytorch_model-00001-of-00004.bin:  58%|██████████████████████████████                      | 2.88G/4.98G [00:54<00:31, 66.5MB/s][A[A[A[A[A


pytorch_model-00002-of-00004.bin:  67%|██████████████████████████████████▉                 | 3.36G/5.00G [00:54<00:29, 56.0MB/s][A[A





pytorch_model-00001-of-00004.bin:  58%|██████████████████████████████▎                     | 2.90G/4.98G [00:55<00:30, 68.0MB/s][A[A[A[A[A


pytorch_model-00002-of-00004.bin:  68%|███████████████████████████████████                 | 3.38G/5.00G [00:55<00:25, 62.9MB/s][A[A




pytorch_model-00003-of-00004.bin:  63%|████████████████████████████████▊                   | 3.10G/4.92G [00:55<00:37, 48.8MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:  68%|███████████████████████████████████▎                | 3.39G/5.00G [00:55<00:27, 58.8MB/s][A[A





pytorch_model-00001-of-00004.bin:  59%|██████████████████████████████▍                     | 2.91G/4.98G [00:55<00:35, 58.2MB/s][A[A[A[A[A


pytorch_model-00002-of-00004.bin:  68%|███████████████████████████████████▍                | 3.41G/5.00G [00:55<00:24, 64.6MB/s][A[A




pytorch_model-00003-of-00004.bin:  63%|█████████████████████████████████                   | 3.12G/4.92G [00:55<00:35, 51.2MB/s][A[A[A[A





pytorch_model-00001-of-00004.bin:  59%|██████████████████████████████▌                     | 2.93G/4.98G [00:55<00:35, 58.0MB/s][A[A[A[A[A


pytorch_model-00002-of-00004.bin:  68%|███████████████████████████████████▌                | 3.42G/5.00G [00:55<00:22, 68.8MB/s][A[A




pytorch_model-00003-of-00004.bin:  64%|█████████████████████████████████▏                  | 3.14G/4.92G [00:55<00:31, 56.0MB/s][A[A[A[A





pytorch_model-00001-of-00004.bin:  59%|██████████████████████████████▊                     | 2.94G/4.98G [00:55<00:34, 59.4MB/s][A[A[A[A[A


pytorch_model-00002-of-00004.bin:  69%|███████████████████████████████████▊                | 3.44G/5.00G [00:56<00:22, 68.6MB/s][A[A


pytorch_model-00002-of-00004.bin:  69%|███████████████████████████████████▉                | 3.46G/5.00G [00:56<00:20, 75.6MB/s][A[A




pytorch_model-00003-of-00004.bin:  64%|█████████████████████████████████▎                  | 3.15G/4.92G [00:56<00:36, 47.9MB/s][A[A[A[A





pytorch_model-00001-of-00004.bin:  59%|██████████████████████████████▉                     | 2.96G/4.98G [00:56<00:35, 56.4MB/s][A[A[A[A[A


pytorch_model-00002-of-00004.bin:  69%|████████████████████████████████████                | 3.47G/5.00G [00:56<00:20, 75.6MB/s][A[A





pytorch_model-00001-of-00004.bin:  60%|███████████████████████████████                     | 2.98G/4.98G [00:56<00:32, 60.8MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:  64%|█████████████████████████████████▌                  | 3.17G/4.92G [00:56<00:33, 52.1MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:  70%|████████████████████████████████████▎               | 3.49G/5.00G [00:56<00:20, 72.9MB/s][A[A




pytorch_model-00003-of-00004.bin:  65%|█████████████████████████████████▋                  | 3.18G/4.92G [00:56<00:31, 55.5MB/s][A[A[A[A





pytorch_model-00001-of-00004.bin:  60%|███████████████████████████████▎                    | 2.99G/4.98G [00:56<00:37, 53.0MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:  65%|█████████████████████████████████▊                  | 3.20G/4.92G [00:57<00:29, 59.1MB/s][A[A[A[A





pytorch_model-00001-of-00004.bin:  60%|███████████████████████████████▍                    | 3.01G/4.98G [00:57<00:33, 58.1MB/s][A[A[A[A[A


pytorch_model-00002-of-00004.bin:  70%|████████████████████████████████████▍               | 3.50G/5.00G [00:57<00:31, 48.2MB/s][A[A





pytorch_model-00001-of-00004.bin:  61%|███████████████████████████████▌                    | 3.02G/4.98G [00:57<00:31, 61.2MB/s][A[A[A[A[A


pytorch_model-00002-of-00004.bin:  70%|████████████████████████████████████▌               | 3.52G/5.00G [00:57<00:28, 52.5MB/s][A[A





pytorch_model-00001-of-00004.bin:  61%|███████████████████████████████▊                    | 3.04G/4.98G [00:57<00:29, 65.2MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:  65%|██████████████████████████████████                  | 3.22G/4.92G [00:57<00:38, 44.0MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:  71%|████████████████████████████████████▊               | 3.54G/5.00G [00:57<00:26, 55.2MB/s][A[A





pytorch_model-00001-of-00004.bin:  61%|███████████████████████████████▉                    | 3.06G/4.98G [00:57<00:27, 69.3MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:  66%|██████████████████████████████████▏                 | 3.23G/4.92G [00:57<00:32, 51.9MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:  71%|████████████████████████████████████▉               | 3.55G/5.00G [00:57<00:24, 59.4MB/s][A[A




pytorch_model-00003-of-00004.bin:  66%|██████████████████████████████████▎                 | 3.25G/4.92G [00:57<00:29, 56.1MB/s][A[A[A[A





pytorch_model-00001-of-00004.bin:  62%|████████████████████████████████                    | 3.07G/4.98G [00:58<00:28, 66.4MB/s][A[A[A[A[A


pytorch_model-00002-of-00004.bin:  71%|█████████████████████████████████████               | 3.57G/5.00G [00:58<00:22, 64.7MB/s][A[A




pytorch_model-00003-of-00004.bin:  66%|██████████████████████████████████▌                 | 3.26G/4.92G [00:58<00:28, 58.2MB/s][A[A[A[A





pytorch_model-00001-of-00004.bin:  62%|████████████████████████████████▎                   | 3.09G/4.98G [00:58<00:28, 66.5MB/s][A[A[A[A[A


pytorch_model-00002-of-00004.bin:  72%|█████████████████████████████████████▎              | 3.58G/5.00G [00:58<00:20, 67.8MB/s][A[A





pytorch_model-00001-of-00004.bin:  62%|████████████████████████████████▍                   | 3.10G/4.98G [00:58<00:27, 69.2MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:  67%|██████████████████████████████████▋                 | 3.28G/4.92G [00:58<00:28, 57.4MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:  72%|█████████████████████████████████████▍              | 3.60G/5.00G [00:58<00:20, 67.6MB/s][A[A





pytorch_model-00001-of-00004.bin:  63%|████████████████████████████████▌                   | 3.12G/4.98G [00:58<00:27, 66.3MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:  67%|██████████████████████████████████▊                 | 3.30G/4.92G [00:58<00:27, 58.8MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:  72%|█████████████████████████████████████▌              | 3.62G/5.00G [00:58<00:23, 59.4MB/s][A[A





pytorch_model-00001-of-00004.bin:  63%|████████████████████████████████▊                   | 3.14G/4.98G [00:58<00:27, 65.8MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:  67%|███████████████████████████████████                 | 3.31G/4.92G [00:58<00:25, 63.9MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:  73%|█████████████████████████████████████▊              | 3.63G/5.00G [00:59<00:21, 64.1MB/s][A[A





pytorch_model-00001-of-00004.bin:  63%|████████████████████████████████▉                   | 3.15G/4.98G [00:59<00:26, 67.6MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:  68%|███████████████████████████████████▏                | 3.33G/4.92G [00:59<00:23, 66.2MB/s][A[A[A[A





pytorch_model-00001-of-00004.bin:  64%|█████████████████████████████████                   | 3.17G/4.98G [00:59<00:25, 71.6MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:  68%|███████████████████████████████████▎                | 3.34G/4.92G [00:59<00:22, 69.5MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:  73%|█████████████████████████████████████▉              | 3.65G/5.00G [00:59<00:22, 60.0MB/s][A[A





pytorch_model-00001-of-00004.bin:  64%|█████████████████████████████████▎                  | 3.18G/4.98G [00:59<00:24, 71.9MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:  68%|███████████████████████████████████▌                | 3.36G/4.92G [00:59<00:23, 67.3MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:  73%|██████████████████████████████████████              | 3.66G/5.00G [00:59<00:21, 63.0MB/s][A[A





pytorch_model-00001-of-00004.bin:  64%|█████████████████████████████████▍                  | 3.20G/4.98G [00:59<00:20, 85.1MB/s][A[A[A[A[A


pytorch_model-00002-of-00004.bin:  74%|██████████████████████████████████████▏             | 3.68G/5.00G [00:59<00:18, 71.7MB/s][A[A




pytorch_model-00003-of-00004.bin:  69%|███████████████████████████████████▋                | 3.38G/4.92G [00:59<00:23, 66.0MB/s][A[A[A[A





pytorch_model-00001-of-00004.bin:  64%|█████████████████████████████████▌                  | 3.21G/4.98G [00:59<00:26, 67.5MB/s][A[A[A[A[A


pytorch_model-00002-of-00004.bin:  74%|██████████████████████████████████████▎             | 3.68G/5.00G [01:00<00:24, 52.6MB/s][A[A





pytorch_model-00001-of-00004.bin:  65%|█████████████████████████████████▌                  | 3.22G/4.98G [01:00<00:28, 61.3MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:  69%|███████████████████████████████████▉                | 3.39G/4.92G [01:00<00:22, 66.6MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:  74%|██████████████████████████████████████▍             | 3.70G/5.00G [01:00<00:24, 52.3MB/s][A[A




pytorch_model-00003-of-00004.bin:  69%|████████████████████████████████████                | 3.41G/4.92G [01:00<00:21, 71.8MB/s][A[A[A[A





pytorch_model-00001-of-00004.bin:  65%|█████████████████████████████████▊                  | 3.23G/4.98G [01:00<00:27, 62.4MB/s][A[A[A[A[A


pytorch_model-00002-of-00004.bin:  74%|██████████████████████████████████████▌             | 3.71G/5.00G [01:00<00:19, 67.6MB/s][A[A




pytorch_model-00003-of-00004.bin:  70%|████████████████████████████████████▏               | 3.42G/4.92G [01:00<00:19, 78.1MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:  74%|██████████████████████████████████████▋             | 3.72G/5.00G [01:00<00:18, 67.4MB/s][A[A





pytorch_model-00001-of-00004.bin:  65%|█████████████████████████████████▉                  | 3.25G/4.98G [01:00<00:26, 65.2MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:  70%|████████████████████████████████████▎               | 3.43G/4.92G [01:00<00:22, 65.4MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:  75%|██████████████████████████████████████▊             | 3.73G/5.00G [01:00<00:22, 56.0MB/s][A[A





pytorch_model-00001-of-00004.bin:  66%|██████████████████████████████████                  | 3.26G/4.98G [01:00<00:26, 65.4MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:  70%|████████████████████████████████████▍               | 3.44G/4.92G [01:00<00:24, 60.5MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:  75%|██████████████████████████████████████▉             | 3.74G/5.00G [01:01<00:20, 61.5MB/s][A[A





pytorch_model-00001-of-00004.bin:  66%|██████████████████████████████████▎                 | 3.28G/4.98G [01:01<00:25, 65.5MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:  70%|████████████████████████████████████▌               | 3.46G/4.92G [01:01<00:22, 66.0MB/s][A[A[A[A





pytorch_model-00001-of-00004.bin:  66%|██████████████████████████████████▍                 | 3.30G/4.98G [01:01<00:25, 65.6MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:  71%|████████████████████████████████████▋               | 3.47G/4.92G [01:01<00:23, 62.2MB/s][A[A[A[A





pytorch_model-00001-of-00004.bin:  67%|██████████████████████████████████▌                 | 3.31G/4.98G [01:01<00:22, 73.2MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:  71%|████████████████████████████████████▉               | 3.49G/4.92G [01:01<00:21, 67.6MB/s][A[A[A[A





pytorch_model-00001-of-00004.bin:  67%|██████████████████████████████████▊                 | 3.33G/4.98G [01:01<00:23, 71.2MB/s][A[A[A[A[A





pytorch_model-00001-of-00004.bin:  67%|██████████████████████████████████▉                 | 3.34G/4.98G [01:01<00:22, 72.1MB/s][A[A[A[A[A





pytorch_model-00001-of-00004.bin:  68%|███████████████████████████████████                 | 3.36G/4.98G [01:02<00:22, 71.1MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:  71%|█████████████████████████████████████               | 3.50G/4.92G [01:02<00:34, 40.7MB/s][A[A[A[A





pytorch_model-00001-of-00004.bin:  68%|███████████████████████████████████▎                | 3.38G/4.98G [01:02<00:21, 73.1MB/s][A[A[A[A[A


pytorch_model-00002-of-00004.bin:  75%|███████████████████████████████████████             | 3.76G/5.00G [01:02<00:56, 21.8MB/s][A[A





pytorch_model-00001-of-00004.bin:  68%|███████████████████████████████████▍                | 3.39G/4.98G [01:02<00:21, 72.9MB/s][A[A[A[A[A


pytorch_model-00002-of-00004.bin:  76%|███████████████████████████████████████▎            | 3.78G/5.00G [01:02<00:43, 28.4MB/s][A[A





pytorch_model-00001-of-00004.bin:  68%|███████████████████████████████████▌                | 3.41G/4.98G [01:02<00:22, 70.0MB/s][A[A[A[A[A





pytorch_model-00001-of-00004.bin:  69%|███████████████████████████████████▊                | 3.42G/4.98G [01:03<00:25, 62.1MB/s][A[A[A[A[A


pytorch_model-00002-of-00004.bin:  76%|███████████████████████████████████████▍            | 3.79G/5.00G [01:03<00:38, 31.2MB/s][A[A





pytorch_model-00001-of-00004.bin:  69%|███████████████████████████████████▉                | 3.44G/4.98G [01:03<00:22, 67.5MB/s][A[A[A[A[A


pytorch_model-00002-of-00004.bin:  76%|███████████████████████████████████████▌            | 3.81G/5.00G [01:03<00:31, 37.8MB/s][A[A





pytorch_model-00001-of-00004.bin:  69%|████████████████████████████████████                | 3.46G/4.98G [01:03<00:21, 69.2MB/s][A[A[A[A[A


pytorch_model-00002-of-00004.bin:  76%|███████████████████████████████████████▊            | 3.82G/5.00G [01:03<00:25, 46.0MB/s][A[A





pytorch_model-00001-of-00004.bin:  70%|████████████████████████████████████▎               | 3.47G/4.98G [01:03<00:20, 71.8MB/s][A[A[A[A[A


pytorch_model-00002-of-00004.bin:  77%|███████████████████████████████████████▉            | 3.84G/5.00G [01:03<00:23, 48.7MB/s][A[A


pytorch_model-00002-of-00004.bin:  77%|████████████████████████████████████████            | 3.86G/5.00G [01:04<00:20, 54.7MB/s][A[A





pytorch_model-00001-of-00004.bin:  70%|████████████████████████████████████▍               | 3.49G/4.98G [01:04<00:26, 56.6MB/s][A[A[A[A[A


pytorch_model-00002-of-00004.bin:  77%|████████████████████████████████████████▎           | 3.87G/5.00G [01:04<00:20, 55.2MB/s][A[A





pytorch_model-00001-of-00004.bin:  70%|████████████████████████████████████▌               | 3.50G/4.98G [01:04<00:24, 59.1MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:  72%|█████████████████████████████████████▏              | 3.52G/4.92G [01:04<01:26, 16.2MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:  78%|████████████████████████████████████████▍           | 3.89G/5.00G [01:04<00:19, 56.2MB/s][A[A




pytorch_model-00003-of-00004.bin:  72%|█████████████████████████████████████▍              | 3.54G/4.92G [01:04<01:03, 21.6MB/s][A[A[A[A





pytorch_model-00001-of-00004.bin:  71%|████████████████████████████████████▊               | 3.52G/4.98G [01:04<00:26, 54.0MB/s][A[A[A[A[A


pytorch_model-00002-of-00004.bin:  78%|████████████████████████████████████████▌           | 3.90G/5.00G [01:04<00:17, 61.4MB/s][A[A




pytorch_model-00003-of-00004.bin:  72%|█████████████████████████████████████▌              | 3.55G/4.92G [01:04<00:47, 29.0MB/s][A[A[A[A




pytorch_model-00003-of-00004.bin:  72%|█████████████████████████████████████▋              | 3.56G/4.92G [01:05<00:43, 31.3MB/s][A[A[A[A





pytorch_model-00001-of-00004.bin:  71%|████████████████████████████████████▉               | 3.54G/4.98G [01:05<00:26, 53.5MB/s][A[A[A[A[A


pytorch_model-00002-of-00004.bin:  78%|████████████████████████████████████████▊           | 3.92G/5.00G [01:05<00:16, 64.3MB/s][A[A




pytorch_model-00003-of-00004.bin:  73%|█████████████████████████████████████▋              | 3.57G/4.92G [01:05<00:41, 32.6MB/s][A[A[A[A





pytorch_model-00001-of-00004.bin:  71%|█████████████████████████████████████               | 3.55G/4.98G [01:05<00:25, 56.7MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:  73%|█████████████████████████████████████▉              | 3.58G/4.92G [01:05<00:33, 40.1MB/s][A[A[A[A





pytorch_model-00001-of-00004.bin:  72%|█████████████████████████████████████▎              | 3.57G/4.98G [01:05<00:22, 62.3MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:  73%|██████████████████████████████████████              | 3.60G/4.92G [01:05<00:26, 48.9MB/s][A[A[A[A





pytorch_model-00001-of-00004.bin:  72%|█████████████████████████████████████▍              | 3.58G/4.98G [01:05<00:24, 56.5MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:  74%|██████████████████████████████████████▏             | 3.62G/4.92G [01:05<00:22, 56.9MB/s][A[A[A[A





pytorch_model-00001-of-00004.bin:  72%|█████████████████████████████████████▌              | 3.60G/4.98G [01:06<00:23, 59.2MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:  74%|██████████████████████████████████████▍             | 3.63G/4.92G [01:06<00:21, 59.5MB/s][A[A[A[A





pytorch_model-00001-of-00004.bin:  73%|█████████████████████████████████████▊              | 3.62G/4.98G [01:06<00:22, 61.2MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:  74%|██████████████████████████████████████▌             | 3.65G/4.92G [01:06<00:20, 61.7MB/s][A[A[A[A




pytorch_model-00003-of-00004.bin:  75%|██████████████████████████████████████▊             | 3.66G/4.92G [01:06<00:18, 68.2MB/s][A[A[A[A





pytorch_model-00001-of-00004.bin:  73%|█████████████████████████████████████▉              | 3.63G/4.98G [01:06<00:21, 62.9MB/s][A[A[A[A[A





pytorch_model-00001-of-00004.bin:  73%|██████████████████████████████████████              | 3.65G/4.98G [01:06<00:23, 57.3MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:  75%|██████████████████████████████████████▉             | 3.68G/4.92G [01:07<00:22, 54.4MB/s][A[A[A[A





pytorch_model-00001-of-00004.bin:  74%|██████████████████████████████████████▎             | 3.66G/4.98G [01:07<00:22, 58.1MB/s][A[A[A[A[A





pytorch_model-00001-of-00004.bin:  74%|██████████████████████████████████████▍             | 3.68G/4.98G [01:07<00:21, 59.7MB/s][A[A[A[A[A





pytorch_model-00001-of-00004.bin:  74%|██████████████████████████████████████▌             | 3.70G/4.98G [01:07<00:20, 63.5MB/s][A[A[A[A[A





pytorch_model-00001-of-00004.bin:  75%|██████████████████████████████████████▊             | 3.71G/4.98G [01:07<00:19, 66.3MB/s][A[A[A[A[A





pytorch_model-00001-of-00004.bin:  75%|██████████████████████████████████████▉             | 3.73G/4.98G [01:08<00:17, 69.8MB/s][A[A[A[A[A





pytorch_model-00001-of-00004.bin:  75%|███████████████████████████████████████             | 3.74G/4.98G [01:08<00:17, 71.9MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:  75%|███████████████████████████████████████             | 3.70G/4.92G [01:08<00:46, 26.2MB/s][A[A[A[A





pytorch_model-00001-of-00004.bin:  76%|███████████████████████████████████████▎            | 3.76G/4.98G [01:08<00:16, 73.0MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:  76%|███████████████████████████████████████▎            | 3.71G/4.92G [01:08<00:37, 32.3MB/s][A[A[A[A





pytorch_model-00001-of-00004.bin:  76%|███████████████████████████████████████▍            | 3.78G/4.98G [01:08<00:16, 75.0MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:  76%|███████████████████████████████████████▍            | 3.73G/4.92G [01:08<00:33, 35.0MB/s][A[A[A[A





pytorch_model-00001-of-00004.bin:  76%|███████████████████████████████████████▌            | 3.79G/4.98G [01:08<00:16, 73.9MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:  76%|███████████████████████████████████████▌            | 3.74G/4.92G [01:09<00:28, 41.2MB/s][A[A[A[A





pytorch_model-00001-of-00004.bin:  77%|███████████████████████████████████████▊            | 3.81G/4.98G [01:09<00:16, 71.0MB/s][A[A[A[A[A





pytorch_model-00001-of-00004.bin:  77%|███████████████████████████████████████▉            | 3.82G/4.98G [01:09<00:16, 69.3MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:  76%|███████████████████████████████████████▊            | 3.76G/4.92G [01:09<00:27, 41.3MB/s][A[A[A[A





pytorch_model-00001-of-00004.bin:  77%|████████████████████████████████████████            | 3.84G/4.98G [01:09<00:15, 73.7MB/s][A[A[A[A[A





pytorch_model-00001-of-00004.bin:  77%|████████████████████████████████████████▎           | 3.86G/4.98G [01:09<00:15, 73.8MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:  77%|███████████████████████████████████████▉            | 3.78G/4.92G [01:09<00:25, 44.4MB/s][A[A[A[A





pytorch_model-00001-of-00004.bin:  78%|████████████████████████████████████████▍           | 3.87G/4.98G [01:10<00:15, 72.3MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:  77%|████████████████████████████████████████            | 3.79G/4.92G [01:10<00:22, 49.9MB/s][A[A[A[A




pytorch_model-00003-of-00004.bin:  77%|████████████████████████████████████████▎           | 3.81G/4.92G [01:10<00:19, 56.2MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:  79%|████████████████████████████████████████▉           | 3.94G/5.00G [01:10<01:55, 9.17MB/s][A[A





pytorch_model-00001-of-00004.bin:  78%|████████████████████████████████████████▌           | 3.89G/4.98G [01:10<00:16, 66.9MB/s][A[A[A[A[A





pytorch_model-00001-of-00004.bin:  78%|████████████████████████████████████████▊           | 3.90G/4.98G [01:10<00:14, 73.3MB/s][A[A[A[A[A


pytorch_model-00002-of-00004.bin:  79%|█████████████████████████████████████████           | 3.95G/5.00G [01:10<01:23, 12.5MB/s][A[A




pytorch_model-00003-of-00004.bin:  78%|████████████████████████████████████████▍           | 3.82G/4.92G [01:10<00:20, 54.0MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:  79%|█████████████████████████████████████████▎          | 3.97G/5.00G [01:10<01:02, 16.6MB/s][A[A





pytorch_model-00001-of-00004.bin:  79%|████████████████████████████████████████▉           | 3.92G/4.98G [01:10<00:15, 68.0MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:  78%|████████████████████████████████████████▌           | 3.84G/4.92G [01:10<00:18, 59.4MB/s][A[A[A[A




pytorch_model-00003-of-00004.bin:  78%|████████████████████████████████████████▊           | 3.86G/4.92G [01:11<00:16, 64.3MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:  80%|█████████████████████████████████████████▍          | 3.98G/5.00G [01:11<00:47, 21.3MB/s][A[A





pytorch_model-00001-of-00004.bin:  79%|█████████████████████████████████████████▏          | 3.94G/4.98G [01:11<00:15, 68.0MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:  79%|████████████████████████████████████████▉           | 3.87G/4.92G [01:11<00:15, 69.1MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:  80%|█████████████████████████████████████████▌          | 4.00G/5.00G [01:11<00:36, 27.3MB/s][A[A





pytorch_model-00001-of-00004.bin:  79%|█████████████████████████████████████████▎          | 3.95G/4.98G [01:11<00:13, 78.8MB/s][A[A[A[A[A





pytorch_model-00001-of-00004.bin:  80%|█████████████████████████████████████████▎          | 3.96G/4.98G [01:11<00:13, 73.0MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:  79%|████████████████████████████████████████▉           | 3.87G/4.92G [01:11<00:17, 58.7MB/s][A[A[A[A





pytorch_model-00001-of-00004.bin:  80%|█████████████████████████████████████████▍          | 3.97G/4.98G [01:11<00:15, 66.5MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:  79%|█████████████████████████████████████████▏          | 3.89G/4.92G [01:11<00:17, 60.1MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:  80%|█████████████████████████████████████████▋          | 4.01G/5.00G [01:11<00:40, 24.8MB/s][A[A





pytorch_model-00001-of-00004.bin:  80%|█████████████████████████████████████████▋          | 3.98G/4.98G [01:11<00:14, 70.3MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:  79%|█████████████████████████████████████████▎          | 3.90G/4.92G [01:11<00:17, 57.5MB/s][A[A[A[A





pytorch_model-00001-of-00004.bin:  80%|█████████████████████████████████████████▊          | 4.00G/4.98G [01:11<00:13, 70.4MB/s][A[A[A[A[A


pytorch_model-00002-of-00004.bin:  80%|█████████████████████████████████████████▊          | 4.02G/5.00G [01:11<00:37, 26.5MB/s][A[A


pytorch_model-00002-of-00004.bin:  81%|█████████████████████████████████████████▉          | 4.03G/5.00G [01:12<00:27, 35.1MB/s][A[A




pytorch_model-00003-of-00004.bin:  80%|█████████████████████████████████████████▍          | 3.92G/4.92G [01:12<00:16, 59.6MB/s][A[A[A[A





pytorch_model-00001-of-00004.bin:  81%|█████████████████████████████████████████▉          | 4.02G/4.98G [01:12<00:13, 70.7MB/s][A[A[A[A[A


pytorch_model-00002-of-00004.bin:  81%|██████████████████████████████████████████          | 4.05G/5.00G [01:12<00:22, 42.1MB/s][A[A




pytorch_model-00003-of-00004.bin:  80%|█████████████████████████████████████████▋          | 3.94G/4.92G [01:12<00:15, 62.7MB/s][A[A[A[A





pytorch_model-00001-of-00004.bin:  81%|██████████████████████████████████████████▏         | 4.03G/4.98G [01:12<00:13, 70.4MB/s][A[A[A[A[A


pytorch_model-00002-of-00004.bin:  81%|██████████████████████████████████████████▎         | 4.06G/5.00G [01:12<00:18, 50.3MB/s][A[A




pytorch_model-00003-of-00004.bin:  80%|█████████████████████████████████████████▊          | 3.95G/4.92G [01:12<00:14, 67.5MB/s][A[A[A[A





pytorch_model-00001-of-00004.bin:  81%|██████████████████████████████████████████▎         | 4.05G/4.98G [01:12<00:12, 72.8MB/s][A[A[A[A[A


pytorch_model-00002-of-00004.bin:  82%|██████████████████████████████████████████▍         | 4.08G/5.00G [01:12<00:16, 56.2MB/s][A[A




pytorch_model-00003-of-00004.bin:  81%|█████████████████████████████████████████▉          | 3.97G/4.92G [01:12<00:13, 68.6MB/s][A[A[A[A





pytorch_model-00001-of-00004.bin:  82%|██████████████████████████████████████████▍         | 4.06G/4.98G [01:12<00:12, 73.1MB/s][A[A[A[A[A


pytorch_model-00002-of-00004.bin:  82%|██████████████████████████████████████████▌         | 4.10G/5.00G [01:12<00:15, 59.3MB/s][A[A




pytorch_model-00003-of-00004.bin:  81%|██████████████████████████████████████████▏         | 3.98G/4.92G [01:12<00:13, 69.6MB/s][A[A[A[A





pytorch_model-00001-of-00004.bin:  82%|██████████████████████████████████████████▋         | 4.08G/4.98G [01:12<00:12, 73.5MB/s][A[A[A[A[A


pytorch_model-00002-of-00004.bin:  82%|██████████████████████████████████████████▊         | 4.11G/5.00G [01:13<00:12, 72.1MB/s][A[A




pytorch_model-00003-of-00004.bin:  81%|██████████████████████████████████████████▎         | 4.00G/4.92G [01:13<00:11, 81.8MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:  82%|██████████████████████████████████████████▊         | 4.12G/5.00G [01:13<00:12, 70.8MB/s][A[A





pytorch_model-00001-of-00004.bin:  82%|██████████████████████████████████████████▊         | 4.10G/4.98G [01:13<00:12, 69.5MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:  82%|██████████████████████████████████████████▍         | 4.01G/4.92G [01:13<00:13, 67.2MB/s][A[A[A[A





pytorch_model-00001-of-00004.bin:  83%|██████████████████████████████████████████▉         | 4.11G/4.98G [01:13<00:11, 72.4MB/s][A[A[A[A[A


pytorch_model-00002-of-00004.bin:  83%|██████████████████████████████████████████▉         | 4.13G/5.00G [01:13<00:15, 55.8MB/s][A[A




pytorch_model-00003-of-00004.bin:  82%|██████████████████████████████████████████▍         | 4.02G/4.92G [01:13<00:14, 60.2MB/s][A[A[A[A




pytorch_model-00003-of-00004.bin:  82%|██████████████████████████████████████████▋         | 4.03G/4.92G [01:13<00:13, 64.9MB/s][A[A[A[A





pytorch_model-00001-of-00004.bin:  83%|███████████████████████████████████████████▏        | 4.13G/4.98G [01:13<00:12, 68.1MB/s][A[A[A[A[A


pytorch_model-00002-of-00004.bin:  83%|███████████████████████████████████████████         | 4.14G/5.00G [01:13<00:16, 52.2MB/s][A[A


pytorch_model-00002-of-00004.bin:  83%|███████████████████████████████████████████▎        | 4.16G/5.00G [01:13<00:13, 61.4MB/s][A[A




pytorch_model-00003-of-00004.bin:  82%|██████████████████████████████████████████▊         | 4.05G/4.92G [01:14<00:19, 44.5MB/s][A[A[A[A




pytorch_model-00003-of-00004.bin:  83%|██████████████████████████████████████████▉         | 4.06G/4.92G [01:14<00:16, 52.7MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:  84%|███████████████████████████████████████████▍        | 4.18G/5.00G [01:14<00:18, 44.4MB/s][A[A




pytorch_model-00003-of-00004.bin:  83%|███████████████████████████████████████████▏        | 4.08G/4.92G [01:14<00:15, 55.4MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:  84%|███████████████████████████████████████████▌        | 4.19G/5.00G [01:14<00:16, 49.8MB/s][A[A




pytorch_model-00003-of-00004.bin:  83%|███████████████████████████████████████████▎        | 4.10G/4.92G [01:14<00:13, 61.2MB/s][A[A[A[A





pytorch_model-00001-of-00004.bin:  83%|███████████████████████████████████████████▎        | 4.14G/4.98G [01:14<00:28, 29.5MB/s][A[A[A[A[A


pytorch_model-00002-of-00004.bin:  84%|███████████████████████████████████████████▊        | 4.21G/5.00G [01:15<00:14, 55.3MB/s][A[A




pytorch_model-00003-of-00004.bin:  84%|███████████████████████████████████████████▍        | 4.11G/4.92G [01:15<00:11, 67.5MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:  84%|███████████████████████████████████████████▉        | 4.22G/5.00G [01:15<00:12, 60.8MB/s][A[A




pytorch_model-00003-of-00004.bin:  84%|███████████████████████████████████████████▋        | 4.13G/4.92G [01:15<00:11, 65.7MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:  85%|████████████████████████████████████████████        | 4.24G/5.00G [01:15<00:11, 65.9MB/s][A[A





pytorch_model-00001-of-00004.bin:  84%|███████████████████████████████████████████▍        | 4.16G/4.98G [01:15<00:28, 28.7MB/s][A[A[A[A[A


pytorch_model-00002-of-00004.bin:  85%|████████████████████████████████████████████▎       | 4.26G/5.00G [01:15<00:10, 69.3MB/s][A[A




pytorch_model-00003-of-00004.bin:  84%|███████████████████████████████████████████▊        | 4.14G/4.92G [01:15<00:11, 64.9MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:  85%|████████████████████████████████████████████▍       | 4.27G/5.00G [01:15<00:10, 67.6MB/s][A[A




pytorch_model-00003-of-00004.bin:  85%|████████████████████████████████████████████        | 4.16G/4.92G [01:15<00:11, 66.6MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:  86%|████████████████████████████████████████████▌       | 4.29G/5.00G [01:16<00:10, 71.1MB/s][A[A




pytorch_model-00003-of-00004.bin:  85%|████████████████████████████████████████████▏       | 4.18G/4.92G [01:16<00:11, 67.1MB/s][A[A[A[A





pytorch_model-00001-of-00004.bin:  84%|███████████████████████████████████████████▋        | 4.18G/4.98G [01:16<00:28, 27.7MB/s][A[A[A[A[A


pytorch_model-00002-of-00004.bin:  86%|████████████████████████████████████████████▊       | 4.30G/5.00G [01:16<00:09, 72.9MB/s][A[A




pytorch_model-00003-of-00004.bin:  85%|████████████████████████████████████████████▎       | 4.19G/4.92G [01:16<00:09, 72.5MB/s][A[A[A[A





pytorch_model-00001-of-00004.bin:  84%|███████████████████████████████████████████▊        | 4.19G/4.98G [01:16<00:22, 35.0MB/s][A[A[A[A[A


pytorch_model-00002-of-00004.bin:  86%|████████████████████████████████████████████▉       | 4.32G/5.00G [01:16<00:08, 76.5MB/s][A[A




pytorch_model-00003-of-00004.bin:  86%|████████████████████████████████████████████▌       | 4.21G/4.92G [01:16<00:10, 70.0MB/s][A[A[A[A





pytorch_model-00001-of-00004.bin:  85%|███████████████████████████████████████████▉        | 4.21G/4.98G [01:16<00:18, 41.9MB/s][A[A[A[A[A


pytorch_model-00002-of-00004.bin:  87%|█████████████████████████████████████████████       | 4.34G/5.00G [01:16<00:08, 75.7MB/s][A[A




pytorch_model-00003-of-00004.bin:  86%|████████████████████████████████████████████▋       | 4.22G/4.92G [01:16<00:09, 74.7MB/s][A[A[A[A





pytorch_model-00001-of-00004.bin:  85%|████████████████████████████████████████████▏       | 4.22G/4.98G [01:16<00:15, 48.4MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:  86%|████████████████████████████████████████████▊       | 4.24G/4.92G [01:16<00:08, 75.3MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:  87%|█████████████████████████████████████████████▎      | 4.35G/5.00G [01:16<00:09, 67.5MB/s][A[A




pytorch_model-00003-of-00004.bin:  87%|█████████████████████████████████████████████       | 4.26G/4.92G [01:17<00:08, 74.9MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:  87%|█████████████████████████████████████████████▍      | 4.37G/5.00G [01:17<00:10, 61.7MB/s][A[A





pytorch_model-00001-of-00004.bin:  85%|████████████████████████████████████████████▎       | 4.24G/4.98G [01:17<00:17, 41.4MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:  87%|█████████████████████████████████████████████▏      | 4.27G/4.92G [01:17<00:08, 73.2MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:  88%|█████████████████████████████████████████████▌      | 4.38G/5.00G [01:17<00:09, 66.2MB/s][A[A





pytorch_model-00001-of-00004.bin:  86%|████████████████████████████████████████████▍       | 4.26G/4.98G [01:17<00:14, 48.2MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:  87%|█████████████████████████████████████████████▎      | 4.29G/4.92G [01:17<00:08, 71.9MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:  88%|█████████████████████████████████████████████▊      | 4.40G/5.00G [01:17<00:08, 72.1MB/s][A[A





pytorch_model-00001-of-00004.bin:  86%|████████████████████████████████████████████▋       | 4.27G/4.98G [01:17<00:13, 53.5MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:  88%|█████████████████████████████████████████████▌      | 4.30G/4.92G [01:17<00:08, 75.3MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:  88%|█████████████████████████████████████████████▉      | 4.42G/5.00G [01:17<00:08, 71.2MB/s][A[A





pytorch_model-00001-of-00004.bin:  86%|████████████████████████████████████████████▊       | 4.29G/4.98G [01:17<00:11, 61.0MB/s][A[A[A[A[A





pytorch_model-00001-of-00004.bin:  86%|████████████████████████████████████████████▉       | 4.30G/4.98G [01:18<00:10, 63.7MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:  88%|█████████████████████████████████████████████▋      | 4.32G/4.92G [01:18<00:11, 50.0MB/s][A[A[A[A





pytorch_model-00001-of-00004.bin:  87%|█████████████████████████████████████████████▏      | 4.32G/4.98G [01:18<00:10, 65.0MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:  88%|█████████████████████████████████████████████▊      | 4.34G/4.92G [01:18<00:09, 58.6MB/s][A[A[A[A





pytorch_model-00001-of-00004.bin:  87%|█████████████████████████████████████████████▎      | 4.34G/4.98G [01:18<00:11, 57.7MB/s][A[A[A[A[A





pytorch_model-00001-of-00004.bin:  87%|█████████████████████████████████████████████▍      | 4.35G/4.98G [01:18<00:10, 57.7MB/s][A[A[A[A[A


pytorch_model-00002-of-00004.bin:  89%|██████████████████████████████████████████████      | 4.43G/5.00G [01:19<00:17, 31.7MB/s][A[A





pytorch_model-00001-of-00004.bin:  88%|█████████████████████████████████████████████▋      | 4.37G/4.98G [01:19<00:09, 62.9MB/s][A[A[A[A[A


pytorch_model-00002-of-00004.bin:  89%|██████████████████████████████████████████████▎     | 4.45G/5.00G [01:19<00:15, 36.4MB/s][A[A





pytorch_model-00001-of-00004.bin:  88%|█████████████████████████████████████████████▊      | 4.38G/4.98G [01:19<00:09, 63.3MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:  89%|██████████████████████████████████████████████      | 4.35G/4.92G [01:19<00:17, 32.7MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:  89%|██████████████████████████████████████████████▍     | 4.46G/5.00G [01:19<00:12, 41.7MB/s][A[A




pytorch_model-00003-of-00004.bin:  89%|██████████████████████████████████████████████▏     | 4.37G/4.92G [01:19<00:14, 38.9MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:  90%|██████████████████████████████████████████████▌     | 4.48G/5.00G [01:19<00:10, 48.5MB/s][A[A




pytorch_model-00003-of-00004.bin:  89%|██████████████████████████████████████████████▎     | 4.38G/4.92G [01:19<00:11, 45.9MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:  90%|██████████████████████████████████████████████▊     | 4.50G/5.00G [01:19<00:09, 55.7MB/s][A[A


pytorch_model-00002-of-00004.bin:  90%|██████████████████████████████████████████████▉     | 4.51G/5.00G [01:20<00:07, 61.3MB/s][A[A




pytorch_model-00003-of-00004.bin:  90%|██████████████████████████████████████████████▌     | 4.40G/4.92G [01:20<00:10, 47.2MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:  91%|███████████████████████████████████████████████     | 4.53G/5.00G [01:20<00:07, 63.8MB/s][A[A




pytorch_model-00003-of-00004.bin:  90%|██████████████████████████████████████████████▋     | 4.42G/4.92G [01:20<00:09, 53.1MB/s][A[A[A[A





pytorch_model-00001-of-00004.bin:  88%|█████████████████████████████████████████████▉      | 4.40G/4.98G [01:20<00:18, 32.0MB/s][A[A[A[A[A


pytorch_model-00002-of-00004.bin:  91%|███████████████████████████████████████████████▎    | 4.54G/5.00G [01:20<00:06, 70.5MB/s][A[A


pytorch_model-00002-of-00004.bin:  91%|███████████████████████████████████████████████▍    | 4.56G/5.00G [01:20<00:06, 70.1MB/s][A[A





pytorch_model-00001-of-00004.bin:  89%|██████████████████████████████████████████████▏     | 4.42G/4.98G [01:20<00:16, 35.0MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:  90%|██████████████████████████████████████████████▉     | 4.43G/4.92G [01:21<00:11, 43.0MB/s][A[A[A[A





pytorch_model-00001-of-00004.bin:  89%|██████████████████████████████████████████████▎     | 4.43G/4.98G [01:21<00:13, 41.1MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:  90%|███████████████████████████████████████████████     | 4.45G/4.92G [01:21<00:10, 43.2MB/s][A[A[A[A




pytorch_model-00003-of-00004.bin:  91%|███████████████████████████████████████████████▏    | 4.46G/4.92G [01:21<00:08, 51.9MB/s][A[A[A[A





pytorch_model-00001-of-00004.bin:  89%|██████████████████████████████████████████████▍     | 4.45G/4.98G [01:21<00:13, 39.7MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:  91%|███████████████████████████████████████████████▍    | 4.48G/4.92G [01:21<00:07, 57.6MB/s][A[A[A[A





pytorch_model-00001-of-00004.bin:  90%|██████████████████████████████████████████████▋     | 4.46G/4.98G [01:21<00:11, 46.3MB/s][A[A[A[A[A


pytorch_model-00002-of-00004.bin:  92%|███████████████████████████████████████████████▌    | 4.58G/5.00G [01:21<00:12, 35.3MB/s][A[A


pytorch_model-00002-of-00004.bin:  92%|███████████████████████████████████████████████▊    | 4.59G/5.00G [01:21<00:09, 42.2MB/s][A[A




pytorch_model-00003-of-00004.bin:  91%|███████████████████████████████████████████████▌    | 4.50G/4.92G [01:21<00:07, 59.5MB/s][A[A[A[A





pytorch_model-00001-of-00004.bin:  90%|██████████████████████████████████████████████▊     | 4.48G/4.98G [01:22<00:09, 50.6MB/s][A[A[A[A[A


pytorch_model-00002-of-00004.bin:  92%|███████████████████████████████████████████████▉    | 4.61G/5.00G [01:22<00:07, 51.7MB/s][A[A




pytorch_model-00003-of-00004.bin:  92%|███████████████████████████████████████████████▋    | 4.51G/4.92G [01:22<00:05, 69.9MB/s][A[A[A[A





pytorch_model-00001-of-00004.bin:  90%|██████████████████████████████████████████████▉     | 4.49G/4.98G [01:22<00:08, 60.3MB/s][A[A[A[A[A


pytorch_model-00002-of-00004.bin:  92%|███████████████████████████████████████████████▉    | 4.61G/5.00G [01:22<00:07, 54.2MB/s][A[A





pytorch_model-00001-of-00004.bin:  90%|███████████████████████████████████████████████     | 4.50G/4.98G [01:22<00:08, 57.4MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:  92%|███████████████████████████████████████████████▊    | 4.52G/4.92G [01:22<00:06, 61.6MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:  92%|████████████████████████████████████████████████    | 4.62G/5.00G [01:22<00:07, 52.7MB/s][A[A





pytorch_model-00001-of-00004.bin:  91%|███████████████████████████████████████████████▏    | 4.51G/4.98G [01:22<00:07, 58.8MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:  92%|███████████████████████████████████████████████▉    | 4.53G/4.92G [01:22<00:06, 60.3MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:  93%|████████████████████████████████████████████████▎   | 4.64G/5.00G [01:22<00:05, 63.3MB/s][A[A





pytorch_model-00001-of-00004.bin:  91%|███████████████████████████████████████████████▎    | 4.53G/4.98G [01:22<00:07, 63.5MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:  92%|████████████████████████████████████████████████    | 4.54G/4.92G [01:22<00:05, 65.2MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:  93%|████████████████████████████████████████████████▍   | 4.66G/5.00G [01:22<00:05, 58.1MB/s][A[A





pytorch_model-00001-of-00004.bin:  91%|███████████████████████████████████████████████▍    | 4.54G/4.98G [01:22<00:06, 63.8MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:  93%|████████████████████████████████████████████████▏   | 4.56G/4.92G [01:23<00:06, 51.5MB/s][A[A[A[A





pytorch_model-00001-of-00004.bin:  92%|███████████████████████████████████████████████▋    | 4.56G/4.98G [01:23<00:06, 63.2MB/s][A[A[A[A[A





pytorch_model-00001-of-00004.bin:  92%|███████████████████████████████████████████████▊    | 4.58G/4.98G [01:23<00:05, 71.1MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:  93%|████████████████████████████████████████████████▍   | 4.58G/4.92G [01:23<00:06, 56.2MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:  93%|████████████████████████████████████████████████▌   | 4.67G/5.00G [01:23<00:07, 43.0MB/s][A[A




pytorch_model-00003-of-00004.bin:  93%|████████████████████████████████████████████████▌   | 4.59G/4.92G [01:23<00:05, 64.0MB/s][A[A[A[A





pytorch_model-00001-of-00004.bin:  92%|███████████████████████████████████████████████▉    | 4.59G/4.98G [01:23<00:05, 72.0MB/s][A[A[A[A[A


pytorch_model-00002-of-00004.bin:  94%|████████████████████████████████████████████████▊   | 4.69G/5.00G [01:23<00:06, 49.6MB/s][A[A





pytorch_model-00001-of-00004.bin:  93%|████████████████████████████████████████████████▏   | 4.61G/4.98G [01:23<00:04, 76.4MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:  94%|████████████████████████████████████████████████▋   | 4.61G/4.92G [01:23<00:04, 64.7MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:  94%|████████████████████████████████████████████████▉   | 4.70G/5.00G [01:23<00:05, 54.9MB/s][A[A





pytorch_model-00001-of-00004.bin:  93%|████████████████████████████████████████████████▎   | 4.62G/4.98G [01:23<00:04, 74.7MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:  94%|████████████████████████████████████████████████▉   | 4.62G/4.92G [01:23<00:04, 65.6MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:  94%|█████████████████████████████████████████████████   | 4.72G/5.00G [01:24<00:04, 56.7MB/s][A[A




pytorch_model-00003-of-00004.bin:  94%|█████████████████████████████████████████████████   | 4.64G/4.92G [01:24<00:04, 64.9MB/s][A[A[A[A





pytorch_model-00001-of-00004.bin:  93%|████████████████████████████████████████████████▍   | 4.64G/4.98G [01:24<00:05, 65.3MB/s][A[A[A[A[A





pytorch_model-00001-of-00004.bin:  94%|████████████████████████████████████████████████▋   | 4.66G/4.98G [01:24<00:04, 69.3MB/s][A[A[A[A[A


pytorch_model-00002-of-00004.bin:  95%|█████████████████████████████████████████████████▎  | 4.74G/5.00G [01:24<00:05, 51.7MB/s][A[A




pytorch_model-00003-of-00004.bin:  95%|█████████████████████████████████████████████████▎  | 4.66G/4.92G [01:24<00:04, 61.8MB/s][A[A[A[A





pytorch_model-00001-of-00004.bin:  94%|████████████████████████████████████████████████▊   | 4.67G/4.98G [01:24<00:04, 72.7MB/s][A[A[A[A[A


pytorch_model-00002-of-00004.bin:  95%|█████████████████████████████████████████████████▍  | 4.75G/5.00G [01:24<00:04, 57.7MB/s][A[A




pytorch_model-00003-of-00004.bin:  95%|█████████████████████████████████████████████████▍  | 4.67G/4.92G [01:24<00:03, 65.1MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:  95%|█████████████████████████████████████████████████▌  | 4.77G/5.00G [01:24<00:03, 65.1MB/s][A[A




pytorch_model-00003-of-00004.bin:  95%|█████████████████████████████████████████████████▌  | 4.69G/4.92G [01:24<00:03, 67.2MB/s][A[A[A[A





pytorch_model-00001-of-00004.bin:  94%|████████████████████████████████████████████████▉   | 4.69G/4.98G [01:25<00:05, 53.5MB/s][A[A[A[A[A


pytorch_model-00002-of-00004.bin:  96%|█████████████████████████████████████████████████▊  | 4.78G/5.00G [01:25<00:03, 64.3MB/s][A[A




pytorch_model-00003-of-00004.bin:  96%|█████████████████████████████████████████████████▊  | 4.70G/4.92G [01:25<00:03, 66.5MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:  96%|█████████████████████████████████████████████████▉  | 4.80G/5.00G [01:25<00:02, 72.0MB/s][A[A





pytorch_model-00001-of-00004.bin:  95%|█████████████████████████████████████████████████▏  | 4.70G/4.98G [01:25<00:04, 55.4MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:  96%|█████████████████████████████████████████████████▉  | 4.72G/4.92G [01:25<00:02, 69.1MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:  96%|██████████████████████████████████████████████████  | 4.82G/5.00G [01:25<00:02, 62.5MB/s][A[A




pytorch_model-00003-of-00004.bin:  96%|██████████████████████████████████████████████████  | 4.74G/4.92G [01:25<00:02, 68.2MB/s][A[A[A[A





pytorch_model-00001-of-00004.bin:  95%|█████████████████████████████████████████████████▎  | 4.72G/4.98G [01:25<00:04, 56.1MB/s][A[A[A[A[A


pytorch_model-00002-of-00004.bin:  97%|██████████████████████████████████████████████████▎ | 4.83G/5.00G [01:25<00:02, 65.4MB/s][A[A





pytorch_model-00001-of-00004.bin:  95%|█████████████████████████████████████████████████▍  | 4.74G/4.98G [01:25<00:04, 58.4MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:  97%|██████████████████████████████████████████████████▎ | 4.75G/4.92G [01:25<00:02, 64.4MB/s][A[A[A[A





pytorch_model-00001-of-00004.bin:  95%|█████████████████████████████████████████████████▋  | 4.75G/4.98G [01:26<00:03, 62.1MB/s][A[A[A[A[A


pytorch_model-00002-of-00004.bin:  97%|██████████████████████████████████████████████████▍ | 4.85G/5.00G [01:26<00:02, 63.6MB/s][A[A




pytorch_model-00003-of-00004.bin:  97%|██████████████████████████████████████████████████▍ | 4.77G/4.92G [01:26<00:02, 59.1MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:  97%|██████████████████████████████████████████████████▌ | 4.86G/5.00G [01:26<00:02, 65.8MB/s][A[A





pytorch_model-00001-of-00004.bin:  96%|█████████████████████████████████████████████████▊  | 4.77G/4.98G [01:26<00:03, 59.8MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:  97%|██████████████████████████████████████████████████▌ | 4.78G/4.92G [01:26<00:02, 58.7MB/s][A[A[A[A


pytorch_model-00002-of-00004.bin:  98%|██████████████████████████████████████████████████▊ | 4.88G/5.00G [01:26<00:01, 67.5MB/s][A[A





pytorch_model-00001-of-00004.bin:  96%|█████████████████████████████████████████████████▉  | 4.78G/4.98G [01:26<00:02, 64.7MB/s][A[A[A[A[A




pytorch_model-00003-of-00004.bin:  98%|██████████████████████████████████████████████████▊ | 4.80G/4.92G [01:26<00:01, 60.6MB/s][A[A[A[A





pytorch_model-00001-of-00004.bin:  96%|██████████████████████████████████████████████████▏ | 4.80G/4.98G [01:26<00:02, 66.2MB/s][A[A[A[A[A


pytorch_model-00002-of-00004.bin:  98%|██████████████████████████████████████████████████▉ | 4.90G/5.00G [01:26<00:01, 56.7MB/s][A[A




pytorch_model-00003-of-00004.bin:  98%|██████████████████████████████████████████████████▉ | 4.82G/4.92G [01:26<00:01, 66.4MB/s][A[A[A[A





pytorch_model-00001-of-00004.bin:  97%|██████████████████████████████████████████████████▎ | 4.82G/4.98G [01:27<00:02, 69.7MB/s][A[A[A[A[A


pytorch_model-00002-of-00004.bin:  98%|███████████████████████████████████████████████████ | 4.91G/5.00G [01:27<00:01, 61.5MB/s][A[A




pytorch_model-00003-of-00004.bin:  98%|███████████████████████████████████████████████████ | 4.83G/4.92G [01:27<00:01, 66.1MB/s][A[A[A[A





pytorch_model-00001-of-00004.bin:  97%|██████████████████████████████████████████████████▍ | 4.83G/4.98G [01:27<00:02, 67.4MB/s][A[A[A[A[A


pytorch_model-00002-of-00004.bin:  99%|███████████████████████████████████████████████████▎| 4.93G/5.00G [01:27<00:01, 62.6MB/s][A[A




pytorch_model-00003-of-00004.bin:  99%|███████████████████████████████████████████████████▎| 4.85G/4.92G [01:27<00:01, 67.8MB/s][A[A[A[A





pytorch_model-00001-of-00004.bin:  97%|██████████████████████████████████████████████████▋ | 4.85G/4.98G [01:27<00:01, 73.5MB/s][A[A[A[A[A


pytorch_model-00002-of-00004.bin:  99%|███████████████████████████████████████████████████▍| 4.94G/5.00G [01:27<00:00, 67.1MB/s][A[A




pytorch_model-00003-of-00004.bin:  99%|███████████████████████████████████████████████████▍| 4.86G/4.92G [01:27<00:00, 70.3MB/s][A[A[A[A





pytorch_model-00001-of-00004.bin:  98%|██████████████████████████████████████████████████▊ | 4.86G/4.98G [01:27<00:01, 67.3MB/s][A[A[A[A[A


pytorch_model-00002-of-00004.bin:  99%|███████████████████████████████████████████████████▌| 4.96G/5.00G [01:27<00:00, 73.4MB/s][A[A




pytorch_model-00003-of-00004.bin:  99%|███████████████████████████████████████████████████▌| 4.88G/4.92G [01:27<00:00, 63.0MB/s][A[A[A[A





pytorch_model-00001-of-00004.bin:  98%|██████████████████████████████████████████████████▉ | 4.88G/4.98G [01:28<00:01, 68.5MB/s][A[A[A[A[A


pytorch_model-00002-of-00004.bin: 100%|███████████████████████████████████████████████████▊| 4.98G/5.00G [01:28<00:00, 72.6MB/s][A[A




pytorch_model-00003-of-00004.bin: 100%|███████████████████████████████████████████████████▊| 4.90G/4.92G [01:28<00:00, 67.1MB/s][A[A[A[A





pytorch_model-00001-of-00004.bin:  98%|███████████████████████████████████████████████████▏| 4.90G/4.98G [01:28<00:01, 72.7MB/s][A[A[A[A[A


pytorch_model-00002-of-00004.bin: 100%|███████████████████████████████████████████████████▉| 4.99G/5.00G [01:28<00:00, 73.3MB/s][A[A




pytorch_model-00003-of-00004.bin: 100%|███████████████████████████████████████████████████▉| 4.91G/4.92G [01:28<00:00, 70.1MB/s][A[A[A[A
pytorch_model-00002-of-00004.bin: 100%|████████████████████████████████████████████████████| 5.00G/5.00G [01:28<00:00, 56.6MB/s]

pytorch_model-00003-of-00004.bin: 100%|████████████████████████████████████████████████████| 4.92G/4.92G [01:28<00:00, 55.5MB/s]






pytorch_model-00001-of-00004.bin:  99%|███████████████████████████████████████████████████▎| 4.91G/4.98G [01:28<00:01, 40.8MB/s][A[A[A[A[A





pytorch_model-00001-of-00004.bin:  99%|███████████████████████████████████████████████████▍| 4.93G/4.98G [01:29<00:01, 46.4MB/s][A[A[A[A[A





pytorch_model-00001-of-00004.bin:  99%|███████████████████████████████████████████████████▋| 4.94G/4.98G [01:29<00:00, 42.0MB/s][A[A[A[A[A





pytorch_model-00001-of-00004.bin: 100%|███████████████████████████████████████████████████▊| 4.96G/4.98G [01:29<00:00, 47.8MB/s][A[A[A[A[A





pytorch_model-00001-of-00004.bin: 100%|███████████████████████████████████████████████████▉| 4.98G/4.98G [01:30<00:00, 46.3MB/s][A[A[A[A[A
pytorch_model-00001-of-00004.bin: 100%|████████████████████████████████████████████████████| 4.98G/4.98G [01:30<00:00, 55.1MB/s]


Upload 4 LFS files:  25%|██████████████████▎                                                      | 1/4 [01:30<04:31, 90.59s/it][A
Upload 4 LFS files: 100%|█████████████████████████████████████████████████████████████████████████| 4/4 [01:30<00:00, 22.65s/it]
🔥 pushed to https://huggingface.co/WenWW/sparse_Llama_8B_2of4_dpo/tree/dpo_tune__42__1736281068
df: /root/.triton/autotune: No such file or directory
df: /root/.triton/autotune: No such file or directory
df: /root/.triton/autotune: No such file or directory
df: /root/.triton/autotune: No such file or directory
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:            epoch ▁▂▃▄▆▇█
wandb:    learning_rate █▇▅▄▃▂▁
wandb:     logps/chosen ▅▂█▆▇▁▆
wandb:   logps/rejected ▇▁▆▁▁▅█
wandb: rewards/accuracy ▁▁▁▁▁▁▁
wandb:  rewards/average ▁▁▁▁▁▁▁
wandb:   rewards/chosen ▁▁▁▁▁▁▁
wandb:   rewards/margin ▁▁▁▁▁▁▁
wandb: rewards/rejected ▁▁▁▁▁▁▁
wandb:       train_loss ▁▁▁▁▁▁▁
wandb:    training_step ▁▂▃▅▆▇█
wandb: 
wandb: Run summary:
wandb:            epoch 0.14
wandb:    learning_rate 0
wandb:     logps/chosen -8.72193
wandb:   logps/rejected -6.69346
wandb: rewards/accuracy 0
wandb:  rewards/average 0
wandb:   rewards/chosen 0
wandb:   rewards/margin 0
wandb: rewards/rejected 0
wandb:       train_loss 0.08664
wandb:    training_step 7
wandb: 
wandb: 🚀 View run dpo_tune__42__1736281068 at: https://wandb.ai/wenvven-university-of-stuttgart/open_instruct_internal/runs/vujib8jk
wandb: ⭐️ View project at: https://wandb.ai/wenvven-university-of-stuttgart/open_instruct_internal
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250107_201919-vujib8jk/logs
/workspace/tulu_env/lib/python3.10/site-packages/wandb/sdk/wandb_run.py:2299: UserWarning: Run (vujib8jk) is finished. The call to `_console_raw_callback` will be ignored. Please make sure that you are using an active run.
  lambda data: self._console_raw_callback("stderr", data),

100%|█████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [05:21<00:00, 45.93s/it]
df: /root/.triton/autotune: No such file or directory
df: /root/.triton/autotune: No such file or directory
df: /root/.triton/autotune: No such file or directory
df: /root/.triton/autotune: No such file or directory
df: /root/.triton/autotune: No such file or directory
df: /root/.triton/autotune: No such file or directory
df: /root/.triton/autotune: No such file or directory
df: /root/.triton/autotune: No such file or directory
